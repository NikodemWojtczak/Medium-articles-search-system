How to Generate “Summaries” for Review Text Without Neural Networks

As a data scientist working in the consumer goods industry at Clorox, one of my “daily challenges” is to extract useful information from a large collection of user reviews about a specific product quickly and concisely. From its appearance, the task requires some kind of summarization of a large collection of text. When it comes to generating text summaries, we usually think of sophisticated, deep-learning related models and solutions — the sequence-to-sequence model, the encoder-decoder RNNs, the Attention mechanism, etc. However, while these deep-learning models take a huge effort to train, their performances are not always satisfactory for specific tasks. More importantly, sometimes we don’t actually need a linguistic summary of text if our goal is to extract insightful information from the text contents. In this blog, I would like to introduce a method that can generate “summaries” from reviews texts with only frequency-based counts.

As an example, I will be using a subset of public user reviews from Amazon for one of the top products in household cleaning, the Clorox wipes. This link will take you to the review page. Below is the “summary” that I’m able to generate from the collection of reviews.

Here is how it works:

Part-of-speech Tagging

The first step is to tag each token in the review text with its part-of-speech. When we think about the user reviews towards a certain product together with the part-of-speech components of text, it shouldn’t be surprising that the terms or tokens which provide the most insightful information are very likely found as Nouns or Adjectives. Nouns help explain the “What” question (what are the consumer talking about the product, the subjects. It could be packaging, delivery, smell, etc), while adjectives help with the “what about” question. (What about packaging; what about delivery; what about the smell). After tagging, we will extract the top-15 most popular nouns and top-15 most popular adjectives.

Related term search

Now that we have the frequencies of Nouns and Adjectives, the next step will be to find the relationships between them.

Specifically, for a popular noun, how can we know the top related Adjectives? Similarly for an Adjective, how can we know the top related Nouns? Knowing the related terms can potentially help us deepen the information we get from the texts, but the task itself is not as straightforward as it seems to be.

Take the term “price” as an example. It is shown in the plot above as the third popular nouns, now I would like to know “what about” the price by looking at top related adjectives that our consumer use. Then we have a problem — how to define related? More specifically, how to construct a search algorithm that finds and displays the top related adjectives?

Let me use the following review as an example to help illustrate my approach:

In this case, let’s call the term “price” the target term as we want to get information about “price.” Within a specific review, we repeat the following steps for each occurrence of the target term.

1. Identify the position of the target term (highlighted in yellow)

2. Find the nearest terms with the same POS tag as the target term. Call them the “boundary terms” (highlighted in blue). In this case, since “price” is a noun, the boundary terms will be the nearest nouns.

3. Find all the adjectives within the boundary terms and save them into a list

Then we repeat the same process above with “price” as the target term for all the reviews. Eventually, we will have a list of adjectives that are potentially “related” with the term price. In this way, I am able to get the top-15 related adjectives as below:

Terms like “good” and “great” are extremely popular. Which is actually true as we can find lots of reviews on Amazon that says “good price” and “great price.” Lastly, we iterate the entire related term search process for all the top mentioned Nouns and Adjs, we will eventually get to the summary table I presented at the beginning.

This is not a complicated approach, but it is very effective and easy to use. It is as simple as constructing a word-cloud but provides more insights and more interpretable as well. To read more about this technique, how it came through and what are some drawbacks, here is the link to the original blog I wrote for it.

Here you can find the code for the method I mentioned above. Below is how to use it:

I hope you find this blog helpful, and I would like to hear your feedbacks

Thanks for reading through!