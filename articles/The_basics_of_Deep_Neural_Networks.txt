With the rise of libraries such as Tensorflow 2.0, PyTorch and Fastai, implementing deep learning has become accessible to so many more people and it helps to understand the fundamentals behind deep neural networks. Hopefully this article will be of help people to people on the path of learning about deep neural networks.

Back when I first learnt about neural nets and implemented my first, they were always represented as individual artificial neurons, essentially nodes with individually weighted inputs, a summed output and an activation function.

When first returning into learning about deep neural networks, the concept of how this equated to matrix multiplication didn’t appear obvious. Also, linked to this is why Graphics Processing Units (GPUs) and their spin-offs have helped advance deep learning results so much.

A very simple Neural Network compared to a matrix multiplication

Let’s take a very simple network with two inputs, with one hidden layer of two neurons. Each neuron has two weights, an individual weight for each of its inputs. Each weight is multiplied by each of the inputs into the neuron, these are then summed and form the output from the neuron after it has been fed through an activation function.

A very simple network

This is the same as applying two matrix multiplications followed by the activation function.

First matrix dot product multiplication. Generated using the excellent http://matrixmultiplication.xyz

Second matrix dot product multiplication. Generated using the excellent http://matrixmultiplication.xyz

Advances enabled by GPUs

One of the reasons we left the last so called ‘AI winter’, where AI and machine learning research and interest slowed and became ‘cold’ was the computational advances made from using GPUs.

GPUs traditionally were used in the domains of computer graphics and computer video games. Computer graphics needs very fast matrix multiplication. The computer video game industry’s demand for the computational power of GPUs caused huge advance from the GPU designers and manufacturers.

Notably Nvidia, one of the largest GPU companies, released CUDA (a language/API for programming on Nvidia GPUs) and cuDNN (a library for deep neural nets built using CUDA). These allow computation in training neural networks to be more easily moved to the GPU, with these being packaged as part of PyTorch and installed relatively easily in TensorFlow. This brings computation time down several orders of magnitude, as long as the computations can be carried out in parallel.

Matrix multiplication is a linear function

A matrix multiplication is a linear function. Many linear functions layered on top of each other are still a linear function, just one with different parameters. Therefore however many matrix multiplications layered together are still only a linear function.

Linear functions are a sub-set of affine functions. An affine function is a function where values are multiplied together and summed. A linear function is always affine.

Convolutions used in Convolutional Neural Networks (CNNs) are a particular kind of affine function that work well on auto correlated data, usually images, although also with other domains such as protein to protein interaction, pharmaceutical design. These convolutions are element wise matrix multiplications, rather than dot product multiplications. Element wise matrix multiplications are also linear.

Non-linear activation functions

An activation function is a form of a non-linear function. Traditionally in neural networks a sigmoid function was used as the non-linear activation function. In modern architectures a Rectified Linear Unit (ReLU) is used as an activation function, or one of its variants.

A ReLU sounds complicated and its Wikipedia page attempts to make it complex: https://en.m.wikipedia.org/wiki/Rectifier_(neural_networks) - a ReLU says if the value is less than zero, round the value up to zero. That is as complex as a ReLU gets.

There is a variation called a Leaky ReLU, which is the same as a ReLU other than if the value is less than zero it is multiplied by 0.1 rather than rounded up to zero. It is so called, as it allows a little of values to leak through to the activations.

The choice of activation function sometime isn’t always that significant, only that it is non-linear. If there was no non-linear activation function then a neural network would not be regarded as deep as it is only a linear function. A linear function will never be able to carry out complex tasks such as classifying a movie review or identifying terrain types in satellite images.

These kind of tasks could be performed with large enough matrix multiplications each followed by a non-linearity. With enough of these linear and non-linear layers sandwiched together the network becomes very deep and can produce any arbitrary shape or function which can approximate anything.

The results or outputs of those calculations are called activations, what causes those activations are learnt by training the network and these trained values are coefficients /parameters, usually referred to as weights and biases.

Parameters/weights

The values of those parameters are randomly initialised, or copied from a pre-trained model where transfer learning is in use.

These parameters are trained with Stochastic Gradient Descent (SGD), or other faster versions of SGD, using back propagation as a gradient computing technique. SGD is an optimisation to minimise the loss function.

Stochastic Gradient Descent (SGD), loss functions and back propagation

The loss function evaluates how far the predicted output is from the actual output during training. This can be in a simple mathematical calculation or a complex combination of various factors, different functions are used based on the talks being performed such as classification or regression.

Loss functions are used to evaluate the network’s performance during training, the gradient of the change in the loss is what’s back propagated and used to update the weights to minimise the loss.

The output values of each node are calculated and cached in the forward pass through the network. Then the partial derivative of the error from the loss function with respect to each parameter is calculated in a backward pass through the network’s computational graph.

From the derivative of the error it shows if the loss is increasing or reducing and how by what rate (how steeply), this is then used to update the parameters by a small amount to reduce the loss. A learning rate multiplier is used to scale down the magnitude of the update to attempt to ensure the training isn’t taking steps in the loss space that are too big. This helps to optimise the amount the weights are updated.

This process is SGD.

Classification

For training networks for classification the loss function used is cross entropy. Cross entropy provides a loss based on predicting the correct class and how confident that the prediction is right. This is achieved by applying a formula if the training example is that it is of a class (a value of 1) and a different formula if the training example is not of that class (a value of 0).

Assuming the prediction is between 0 and 1 then:

Training example actual is 1, the loss calculated as is: log(prediction)

Training example actual is 0, the loss is calculated as: log(1 - prediction)

Regression

If the network is used for regression then the training loss function is normally nothing at all, usually the mean square error (MSE) or root mean square error (RMSE) is used as a metric within the loss function.

Last layer activation function

The last output layer in classification represents the class scores, which are arbitrary real-valued numbers, or in regression kind of real-valued target. Varying activation functions are used for these tasks.

Binary classification

A binary classification problem is one that classifies data as in a class or not in that class, such as ‘has a tumour’ or ‘does not have a tumour’. A sigmoid function is usually used as the last layer activation function as it differentiates well between two classes.

A graph of a sigmoid function for: f(x) = 1 / (1 + e^-x)

Graph for a sigmoid function: f(x) = 1 / (1 + e^-x)

A sigmoid function asymptotes to zero and asymptotes to one, outputting values between zero and one. A value of above or below 0.5, indicates which of the two classes the data is predicted to be classified as.

With the shape of the outputs from a sigmoid function, where the activation is less than 0.5 then a sigmoid function will return a lower probability value and if the activation is greater than 0.5 a sigmoid function will return a higher probability value.

The Softmax function, described below, could be used here instead of a sigmoid function. If there are two outputs, with each indicating one of the two classes. It is better to use a sigmoid function here as it’s faster.

Multiple class single label classification

In multiple class single label classification, there are many classes the data can be classified as but it can only be classified with a single label from those classes. An example of this is trying to classify dog breed, the data representing the dog can only have one breed.

The Softmax function is used for multi class single label classification. If the output for a given class i is taken as e^i divided by the sum of e^k where k is each of the classes.

The Softmax function, source: Finn Eggers’s activation function on YouTube.

The Softmax function is extremely good at picking a single label. It outputs a probability distribution where the outputs sum to one. The range of the output for each class is between zero and one. The sum of all the classes’ probabilities is one.

The predicted class is the one having the highest probability output.

Multiple class, multiple label classification

In multiple class, multiple label classification, there are many classes the data can be classified as and the data can be classified with one or more labels from those classes.

For example satellite images could be labelled as different weather and feature classes such as clouds, forest, agricultural, water, lakes and rivers. An image could easily belong to many of these classes.

A sigmoid function is used as there may be multiple labels that have high probabilities for a given data item.

Softmax can’t be used effectively as the sum of the probabilities doesn’t add to one, each class’s probability is between zero and one. This is a common mistake even with experienced practitioners.

Regression

With Regression the output is one number or a set of numbers, depending on the problem. If there is more than one output this is called multivariate regression.

The output values when performing regression are generally unbounded.

Usually with regression there is no last layer activation function, although occasionally in some applications a sigmoid function can be beneficial, for example when using regression for collaborative filtering.