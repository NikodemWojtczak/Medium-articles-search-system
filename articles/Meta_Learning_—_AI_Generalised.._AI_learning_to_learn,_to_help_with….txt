Meta Learning — AI Generalised.

Deep Learning has shown immense success in various fields and is continuing to spread its wings. But one of the major issues with training any traditional neural network model is the requirement of colossal amounts of data, and using this data to perform many iterative updates across many labeled examples.

Let’s take a look at a classic example of cats vs dogs classification. Although over the last two decades, we have made our models better and better to increase the accuracy, but the fundamental problem mentioned above still persists. We still need loads of labelled dogs and cats to get a decent accuracy.

How do humans classify them with much lesser examples. Lets say all of a sudden you are shown two new animals, which are as visually distinguishable as a cat and a dog. I am pretty sure any normal person can get a decent accuracy in less than 100 examples. HOW ?? Well, we have over the years understood the basic structure of animals. We know how to extract features, such as face shape, hair, tail, body structure etc.. In short, we have mastered the art of Learning to learn.

Meta Learning aims to basically learn to learn, and generalise AI to many different scenarios with minimum amount of data. You could say, isn’t transfer learning doing the same. Well, yeah it has gone kind of in the right direction, but it cannot get us far enough. It has been observed that the benefit of a pre-trained network greatly decreases as the task the network was trained on diverges from the target task. Meta Learning suggests framing the learning problem at two levels. The first is quick acquisition of knowledge within each separate task presented. This process is guided by the second, which involves slower extraction of information learned across all the tasks.

Meta Learning algorithms can be broadly classified into three buckets —

Classic Gradient — Descent based methods

The intuition behind this class of methods is to use standard gradient descent updates again for making the neural network, generalise to wide variety of datasets.

In this method, we use a set of datasets each with few examples each, lets say k examples each, for “k-shot learning”. Let the set of datasets be p(T). Let our model be a parametrized function fₜₕₑₜₐ. If we start with parameters θ , we know the model updates with each for the individual dataset with standard gradient descent update.

We want our model to generalise for a wide range of datasets. So we want the sum of errors for all the datasets in p(T) with the updated parameters. This can be expresses mathematically as follows —

For a batch of datasets in p(T), we update θ w.r.t. the meta-objective mentioned above with a standard SGD update —

We can see that back propagating the meta-loss through the model’s gradients involves computing derivatives of derivative. This can be done using Hessian-vector products, supported by TensorFlow.

Nearest Neighbor Methods

This set of methods are guided by the fact that nearest neighbors algorithm does not require any training but performance depends on the chosen metric.

These consist of an embedding model that maps the input domain into a feature space and a base learner that maps the feature space to task variables. The meta-learning objective is to learn an embedding model such that the base learner generalizes well across tasks. Here a distance-based prediction rule over the embeddings. We will look at a specific example, Matching Networks to understand the working.

Matching Networks support set of k examples of image-label pairs S={(xᵢ ,yᵢ)} to a classifier cₛ(x’) which, given a test example x’, defines a probability distribution over outputs y’. S → cₛ(x’) is defined as P(y’|x’,S), where P is parameterised by a neural network. Thus given a new support set of examples S′ from which to one-shot learn, we simply use the parametric neural network defined by P to make predictions about the appropriate label y’ for each test example x’ : P(y’|x’, S′). Simply, we can say —

y cap and x cap — new example

the attention mechanism

The above method is reminiscent of KDE and kNN algorithms.

f and g are appropriate embedding neural networks for x cap and xᵢ, which depend on the tasks. They are deep CNNs for image tasks (like Inception) or a simple form word embedding for language tasks.

Model-based methods using auxiliary space

We, humans along with processing stuff, we also store representations for future use. So these algorithms tried to mimic that by some some auxiliary memory blocks. The basic strategy is to learn the types of representations it should place into memory and how it should later use these representations for predictions.

In these methods, input sequence and output labels are given sequentially. One dataset D ={dₜ}={(xₜ, yₜ)}, where t represents time step. Output label yₜ is not given right after xₜ. Input and ideal label for that input are jumbled for each dataset. It is expected of the model to output the appropriate label for xₜ(i.e., yₜ) at the given time step. So it is forced to store data samples in memory until appropriate labels are found, after which sample-class information can be bound and stored for later use.

The memory module in this specific implementation we are going to talk about is is Neural Turing Machine(NTM). It is basically a turing machine(read and write heads on a memory block) with a LSTM(or sometimes simple neural networks) based controller. Memory encoding and retrieval in a NTM external memory module is rapid, with vector representations being placed into or taken out of memory potentially every time-step. This ability makes the NTM a perfect candidate for meta-learning and low-shot prediction, as it is capable of both long-term storage via slow up-dates of its weights, and short-term storage via its external memory module.

At some time step t, given xₜ, LSTM controller gives out a key kₜ for access the memory matrix Mₜ.

Softmax is used to produce read-write vectors.

This is used to get the memory rₜ.

This is used as input for next controller state as well as input to soft-max based classifiers.