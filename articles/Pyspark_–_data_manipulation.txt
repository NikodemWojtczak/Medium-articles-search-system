Pyspark – data manipulation

Calcul, aggregate, transform any data Alexandre Warembourg · Follow 5 min read · Oct 14, 2019 -- Share

In this article, we will see how to calculate new variables via joins, window functions, UDFs and vector manipulations.

As a reminder, here is the table we use :

.WithColumn and sql function

to create a new column on Spark just pass the function .withColumn and add sql functions

df = (df

.withColumn('dayofyear', F.dayofyear(F.col("ID_DAY")))

.withColumn('Month', F.Month(F.col('ID_DAY')))

.withColumn('ratio_ticket_qty', F.col('F_TOTAL_QTY')/F.col('NB_TICKET'))

)

For more go here : SQL functions

.WithColumn with condition F.when

as with sql it is possible to use the when box for conditional calculations.

df = df.withColumn("day_to_xmas", F.when((F.col("ID_DAY").between("2018-12-01", "2018-12-31")) | (F.col('ID_DAY').between("2019-12-01", "2019-12-31")),

F.lit('xmas_is_coming')).otherwise(F.datediff(F.col("ID_DAY"), F.lit('2018-12-01').cast(DateType())))

)

.WithColumn and Windows function

windows function are very useful to calculate values on time axes or to save us joins

grouped_windows = Window.partitionBy(F.col('SID_STORE'), F.col('Month'))

rolling_windows = (Window.orderBy(F.col("dayofyear").cast(IntegerType())).rangeBetween(-7, 0))

df = (df

.withColumn('rolling_average', F.avg("F_TOTAL_QTY").over(rolling_windows))

.withColumn('monthly_qty', F.avg('F_TOTAL_QTY').over(grouped_windows))

)

Just a join

We can do exactly the same calculation for monthly_qty as with the windows function via a join.