Black garden ants (Lasius niger)

Learning the lesson — The ACO

It is from the early 90s that the biological example of the ant colonies was for the first time translated into a real method for combinatorial optimization problems.

This kind of problem consists of finding the global maximum of a given function within a framework of constraints.

Combinatorial optimization problems can be described by the model:

P=(S, Ω, ƒ)

Where S is a finite set of variables with a specific finite domain, Ω is a set of constraints among the variables and ƒ is the objective function: to be minimize\maximize.

A feasible solution is simply the collection of all decision variables with an assigned value from their domain in such a way that none of the omega constraints are violated.

The solution algorithm will assign to each variable a specific value called solution component;

a single solution is the set of all the solution components.

The search of the optimal solution and the large number of constraints make those kinds of problems hard to solve, also because the exhaustive search is often not possible.

Let us consider for example the traveling salesman problem (TSP).

The problem is the following:

“Given a list of cities and the distances between each pair of cities, what is the shortest possible route that visits each city and returns to the origin city?”.

This is a clear example of combinatorial optimization that often cannot be resolved with an exhaustive search and mathematical exact methods fail to converge in an acceptable amount of time due to a large number of constraints that the mathematical model has to generate and handle.

Throughout the years, many heuristic algorithms have been developed and applied to this kind of NP-hard problems.

Going back to the ACO, we can formalize the TSP problem using the P model introduced before:

First, we associate to each of the nodes representing the n cities a variable X(i) whose domain has n-1 values: j = {1, … , n} where j != i.

On each edge between a pair of cities, we associate a numerical value τ representing the pheromone value.

A set of all X(i) with an assigned domain value is a solution if and only if the set of edges corresponding to those values forms a Hamiltonian cycle (this constraint can be introduced in the ant selection behavior).

Finally, the function (minimize) ƒ computes for each solution the sum of the lengths of the edges.

The algorithm — The metaheuristic

Since its introduction the ACO algorithm has been proposed in many different versions; the following is the general algorithmic idea of the ACO:

begin

initialization();

while (termination condition not met) do

ConstructAntSolutions();

LocalSearch();

GlobalPheromoneUpdate();

end

end

Initially, we start with a series of solution components that will allow us to build up the construction graph, a list of constraints to take into account and some parameters that will be used in the algorithm.

In the initialization step, all the parameters are set and the construction graph is laid out, and an initial pheromone value is assigned to each variable (edge).

In the ConstructAntSolutions phase, a set of m ants start one after the other, to construct a solution by traversing the graph.

Each ant stores its own solution and updates it while traveling along the graph.

In the beginning, all the ants start with an empty solution, and on each construction step, the current solution is extended by choosing one of the nodes representing a feasible component.

Ideally, this process of choosing components and update the partial solution is carried out maintaining solution feasibility, if this is not possible or too hard to maintain, the partial solution can be dropped or penalized, depending on the degree of constraints violation, once it has been completed.

The selection process is implemented computing for each solution component its possibility to be chosen:

Where τ is the pheromone on the arc, η is the heuristic information, for now, we can define it as a special value of each component solution. (It will be described few lines below).

N(s(p)) is the solution components set that can be added maintaining feasibility.

α and ß are parameters with a value fixed in the initialization step that determines the relative importance of the pheromone value and the heuristic information.

Once p has been computed we can apply the so-called pseudorandom proportional rule:

Where r is a random number between 0 and 1 and q0 is a fixed parameter also between 0 and 1 that works as a threshold on r, which allows us to give more emphasis on exploration or exploitation.

In fact, if r is greater or equal q0 the ant will choose randomly between the solution components according to the probability distribution of p (exploration).

The heuristic information is a numerical value computed on the component solution (node) that represents the quality or the importance of that component in terms of the problem specific knowledge.

The heuristic information can be computed a priori in the initialization step or it can be computed at run-time.

For most of the NP-hard problems, the heuristic information is known a priori and it does not change throughout the algorithm execution.

In other cases, the heuristic information depends on the partial solution constructed so far and therefore computed on each step.

In the TSP, for example, η is defined by the length of the edge: d(ij) to the power of minus one.

The LocalSearch step is optional; the purpose of this phase is to improve the solutions obtained by the ants by exploiting the problem knowledge i.e. moving and replacing some solution components with others.

Finally, in the GlobalPheromoneUpdate the pheromone deposit and evaporation process are applied to the graph.

The goal of this phase is to make good solutions more desirable for the next iterations.

This is achieved in two ways, first by increasing the pheromone value on the edges belonging to the best solution or to a list of good solutions, then by simulating the pheromone evaporation.

At this point of the process, we already have a list of complete solutions produced by the ants; moreover, we can evaluate such solutions by the so-called evaluation function or objective function that determines the quality of a given complete solution (the sum of the distances in the case of the TSP).

Also for this phase, there exist many pheromone update approaches, for example, we can decide to strengthen the pheromone of the single best solution so far.

The common approach is more general, for each complete solution of the current iteration; we update each pheromone variable using the following formula:

Where 1 is the fraction of the old pheromone that persists on the edge

(0 < ρ< 1) and 2 is the sum of all the evaluation functions that contain that particular component.

Therefore, if the current edge i-j is not contemplated by any solution then the second term of the formula is zero and the pheromone is updated using the simple evaporation mechanism (1).

After the update of the pheromone, the termination condition is checked to see if the results are good enough, for example, if there is no improvement of the best solution in 100 consecutive iterations we may decide to stop.

Applications and Results

Now that I know what Ant Colony optimization is, what do I get out of it?

The applications of ACO are many and related to different fields, here some examples:

- Routing problems

- Assignment and Scheduling problems

- Classification Rules

- Protein Folding

- DNA sequencing

- Bayesian Network

… and many others

Its wide potential application derived from the flexibility of the whole method to introduce, alongside the pheromone mechanism, a specific problem knowledge in the pheromone distribution and in the evaluation phase.

Last example: Feature Selection

Before concluding this glimpse on the ACO metaheuristic is worth to show an example of its application on a common research area in data science: feature selection.

The problem of high-dimensional spaces is a common issue that affects many datasets used for numerical analysis, machine learning, data mining, etc.

Leaving aside the problems that a high-dimensional space can entail (learn more about it), is interesting how ACO can be used as feature reduction method.

Starting with a list of n features, we want to reduce the entire set, searching for a subset that contains the most important and representative features.

First, since the ACO is a graph based metaheuristic we need to lay out the entire graph.

Each node of the graph represents a feature of the initial n feature set, and each edge represents a choice from the actual feature to the other.

Each ant will start with an empty set of feature and travel through the graph visiting a minimum number of nodes that can satisfy the traversal-stopping criterion, and finally output a candidate subset.

Dotted lines: possible feature to add, Bold lines: ant feature selection

As shown in the figure, an ant started on node A performed a route up to node F and then stop having built the following subset {A, B, C, D, F} that satisfy the traversal stopping criterion (e.g. suitably high classification accuracy).

The heuristic information can be computed by any appropriate metric function, for example, an entropy-based measure can be very effective.

The ant selection process and the pheromone update is performed with the same approach described.

The whole process follows these steps: m ants are generated and placed randomly across the graph.

From these random positions, each ant starts to construct a path by traversing the edges until the traversing stop criterion is satisfied.

All the subset are collected and evaluated; if one of these subsets is good enough or the iterations have been executed a certain number of times, the process stops and the best subset found is returned.

If none of these stopping conditions holds, then the pheromone is updated and a new group of ants is generated and a new iteration starts.

Conclusions

As described the ACO is born from the observation of the animal world, in particular from the observation of large groups of single simple organisms such as the ants.

This kind of intuitions derived directly from nature is the evidence that the animal world can be a source of inspiration and knowledge.

The ACO metaheuristic represents nowadays a well-known and solid approach to a multiplicity of problems; it exists in a variety of flavors with simple or very complex formalizations.

One of the strongest points of the ACO is its ability to discover rapidly good solution; this advantage is counterbalanced sometimes by its premature convergence.

Even though is not applicable to continuous problems the ACO is surely an effective solution for many optimization/discrete problems that deserve to be included in a data scientist toolbox.