A wrapper that simplifies task definition

Our wrapper (https://github.com/TheMTank/cups-rl) attempts to make AI2Thor usable as an OpenAI gym environment by implementing the corresponding gym env sub-class. We also include a few examples of tasks, agent training, and scripts to simplify the learning process. We are constantly improving it to provide a better user experience, but this is iterative, so please bear with us.

Gym environments provide just the right amount of abstraction so that running your experiments can be done in the usual RL fashion by calling the reset function at the start of each episode and the step function to give your action to the environment at every step, similar to below:

This is very powerful if you want to use the environment “as is”, but we found it problematic when trying to make it customisable to many tasks while keeping this simple interface. Following gym recommendations, we should define an environment class as generic as possible and then inherit from one or several of these base classes to customise the environment for a specific task.

We thought this procedure might be over-complicated for our use cases, and we decided to take a different approach to make it more intuitive for us. We hoped that what made sense to us, might be easier to understand for the end user too. We believe this is more scalable when aiming for hundreds of tasks and innumerable “task variations”, but still wanting to keep the important, common parts of the environment, shared between all tasks.

Same interface, more control

The natural divide between base environment and the specific task within that environment led us to two main classes: ‘AI2ThorEnv’ and ‘BaseTask’. The goal here is to decouple our wrapper to AI2Thor from the details of the reward functions, and the specifics of resetting the environment for the specific task.

The former includes details of the scene, objects that will be taken into account, image details like resolution or grayscale format. The latter includes the initialisation/reset conditions necessary for that particular task as well as the computation of the rewards observed at each step.

Why did we do this?

Well this way the user can customise and create the task independent of the environment specifics. We achieve this by subclassing the base task to their goals, and relying on the underlying environment code to function and remain unchanged across tasks. At the same time, we ensure that the user doesn’t have to divide his mind into thinking about environment and tasks independently for the experiment definition. We achieve this by using a single config file to specify the environment and task parameters.

As an example, if we wanted to change the example task given on the repository of a task to pick up cups, instead of creating a new subclass from the environment and modifying the step and reset functions (potentially adding large amounts of boilerplate and spaghetti code and maybe a few bugs) as is usually done within gym environments, we would create a new task class as follows:

And then easily modify the config file to change the environment and task conditions within seconds:

Change the task to pick up (and put down) apples as fast as possible instead

Task and config combinations allow “task variations”, e.g. PickUpTask allows you to specify “Apple” or “Cup” picking or both; each of these is the same task but a specific variation on it.

If all of this talk of wrappers, tasks and generalisable code interfaces got you excited, then don’t hesitate to try it out. Feel free to add an issue, or your own pull request, including new tasks so that you can compare your results with other people running the same kind of experiments. You can always drop us an email with any questions or for clarifications.