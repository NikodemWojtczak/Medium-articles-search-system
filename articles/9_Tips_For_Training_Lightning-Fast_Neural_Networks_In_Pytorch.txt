Letâ€™s face it, your model is probably still stuck in the stone age. I bet youâ€™re still using 32bit precision or *GASP* perhaps even training only on a single GPU.

ðŸ˜±.

I get it though, there are 99 speed-up guides but a checklist ainâ€™t 1? (yup, that just happened). Well, consider this the ultimate, step-by-step guide to making sure youâ€™re squeezing all the (GP-Use) ðŸ˜‚ out of your model.

This guide is structured from simplest to most PITA modifications you can make to get the most out of your network. Iâ€™ll show example Pytorch code and the related flags you can use in the Pytorch-Lightning Trainer in case you donâ€™t feel like coding these yourself!

Who is this guide for? Anyone working on non-trivial deep learning models in Pytorch such as industrial researchers, Ph.D. students, academics, etcâ€¦ The models weâ€™re talking about here might be taking you multiple days to train or even weeks or months.

Weâ€™ll cover (from simplest to most PITA)

Using DataLoaders. Number of workers in DataLoader. Batch size. Accumulated Gradients. Retained graphs. Moving to a single GPU. 16-bit mixed-precision training. Moving to multiple GPUs (model duplication). Moving to multiple GPU-nodes (8+GPUs). My tips for thinking through model speed-ups

Pytorch-Lightning

You can find every optimization I discuss here in the Pytorch library called Pytorch-Lightning. Lightning is a light wrapper on top of Pytorch that automates training for researchers while giving them full control of the critical model parts. Check out this tutorial for a more robust example.

Lightning uses the latest best practices and minimizes the places where you can make a mistake.

Weâ€™ll define our LightningModel for MNIST here and fit using the Trainer.