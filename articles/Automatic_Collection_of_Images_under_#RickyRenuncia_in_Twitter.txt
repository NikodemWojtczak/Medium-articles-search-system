Development Requirements

To run the code, you need Python 3 installed on your computer. You also need a Twitter developers account, which you can create here. To create a Twitter developer account, you need a Twitter account.

These are the libraries you’ll be using:

python-twitter : Used to easily connect to the Twitter API and search for tweets containing the hashtag #RickyRenuncia. Install by running pip3 install python-twitter .

: Used to easily connect to the Twitter API and search for tweets containing the hashtag #RickyRenuncia. Install by running . urllib.request : Used for opening and reading content of URLs. Python has this library by default.

: Used for opening and reading content of URLs. Python has this library by default. os : Used for operating system dependent functionality. We used it to ensure that we are not saving images with the same Twitter ID more than once. Python has it by default.

: Used for operating system dependent functionality. We used it to ensure that we are not saving images with the same Twitter ID more than once. Python has it by default. pandas: Used to create a table containing information on the downloaded images and creating a CSV file from it.

Connect to Twitter API

Search for your credentials on your Twitter Developer account. You need the following:

consumer key

consumer secret

access token

access token secret

To get these values, you must have an app created under your Twitter Developer account, and then access it. To get to your existing app, first click the down arrow to the right of your username on the far right side of the Twitter Developer page, then click on the option titled “Apps”. Once there, click on the button “Details” on the app of interest. Then click on “Keys and Tokens”. Under “Consumer APIs” you will find the consumer key and consumer secret, and under “Access token & access token secret” you will find the access token and access token secret.

If you are keeping your code in a version control system (VCS) such as GitHub or BitBucket, I recommend saving your API credentials in a separate file from your code and adding that file to the .gitignore file so you don’t accidentally push your credentials to your repository.

We’ll follow the following format for the configuration file which will contain the Twitter API credentials. The script will have one dictionary variable named credentials , containing the keys consumer_key , consumer_secret , access_token , and access_token_secret . We name the file config.py , but it can have any other name.

Set Twitter API credentials.

To connect to the Twitter API using the library python-twitter and the credentials stored in config.py , run the following:

Connect to the Twitter API.

Build Search Query

You want to use the Standard search API, which returns a collection of relevant Tweets matching a specified query. Then you need to define what you want to search for.

We want tweets with the hashtag ‘rickyrenuncia’. We want both popular and non-popular tweets, and we want to limit these tweets to be from July 20 to July 23. We want to get the maximum number of tweets allowed by the API per call, which is 100.

There are various parameters you can specify when calling the endpoint. Here are the query parameters we use:

q : required. Expects a UTF-8, URL-encoded search query. Allows a maximum of 500 characters. Can include operators. We set it to rickyrenuncia .

: required. Expects a UTF-8, URL-encoded search query. Allows a maximum of 500 characters. Can include operators. We set it to . result_type : optional. What type of search results you want to receive. Default value is mixed . The valid values are: mixed , recent , popular . mixed includes both popular and real time results in the response. recent only includes the most recent results. popular only includes the most popular results. We set it to mixed .

: optional. What type of search results you want to receive. Default value is . The valid values are: , , . includes both popular and real time results in the response. only includes the most recent results. only includes the most popular results. We set it to . include_entities : optional. Whether to include the username of the person who wrote the tweet or not. Default value is false . We set it to true .

: optional. Whether to include the username of the person who wrote the tweet or not. Default value is . We set it to . with_twitter_user_id : optional. Whether to include Twitter ID of user who wrote tweet or not. Default value is false . We set it to true .

: optional. Whether to include Twitter ID of user who wrote tweet or not. Default value is . We set it to . since : optional. Returns tweets created after given date. Accepted date format is YYYY-MM-DD . We set it to 2019-07-20 .

: optional. Returns tweets created after given date. Accepted date format is . We set it to . until : optional. Returns tweets created before given date. Accepted date format is YYYY-MM-DD . We set it to 2019–07–23 . [ IMPORTANT : The search index has a 7-day limit! No tweets will be found for a date older than one week. There are workarounds to this which are not discussed in this blog.]

: optional. Returns tweets created before given date. Accepted date format is . We set it to . [ : The search index has a 7-day limit! No tweets will be found for a date older than one week. There are workarounds to this which are not discussed in this blog.] count : optional. Number of tweets to return per page. Maximum allowed is 100 . Default is 15. We set it to 100.

Define query parameters.

Build query.

Our resulting query is:

q=rickyrenuncia&result_type=mixed&include_entities=true&with_twitter_user_id=true&since=2019–07–20&until=2019–07–23&count=100

Run Search Query

To run the search query, you run the following:

How to make a call to the search endpoint of the Twitter API.

The search endpoint will only provide a maximum of 100 results. To get more we must call the search endpoint multiple times with different max_id values. According to the documentation:

To use max_id correctly, an application’s first request to a timeline endpoint should only specify a count. When processing this and subsequent responses, keep track of the lowest ID received. This ID should be passed as the value of the max_id parameter for the next request, which will only return Tweets with IDs lower than or equal to the value of the max_id parameter. Note that since the max_id parameter is inclusive, the Tweet with the matching ID will actually be returned again.

As the documentation states, to retrieve more tweets we need to specify the max_id as the smallest tweet ID returned, and continue repeating this until we get all the tweets we want.

We decided to make as many consecutive calls as the Twitter API would permit. We found that we could make 180 back-to-back calls and retrieve 18000 tweets in one go. Feel free to experiment with different numbers of iterations.

In the following code we initialize two empty arrays, one called all_results in which we will store all API search results and one called IDs in which we will store the IDs of the tweets in order to keep track of the smallest ID for specifying max_id on subsequent calls. IDs is re-assigned in every loop. We initialize max_id as None because in the first API call we should not specify a value for max_id . After the first iteration the max_id will be defined using the smallest ID present in the list IDs , and the query string will be extended to contain the max_id parameter. In later iterations the section of the query string containing the max_id parameter is replaced with a new max_id value.

After running the loop we will have 18000 tweets stored in all_results . About 179 tweets will be repeated since the max_id query parameter is inclusive, as specified in the documentation. We choose to keep the repetitions.

Make 180 calls to the search endpoint for a total of 18000 results.

Download Media from Tweets

Now to download the media (photos) in every read-in tweet.

Each element in all_results is an object of the class Status from the python-twitter package. The Status class represents the Status structure used by the twitter API. It has various attributes containing information on the Tweet. We only use the media attribute in order to get the URLs of images under the tweet.

The media attribute is a list of objects of class Media from the python-twitter package. The Media class represents the media component of a tweet. It contains various attributes related to the media. We use the attributes id and media_url . id is the ID of the image in Twitter, and media_url is the URL of the image/video present in the tweet. We use the IDs for assigning names to the media we download, in order to keep track of the images’ origins.

Create a folder named downloaded_media in the folder in which you are running your code, and run the following:

Function for creating URL of tweet where downloaded image comes from.

Download images and keep track of image origins by saving image ID, image URL, and tweet URL.

downloaded_img_ids contains the file names present in the folder downloaded_media . We use this to make sure we don’t re-download images, since some of the tweets read-in may be retweets which share media we already downloaded.

To create a CSV file with information of the images’ origins (image ID, image URL, tweet URL), we use the Pandas library to create a dataframe from the image_origins dictionary, remove rows with duplicate image IDs if any, and create a CSV file from it.

Create CSV with information on the images’ origins.

Resulting CSV file will be saved in the location in which you ran the code, as a file named image_origins.csv .

Results

Images will now be present in your download_media folder within the folder in which you ran the code. We downloaded 895 images.

Subset of images downloaded from the Tweets the Search API returned on July 25.

The images we downloaded are available here under the folder titled downloaded_media . A CSV and a Spreadsheet with information on the origins of the images (a column for image ID which is associated with the file names, a column for image URL, and a column for tweet URL) are present there too, titled image_origins .

Conclusion

In this article we show how to retrieve tweets under a certain hashtag in a certain time frame and how to download the media files under those tweets. We provided a link to a folder in Google Drive containing a folder with the downloaded images and a Spreadsheet with information on the origins of the images.

We unfortunately encountered the limitation that the Twitter API does not allow for retrieval of posts older than 7 days of calling the endpoint. This makes the results of the code discussed here not reproducible (the code runs and works, but it won’t return images from the specified time frame). However, there are alternate ways of retrieving historic Tweets. There are repositories in GitHub which solve this problem. One of these repositories is the following:

In a future blog we may discuss how to use one of these repositories in order to retrieve historic Twitter data.

Now that we have photos from the #RickyRenuncia hashtag in Twitter, and from a subset of the dates of the protests, we can handpick images of crowds and use them in order to count the number of people in them.