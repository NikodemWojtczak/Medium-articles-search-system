Language Models (LMs) estimate the relative likelihood of different phrases and are useful in many different Natural Language Processing applications (NLP). For example, they have been used in Twitter Bots for ‘robot’ accounts to form their own sentences.

In this post, we will first formally define LMs and then demonstrate how they can be computed with real data. All the methods shown are demonstrated fully with code in the following Kaggle notebook.

https://www.kaggle.com/osbornep/education-learning-language-models-with-real-data

Part 1: Defining Language Models

The goal of probabilistic language modelling is to calculate the probability of a sentence of sequence of words:

and can be used to find the probability of the next word in the sequence:

A model that computes either of these is called a Language Model.

Initial Method for Calculating Probabilities

Definition: Conditional Probability

let A and B be two events with P(B) =/= 0, the conditional probability of A given B is:

Definition: Chain Rule

In general cases, the formula is as follows:

The chain rule applied to compute the joined probability of words in a sequence is therefore:

For example:

This is a lot to calculate, could we not simply estimate this by counting and dividing the results as shown in the following formula:

In general, no! There are far to many possible sentences in this method that would need to be calculated and we would like have very sparse data making results unreliable.

Methods using the Markov Assumption

Definition: Markov Property

A stochastic process has the Markov property if the conditional probability distribution of future states of the process (conditional on both past and present states) depends only upon the present state, not on the sequence of events that preceded it. A process with this property is…