Age, BMI, MCP-1 appear to be normally distributed. Glucose, Insulin, HOMA, Leptin, Adiponectin, and Resistin have a right skew distribution. If you look at HOMA and Insulin you will see a strong positive linear relationship in the scatter plot. Meaning, the relationship between those two variables can be accurately modeled using a straight line regression model. The same goes for Insulin and HOMA. BMI and Glucose seem to have a solid cluster separation where we can see the two classes existing in their own areas. These two variables could, later, be strong features if used in a KNN type model.

Let’s look to see if any of these bio-markers are highly correlated with one another. If we try to train a machine learning model using variables with no or little correlation our model will return inaccurate results. To make a correlation matrix heat-map I bring in the Python package ‘seaborn.’ It visualizes the correlation coefficient that calculates the positive or negative linear strength of the relationship between two variables.

Values close to 1.0, like 0.93 and 0.7, show a strong positive linear relationship between the two variables while values close to -1.0 show the inverse, strong but negatively correlated.

BMI and Leptin have a positive correlation value of 0.57

Glucose and HOMA have a moderately strong positive correlation value of 0.7

Glucose and Insulin have a positive correlation value of 0.5

Insulin and HOMA have a very strong positive correlation value of 0.93

Glucose, Insulin, HOMA, and Resistin are all positively correlated with the classification variable at a level above 0.2

All variables are, in some way, correlated with one another to some degree

Next I test out various classification models provided by the sklearn package and, upon determining the best one, I will investigate its architecture. I use the train_test_split function from the sklearn package to create training and test data sets, as well as apply scaling. It’s important to note that the numerical values do not all have the same scale, some are measured in mg/dL, pg/dL, and kg/m², so therefore applying a scaling function is a critical pre-processing step before feeding the data to an ML model.

The first model I trained on the data was a logistic regression, a simple classification method. Initially, it returned an accuracy of 62% on the test data with 8 false negatives (missed 8 cancer samples) and 3 false positives (3 healthy samples classified as cancer). The model returned a 57% precision value and 33% recall value for predicting healthy classes. As a result of the small sample size, the results are preliminarily low. A good remedy for low sample sizes in classification problems is to change the loss function. Classification problems use cross-entropy loss to train and optimize the model, with a low sample size the model becomes biased towards one class as it tries to optimize its loss value. To remedy, I add weight values to the losses that correspond with different classes to even out the bias. This is easily done by adding the argument class_weight = ‘balanced’ which finds the class weights and balances the data ratio to remove bias. I run another logistic regression model using the balanced class weights argument.

This returns an accuracy of 69% on the test set, a 7% increase, and we also eliminated 3 of the false negatives.

Next, I trained a linear-discriminant analysis model, a single decision tree model, and a k-nearest-neighbors model next using both balanced and non-balanced class weight arguments. The best model was the KNN with an accuracy of 79% and only 3 false negatives and 3 false positives.

I attempted tuning the k-nn by finding the optimal value of k. The max accuracy I could achieve was 79%, though, so there was no improvements to be made there.

This visualizes the attempt to find the best value for k based on accuracy. We see that 1, 5, 7 are the best values for k when optimizing for accuracy.

An important part of this project to me is the identification of bio-markers. I chose to train a random forest classifier so we could receive a variable importance output and look at what bio-markers are considered the ‘most important,’ statistically speaking, in determining if a sample has cancer or not.

A random forest is an aggregation of many single decision trees. It takes the majority result of the decision trees as the final value. The initial random forest model performed horribly with an accuracy of 58.6%. I implemented a randomized search using cross validation to identify the best hyper-parameters for the random forest model in hopes of achieving better accuracy and a more transparent machine model.

This code builds a random search to identify the best parameters for the random forest to make its classifications. The following code uses the above defined random grid search, runs 100 different combinations of models, and identifies the best one.

These are the best parameters:

{‘n_estimators’: 266, ‘min_samples_split’: 5, ‘min_samples_leaf’: 1, ‘max_features’: ‘sqrt’, ‘max_depth’: 30, ‘bootstrap’: True}

Using the best possible random forest model we achieve an accuracy of 68.97%. This score is on par with the logistic regression’s results and performs worse than the KNN. The most useful output of the random forest, in my opinion, is the feature importance. It shows what variables were, statistically speaking, the most important in making the classification. This output makes the random forest model less of a ‘black box’ of abstraction and allows us to take a look at how its making certain decisions.

Based on the variable importance results above, I take the two most important features and visualize them together and apply the best performing model thus far, the KNN.

In the code chunk above I set the value of K to one of the values I identified earlier that attained the best accuracy, in this case that is K = 5. I then fit a k-nearest-neighbors model to just the two features: Glucose and BMI.

Red = Healthy Controls. Green = Breast Cancer Patients

This visualization is useful for making inferences about our predictor model. At around 90 (mg/dL) Glucose level there is a vertical cut off where some samples from either class intermingle. After 120 (mg/dL) all samples are breast cancer patients.

Conclusion

Glucose and BMI, together, could be two strong bio-markers for detecting breast cancer. Modelling these two bio-markers, and adding HOMA and Resistin levels (the next two features of importance), I achieved almost 80% accuracy in classifying breast cancer samples with the k-nearest-neighbors method. So, this project shows that combining these four important bio-markers breast cancer could be classified with some ease. If the data-set could be expanded upon, other methods could prove to be even more accurate, like the random forest. Adding another hundred or so samples has the potential to return even better results than the 80% accuracy of the KNN Classifier. These models all run extremely quick, too, and therefore are practical for medical applications if we could negate the false positive/false negative predictions that are most likely a result of bias due to the low sample size. This article aimed to show the influence small sample sizes can have on powerful machine learning methods and how easily applied methods can be used for medical diagnosis.