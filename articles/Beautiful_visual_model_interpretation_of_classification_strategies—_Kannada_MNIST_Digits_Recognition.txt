Beautiful visual model interpretation of classification strategies— Kannada MNIST Digits Recognition

Kannada handwritten digits

The Kannada MNIST dataset is a great recent work (details here), and I’m delighted that it is available to the public as well. I’m sure pretty soon the community here would be posting state of the art accuracy numbers on this dataset. Which is why, I’m doing something different.

Instead, we will try to visualize, try to see what the model sees, assess things pixel by pixel. Our goal would be interpretability. I’ll start with the ‘simplest’, easiest to interpret algorithm in this article. Hopefully, I’ll post results with other modeling techniques in later article.

To reiterate and clarify: I will not be focusing on getting best possible performance. Rather, I’ll focus on visualizing the output, making sense of the model, and understanding where it failed and why. Which is more interesting to assess when the model isn’t working extremely well. :)

Visualizing the digits data

Function to plot one random digit along with its label

def plot_random_digit():

random_index = np.random.randint(0,X_train.shape[0])

plt.imshow(X_train[random_index], cmap='BuPu_r')

plt.title(y_train[random_index])

plt.axis("Off") plt.figure(figsize=[2,2])

plot_random_digit()

A random Kannada digit plotted as image

Looking at 50 samples at one go

plt.figure(figsize=[10,6])

for i in range(50):

plt.subplot(5, 10, i+1)

plt.axis('Off')

if i < 10:

plt.title(y_train[i])

plt.imshow(X_train[i], cmap='BuPu_r')

As someone who is not good at reading Kannada script, to me the symbols seem somewhat similar for -

3 and 7

6 and 9

At the onset, I would expect that the predictors could be somewhat confused between these pairs. Although this isn’t necessarily true — maybe our model can identify the digits better than I can.

Reshaping the datasets for predictive model building

The individual examples are 28 X 28. For most predictive modeling methods in scikit learn, we need to get flatten the examples to a 1D array.

We’ll use the reshape method of numpy arrays.

X_train_reshape = X_train.reshape(X_train.shape[0], 784)

X_test_reshape = X_test.reshape(X_test.shape[0], 784)

Building and understanding the Logistic regression model

Let’s build a Logistic regression model for our multiclass classification problem.

Note again that we’ll not be focusing on getting the best possible performance, but on how to understand what the model has learnt.

A logistic regression model will be easy and interesting to analyse the coefficients to understand what the model has learnt.

The formulation of a multi-class classification can be done in a couple of ways in SciKit-learn. They are -

One vs Rest

Multinomial

1. One vs Rest:

Also known as one-vs-all, this strategy consists in fitting one classifier per class. For each classifier, the class is fitted against all the other classes. One advantage of this approach is its interpretability.

Since each class is represented by one and one classifier only, it is possible to gain knowledge about the class by inspecting its corresponding classifier. This is the most commonly used strategy for multi-class classification and is a fair default choice.

For our case, it would mean building 10 different classifiers.

Read more about it here:

https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html

2. Multinomial:

In this strategy, we model the logarithm of the probability of seeing a given output using the linear predictor.

For multinomial the loss minimised is the multinomial loss fit across the entire probability distribution. The softmax function is used to find the predicted probability of each class.

Read more about this here:

https://en.wikipedia.org/wiki/Multinomial_logistic_regression#As_a_log-linear_model

Note: This distinction is important, and needs you to interpret the coefficients differently for the models.

First, let’s built our model using the One vs. Rest scheme

from sklearn.linear_model import LogisticRegression

lr1 = LogisticRegression(solver="liblinear", multi_class="ovr")



# Fitting on first 10000 records for faster training

lr1.fit(X_train_reshape[:10000], y_train[:10000])

Assessing performance on the train set

The predictions of the model for the training data

from sklearn.metrics import confusion_matrix, accuracy_score, classification_report

y_train_pred = lr1.predict(X_train_reshape[:10000]) cm = confusion_matrix(y_train[:10000], y_train_pred[:10000])



plt.figure(figsize=[7,6])

sns.heatmap(cm, cmap="Reds", annot=True, fmt='.0f')

plt.show()

That’s VERY high training accuracy! Overfitting?

Also, looks like the model is NOT very confused between 3 and 7, 6 and 9, at least not on the train set.

Error Analysis: Checking out the mis-classified cases

We’ll covert to a Pandas series for ease of indexing, isolate the mis-classification cases, plot some examples.

11 cases were mis-classified

Studying some cases

Picking 9 random cases — we’ll plot the digits, along with the true and predicted labels

The mis-classified cases

Can you see why the model was confused?

Let’s see how the model fares on the test set.

Confusion matrix on the test set

Making predictions on the test data, and plotting the confusion matrix.

Confusion Matrix on test set — smells like over-fitting

Looking at the confusion matrix and the classification report -

Recall is least for 3, 7 — model is confused between them significantly. Similarly, there is confusion between 4 and 5. Also, many 0s are mistaken for 1 and 3.

Okay! So it looks like the performance has fallen sharply on the test set. There’s a very good chance we’re over-fitting on the train set.

We acknowledge that the model could be improved.

But, let’s not worry about that for now. Let’s focus on the way to understand what the model learnt.