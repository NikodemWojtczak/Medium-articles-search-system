Core requirements to understand this article

Linear Algebra

Multi-variable calculus

Basic idea of convex functions

As we all know, optimization is one of the most important factors in machine learning. Thus it is of our interest to find an algorithm that optimizes functions in a reasonable time. One of the most common algorithms used today is gradient descent. Today, we will take a look at other optimization algorithms and have a theoretical understanding of them.

The core algorithms that will be discussed in this article are:

Newton’s method

Steepest Descent

Gradient Descent

You can learn more about these algorithms from the textbook Convex Optimization: Part 3. We will mostly focus on Quadratic/Polynomial functions in this article

An assumption made on our functions

We always make the assumption that the functions we are dealing with along with their derivatives are continuous (i.e. f ∈ C¹). In the case of Newton’s method, we also need to assume that the second derivatives are continuous. (i.e. f ∈ C²). The last assumption that we make is that the functions we are trying to minimize are convex. Thus, if our algorithm converges to a point (typically known as the local minimum), then we are guaranteed that it is a global optimizer.

Newton’s method

Algorithm for a single-variable function

x_n = starting point

x_n1 = x_n - (f'(x_n)/f''(x_n))

while (f(x_n) != f(x_n1)):

x_n = x_n1

x_n1 = x_n - (f'(x_n)/f''(x_n))

The idea behind Newton’s method is that the function f being minimized is approximated locally by a quadratic function. We then find the exact minimum of that quadratic function and set the next point to be that. We then repeat the procedure.

The case for multi-variable functions

So far, it looks like the Newton’s method is a solid candidate. But typically, we won’t be dealing with a lot of single variable functions. Most of the time, we will be need to optimize functions that have a lot of parameters (i.e., functions in ℝn). So here is the algorithm for the multi-variable case:

Assuming x∈ ℝn , we have:

x_n = starting_point

x_n1 = x_n - inverse(hessian_matrix) (gradient(x_n)) while (f(x_n) != f(x_n1)):

x_n = x_n1

x_n1 = x_n - inverse(hessian_matrix) (gradient(x_n))

where gradient(x_n) is the gradient vector at x_n and hessian_matrix is an n×n hessian symmetric matrix whose entries consist of second derivatives at x_n .

As we all know, inverting a matrix is expensive ( O(n³) ) and thus this method is not commonly used.

Gradient Descent

This is by far the most popular method of optimization used in machine learning and other approximate optimizations. It is an algorithm that involves taking a step in the direction of the gradient at every iteration. It also involves a constant alpha that factors the size of the step to be taken at every iteration. Here is the algorithm:

alpha = small_constant

x_n = starting_point

x_n1 = x_n - alpha * gradient(x_n) while (f(x_n) != f(x_n1)): # May take a long time to converge

x_n = x_n1

x_n1 = x_n - alpha * gradient(x_n)

Here, alpha is a value that you have to pick to update x_n at every iteration (This is called a hyperparameter). We will analyze the values for alpha we pick

If we choose a large value for alpha, we tend to overshoot and go further from the optimizer point. In fact, you can diverge if you choose it to be too big.

Gradient Descent after 10 iterations with an alpha value that is too large.

On the other hand, if we choose alpha to be small, then it will take a lot of iterations to converge to the optimum value. As you get closer to the optimum value, the gradient tends to approach to zero. So, if your alpha value is too small then it may take almost forever to converge to the minimum point.

Gradient Descent after 10 iterations with an alpha value that is too small.

Thus, as you can see, you have a burden of choosing a good constant for alpha. However, if you choose a good alpha, then you can save a lot of time at every iteration

Gradient Descent after 10 iterations with an alpha value that is good enough.

Steepest Descent

Steepest descent is very similar to gradient descent, except it’s more rigorous in that the steps taken at every iteration are guaranteed to be the best steps. Here’s how the algorithm works:

x_n = starting_point

alpha_k = get_optimizer(f(x_n - alpha * gradient(x_n)))

x_n1 = x_n - alpha_n * gradient(x_n) while (f(x_n) != f(x_n1)):

x_n = x_n1

alpha_k = get_optimizer(f(x_n - alpha * gradient(x_n)))

x_n1 = x_n - alpha_n * gradient(x_n)

where x_n and x_n1 are the input vectors in ℝn , gradient is the gradient of f at x_n and alpha_k is such that:

So while optimizing our original function, we need to optimize an inner function at every iteration. Now the good news is that this function is a single-variable function, which means that it’s not as complicated (for example we can use Newton’s method here). However, in most cases, it does tend to get a bit expensive to optimize such a function at every step.

A special case for Quadratic Functions

Consider the squared error loss function:

where I is the identity matrix and y=Qw+b

For simplicity, we’ll only consider finding the optimal value of the weight w (assume b is constant). Through substituting y and simplifying everything, we get the following:

Taking a look back at g(α), we know that if we take the gradient at αk, it should be 0 since it is the minimizer. Taking advantage of that, we have the following:

Simplifying the above mess and substituting for the gradient of f at the two points, we get the following for αk

Now we have a concrete value for αk in the case of quadratic functions.

Convergence analysis for quadratic functions

In an example for a quadratic function in ℝ², steepest descent typically tends to get very close to the optimal within less than ten steps.

Steepest descent in 2 dimensions after 4 iterations.

In the above figure, notice that the change in direction is perpendicular at every iteration. After 3 to 4 iterations, we notice that the change in derivatives is almost negligible.

Why isn’t steepest descent used?

So why isn’t this algorithm used quite often? It clearly gets rid of the need of a hyperparameter to tune and it is guaranteed to converge to a local minimum. The issue here is that at every iteration, we need to optimize alpha_k which is a little expensive considering that we have to do it at every step.

For example, in the case of the quadratic function, we had to compute multiple matrix multiplications and vector dot products at every iteration. Compare this to Gradient Descent, where at every step, we just need to compute the derivative and update the new value which is way less expensive.

Steepest descent is also difficult to generalize in the case of non-quadratic functions where there may not be a concrete value for alpha_k

A comparison between Gradient Descent and Steepest Descent

We will make a comparison between gradient descent and steepest descent and analyze their time complexities. First, we will make a comparison in timing between the two algorithms. We will create a quadratic function f:ℝ²⁰⁰⁰→ℝ (Which involves a 2000×2000 matrix). We will then optimize the function and restrict the number of iterations to 1000. Then, we will compare the time taken and how close value x_n is is to the optimizer between the two algorithms.

Let’s take a look at steepest descent first:

0 Diff: 117727672.56583363 alpha value: 8.032725864804974e-06

100 Diff: 9264.791000127792 alpha value: 1.0176428564615889e-05

200 Diff: 1641.154644548893 alpha value: 1.0236993350903281e-05

300 Diff: 590.5089467763901 alpha value: 1.0254560482036439e-05

400 Diff: 279.2355946302414 alpha value: 1.0263893422517941e-05

500 Diff: 155.43169915676117 alpha value: 1.0270028681773919e-05

600 Diff: 96.61812579631805 alpha value: 1.0274280663010468e-05

700 Diff: 64.87719237804413 alpha value: 1.027728512597358e-05

800 Diff: 46.03102707862854 alpha value: 1.0279461929697766e-05

900 Diff: 34.00975978374481 alpha value: 1.0281092917213468e-05

Optimizer found with x = [-1.68825261 5.31853629 -3.45322318 ... 1.59365232 -2.85114689 5.04026352] and f(x)=-511573479.5792374 in 1000 iterations

Total time taken: 1min 28s

Here’s the output from gradient descent, with alpha = 0.000001

0 Diff: 26206321.312622845 alpha value: 1e-06

100 Diff: 112613.38076114655 alpha value: 1e-06

200 Diff: 21639.659786581993 alpha value: 1e-06

300 Diff: 7891.810685873032 alpha value: 1e-06

400 Diff: 3793.90934664011 alpha value: 1e-06

500 Diff: 2143.767760157585 alpha value: 1e-06

600 Diff: 1348.4947955012321 alpha value: 1e-06

700 Diff: 914.9099299907684 alpha value: 1e-06

800 Diff: 655.9336211681366 alpha value: 1e-06

900 Diff: 490.05882585048676 alpha value: 1e-06

Optimizer found with x = [-1.80862488 4.66644055 -3.08228401 ... 2.46891076 -2.57581774 5.34672724] and f(x)=-511336392.26658595 in 1000 iterations

Total time taken: 1min 16s

As you can see, gradient descent tends to be faster although not by a lot (seconds or minutes). But more importantly, although the value for α was not chosen as the best parameter in gradient descent, steepest descent takes steps that are much better than gradient descent. In the above example, the difference between f(xprev) and f(xcurr) in the 900th for gradient descent was 450. That difference was passed way earlier in steepest descent (Roughly between iteration 300 and 400).

Thus, if we factor in taking only 300 iterations for steepest descent, we get the following:

0 Diff: 118618752.30065191 alpha value: 8.569151292666038e-06

100 Diff: 8281.239207088947 alpha value: 1.1021416896567156e-05

200 Diff: 1463.1741587519646 alpha value: 1.1087402059869253e-05

300 Diff: 526.3014997839928 alpha value: 1.1106776689082503e-05 Optimizer found with x = [-1.33362899 5.89337889 -3.31827817 ... 1.77032789 -2.86779156 4.56444743] and f(x)=-511526291.3367646 in 400 iterations

Time taken: 35.8s

Therefore, steepest descent is actually faster. It just goes to show that you really need fewer steps per iteration if you want to approximate the optimum. In fact, if your goal is to approximate the optimum, then steepest descent generates values closer to the optimum for small dimension functions in just 10 steps compared to 1000 in gradient descent!

Here’s an example where we have a quadratic function from ℝ³⁰→ℝ. With 10 steps, steepest descent generated f(x) = -62434.18 . Within 1000 steps, gradient descent generated f(x) = -61596.84 . In just 10 steps, steepest descent decrease to an f-value lower than that of gradient descent in a 1000 steps!

Keep in mind that the above works really well only because we are dealing with quadratic functions. In general, it does get difficult to find the value for αk at every iteration. Optimizing g(α) doesn’t always lead you to find a concrete value for αk. Usually, we tend to use iterative algorithms to minimize such functions. In this case, things get tedious and much slower compared to gradient descent. This is why steepest descent isn’t as popular.

Conclusion

In conclusion, we learned three types of algorithms:

Newton’s method

Newton’s method provides a quadratic approximation to a function and optimizes that at every step. The biggest disadvantage is that it involves inverting a matrix for the multi-variable case(which can be expensive when dealing with a vector with a lot of features)

Gradient Descent

Gradient descent is the most common optimization algorithm. It is very quick in that the most expensive thing to do at each step is just computing the derivative. However, it does involve “guessing” or “tweaking” a hyperparameter that tells you how far you should go at each step.

Steepest Descent

Steepest descent is an algorithm that finds the best step to take given the gradient vector of the function. The only issue is that it involves optimizing a function at every iteration which typically tends to be expensive. In the case of quadratic functions, steepest descent performs generally well, although it does involve a lot of matrix calculation per step.

A notebook version of this article can be found here