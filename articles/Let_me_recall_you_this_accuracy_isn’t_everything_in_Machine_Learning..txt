Let me recall you this: accuracy isn’t everything in Machine Learning.

Everyday, data science professionals can’t stop thinking one thing: is that model really working? Data is like a live creature, and it changes and get messy almost everyday. At the end, all we want is to find a way to handle it and make good predictions, but how do we know how valuable are our results?

Well, one of the most basic things is to evaluate the success rate of our results, or how much of the test data we are predicting correctly. If this number is high, in theory our model is doing a good job. We like to call this value precision.

The starting point is to understand what are we trying to predict. It can be a number, a class, or even the answer for a yes/no question. After knowing this, we can define the parameters to evaluate performance. For this article, I’ll use a model that is trying to predict if the review left by a customer after an online shop experience will be good or not, which is a typical classification problem, with two possible outcomes:

0 = is not a good review

1 = is a good review

To answer that question, I did the traditional steps: data cleaning and feature selection, then I’ve trained several models, picked one and tried to improve it. You can find the full project, including the dataset I used, here on my GitHub.

Initially, I’ve tried 3 classification algorithms: KNN, Random Forest and Support Vector Machines (SVM), using the native functions available on scikit-learn, and tuning one parameter on each model. Let’s check the results:

Source: author

Let’s start looking at the column “F1 for Accuracy”. There we can see the results are very similar, around 80% each, what means that from all the predictions, the algorithm got it right 80% of the time, which can seem good if we just have that information. But when we look at recall for classes “0” and “1”, the values are substantially different. But what that means?