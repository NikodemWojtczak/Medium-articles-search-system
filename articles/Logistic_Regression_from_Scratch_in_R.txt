Introduction

In statistics and data science, logistic regression is used to predict the probability of a certain class or event. Usually, the model is binomial, but can also extend to multinomial. It probably is one of the simplest yet extremely useful models for a lot of applications, with its fast implementation and ease of interpretation.

This post will focus on the binomial logistic regression (with possible follow up on a multinomial model). I will discuss the basics of the logistic regression, how it is related to linear regression and how to construct the model in R using simply the matrix operation. Using only math and matrix operation (not the built-in model in R) will help us understand logistic regression under the hood.

Finally, I will use the constructed model to classify some generated data and show the decision boundary.

Logistic regression

We can think logistic regression is a generalized linear model, with a binominal distribution and a logit link function. This similarity with linear regression will help us construct the model. However the difference between the two models is that: in linear regression, the range of predicted value is (-∞, ∞), while in logistic regression, it is the probability p ranging [0, 1]. That’s why we need to use the logit link function.

Instead of predicting p directly, we predict the log of odds (logit):

which has a range from -∞ to ∞. When p → 0, logit(p) → -∞ and when p → 1, logit(p) → -∞. As a result, the logit function effectively maps the probability values from [0, 1] to (-∞, ∞). Now the linear relationship is:

where the superscript denotes the ith example, and the subscript denotes the feature or predictors x1, x2 etc ( x0 is 1 as bias). For total of m training examples, the shape of the predictor matrix X will be m×(D+1), where D is the dimensionality of the predictor variables ( x1, x2, …, xD). Adding 1 is to include the bias column x0.

And (θ0, θ1, …, θD) is a (D+1)×1 column vector. To vectorize the calculation, the right hand side (RHS) can be written as transpose(θ)⋅X or X⋅θ. Next the task is to find θ, which best represents the variation in p with varying X amongst m training examples.

To find θ, we need to define a cost function. The cost function is such that every incorrect prediction (or further away from the real value) will increase its value. In logistic regresion, the cost function is defined as:

where h(x) is the sigmoid function, inverse of logit function:

For every example, y is the actual class label 0 or 1, and h(x) is the predicted probability of getting the value of 1. If y = 1 (the second term with (1-y) will be 0), J(i) = -y⋅log(h(x)). When h(x) → 1, J(i) → 0 since log(1) = 0; when h(x) → 0, J(i) → ∞. If y = 0, J(i) = -log(1-(h(x))). When h(x) → 0, J(i) → 0, when h(x) → 1, J(i) → ∞. As h(x) furthers from y, the cost function increases rapidly.

This is the basic process to construct the model. Surprisingly it is simpler than I thought when I start coding.

Model construction in R

Now that we have the math part, let’s build our logistic regression. First I will define helper functions such as the sigmoid function, cost function J and the gradient of J. Note %*% is the dot product in R. All the functions below uses vectorized calculations.

Next is the logistic regression fuction, which takes training data X, and label y as input. It returns a column vector which stores the coefficients in θ. One thing to pay attention to is that the input X usually doesn’t have a bias term, the leading column vector of 1, so I added this column in the function.

Finally, I can write two prediction functions: first one predicts the probability p with X and θ as input, the second one returns y (1 or 0) with p as input.

Classification and decision boundary

The training data is generated such that it has two classes (0, 1), two predictors (x1, x2) and can be separated by a linear function.

There is some slight overlap so no such line will perfectly separate the two classes. However, our model shall still be able to find the best line.

Now I can train the model to get θ.

A grid is also created, which can be seen as a test set. The trained model will be applied to this grid, and predict the outcome Z. This can be used to create a decision boundary.

In the plot below, the model predicts a boundary that separates most of the two classes. Some data points are not correctly predicted as expected. However, a model that makes 100% prediction on the training data may not be a good one most of the time, as it overfits the data. In fact based on how I generated the data, the analytical solution should be x/3 + y = 2. And my decision boundary is very close to this analytical line.

Conclusion

There you have it, it is not that hard for ourselves to build a regression model from scratch. If you follow this post, hopefully by now, you have a better understanding of logistic regression. One last note, although logistic regression is often said to be a classifier, it can also be used for regression: to find the probability as we see above.