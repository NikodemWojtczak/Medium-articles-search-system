Reinforcement Learning Introduction

An introduction to reinforcement learning problems and solutions Y Tech · Follow 4 min read · Jul 25, 2019 -- Share

This post will be an introductory level on reinforcement learning. Throughout this post, the problem definitions and some most popular solutions will be discussed. After this article, you should be able to understand what is reinforcement learning, and how to find the optimal policy for the problem.

The Problem Description

The agent-environment interaction in reinforcement learning

The Setting

The reinforcement learning (RL) framework is characterized by an agent learning to interact with its environment.

learning to interact with its environment. At each time step, the agent receives the environment’s state (the environment presents a situation to the agent), and the agent must choose an appropriate action in response. One time step later, the agent receives a reward (the environment indicates whether the agent has responded appropriately to the state) and a new state .

(the environment presents a situation to the agent), and the agent must choose an appropriate in response. One time step later, the agent receives a (the environment indicates whether the agent has responded appropriately to the state) and a new . All agents have the goal to maximize the expected cumulative reward.

Episodic vs. Continuing Tasks

Continuing tasks are tasks that continue forever, without end.

are tasks that continue forever, without end. Episodic tasks are tasks with a well-defined starting and ending point.

* In this case, we refer to a complete sequence of interaction, from start to finish, as an episode.

* Episodic tasks come to an end whenever the agent reaches a terminal state.

Cumulative Reward

The discounted return at time step t is G(t) = R(t+1) + γ*R(t+2) + γ^2*R(t+3) + ...

The agent selects actions with the goal of maximizing expected (discounted) return.

The discount rate γ is something that you set, to refine the goal that you have the agent.

* It must satisfy 0 ≤ γ ≤ 1 .

* If γ = 0 , the agent only cares about the most immediate reward.

* If γ = 1 , the return is not discounted.

* For larger values of γ , the agent cares more about the distant future. Smaller values of γ result in more extreme discounting.

MDPs and One-Step Dynamics