How to tune hyperparameters of tSNE

This is the second post of the column Mathematical Statistics and Machine Learning for Life Sciences. In the first post we discussed whether and where in Life Sciences we have Big Data suitable for Machine / Deep Learning, and emphasized that Single Cell is one of the most promising Big Data resources. t-distributed stochastic neighbor embedding (tSNE) is a Machine Learning non-linear dimensionality reduction technique which is absolutely central for Single Cell data analysis. However, the choice of hyperparameters for the tSNE might be confusing for beginners.

In this post, I will share my recommendations on selecting optimal values of hyperparameters such as perplexity, number of principal components to keep, and number of iterations for running tSNE.

How to Use tSNE Effectively

When teaching single cell RNA sequencing (scRNAseq) course I keep getting questions about sensitivity of tSNE with respect to hyperparameters such as perplexity. The questions are usually inspired by this fantastic post about challenges with interpreting tSNE plots.

A popular tutorial on developing intuition behind tSNE

Despite my great respect for the main message of the post, I think scRNAseq community should not worry too much about perplexity and other tSNE hyperparameters based on what they learn from that post because: a) many examples in the post come from abstract mathematical topologies which do not really resemble scRNAseq data, b) the post concentrates on extreme tSNE hyperparameters which are rarely used in the real world scRNAseq analysis.

If you do scRNAseq analysis you will not avoid the popular Rtsne function and R package which is based on Barnes-Hut C++ implementation of the original tSNE algorithm. The Rtsne function has three main hyperparameters: