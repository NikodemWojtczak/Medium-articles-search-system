Introduction

Why?

There are many articles and courses dedicated to the latest ML/AI research aimed at training bigger models and achieving higher classification accuracy. This is great for research and academia and pushing the limits of what AI can do. However, these are not really tailored for poor student practitioners starting off with their first major AI projects or penny conscious entrepreneurs looking to build an MVP of their cool revolutionary idea.

What?

In this work I take a budgeted approach to model training and try to answer the question:

What is the minimum, practical cost to complete a real world AI project?

The problem that I chose for this was an Image Classification problem.This article captures the process I followed and key budgeting lessons learned from each step.

Summary

The answer is roughly $300 â†’ This is the amount it takes to train a well performing Computer Vision model using cloud computing. Incidentally (or not) this is also the amount of credit that Google gives as incentive to get started on the Google Cloud Platform (GCP) [1].

The breakdown of the budget is given below. The two rightmost columns list the instances from AWS and GCP that are most amenable to this task. The cost is an average of the instances listed in these columns. At present there is not a lot separating the two cloud providers in terms of cost.