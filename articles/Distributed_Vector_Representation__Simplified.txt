Arguably the most essential feature representation method in Machine Learning

Let’s play a simple game. We have 3 “people” with us — Micheal, Lucy and Gab. We wanna know which person’s playlist matches the most with Lucy, is it Micheal or Gab?? Let me give you some hints. Lucy loves classic rock, and so does Gab but Micheal is not a fan. Lucy prefers instrumental versions and Micheal is also the same, but Gab does not fancy them at all. Lucy does not like Pop, Micheal outright hates it but Gab is definitely a Pop fanatic!!

Was this representation of the information helpful? Can you conclusively say whose playlist matches more with Lucy? Lucy and Micheal have common interest in 2 different song genres while Lucy and Gab share only one common song genre. But can you be sure that the one genre Lucy and Gab share will not outweigh the other two? (I mean comeon .. it’s classic rock!!!)

What if I told you that I have some magic formula with which I can represent their music interests as one single value? Lucy can be represented as -0.1, Gab can be represented as -0.3 and Micheal can be represented as 0.7. That certainly makes the job easier because if you trust the formula, I think it is clear that Lucy’s playlist will match the most with Gab. But how did I come up with the formula? Let’s take it from the top…

What is Distributed Representation?

Distributed Representation refers to feature creation, in which the features may or may not have any obvious relations to the original input but they have comparative value i.e. similar inputs have similar features. Converting inputs to numerical representation (or features) is the first step to any Machine Learning algorithm in every domain.

Why is non-distributed representation not enough?

Non-distributed representation (also called one-hot vector representation) adds a new vector dimension for every new input possibility. Obviously, the more number of unique possible inputs, the longer the feature vector will be. This kind of representation has 2 major flaws,