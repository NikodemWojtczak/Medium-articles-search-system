# fit grid search

best_model = grid.fit(X_train,y_train)

This configuration allow the algorithm uses all the available cores, which will test 960 different configuration of decision trees, random forests and bagging classifiers(with decision trees as internal model) in a parallel way.

After this process, the best model produced by the GridSearchCV was a random forest model with the following configuration:

Before we define if this is the best model, letâ€™s check the accuracy of the model to train and test datasets. The purpose of this comparison is verify if the model is underfitted or overfitted.

As we can see above, the accuracy for both sets are good, and more than that, the values of train and test accuracy are very close. Thus, this results show us that the best model produced by GridSearchCV was well generalized.

In a random forest classifier we can set a hyperparameter called bootstrap, which defines weather samples will be trained with replacement or not. Although the best model was selected with this parameter as false, which tries to help the model to minimize the chance of overfitting, several other models present similar results when the parameter was set as true, as we can see in the image below.

So, for this dataset, we achieved good results regardless of the value of the bootstrap variable. However, the worst results due to possible overfitting came with bootstrap equal to true.

Understanding the feature importance

Via Giphy

Now, lets check the importance of each feature of the dataset for our model. For this task, we used two tools: The feature importance, from random forest classifier and the library SHAP (SHapley Additive exPlanations), which is a unified approach to explain the output of any machine learning model.