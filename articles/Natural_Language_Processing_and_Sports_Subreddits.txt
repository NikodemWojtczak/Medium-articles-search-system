Count Vectorization VS. TF-IDF

So we have our reddit posts in-hand, what do we do now? The NLP models I’ll be exploring can’t simply interpret english language as it appears on a website, we need to transform it into numerical data before using it in a machine learning model. A computer wants to read documents when they’re represented by numbers. Documents are a body of text for an NLP model to evaluate, which in our case, would be the combination of the text of the title and the “self text” body.

It’s easier on both a computer and programmer to understand documents when they can be interpreted numerically. To classify documents, each document is used an “input”, and the class label is the “output”, which in this case, would be r/NBA or r/NFL. Most classification machine-learning algorithms take vectors of numbers as input, so we need to convert the post documents to fixed-length vectors of numbers.

Count vectorization in NLP allows us to both tokenize, or segment text into sentences and words, collections of text documents and build a vocabulary of known words. This is done by evaluating the entire corpus of documents, taking each word found the vocabulary of the corpus, and assigning the document a vector based on the vocabulary found in the individual document.

In essence, a Count Vectorizer counts the the amount of times each word appears in a document, and assigns the document the appropriate values within the vector based on the established vocabulary of the corpus.

Similar to Count Vectorization, Term Frequency — Inverse Document Frequency, or TF-IDF, looks an entire corpus to form its vocabulary. What differentiates TF-IDF is that it weights the words based on how often they appear in a document. The logic behind this is that if a word occurs multiple times in a document, we should boost its relevance as it should be more meaningful than other words that appear fewer times (TF). However, if a word occurs many times in a document but also along many other documents, the model weights the word down as if is just a frequent word, and not because it is relevant or meaningful (IDF).

So, now that I’ve established how I will preprocess the documents for usage in my models, I need to clean the actual documents themselves. I used Regular Expressions, or RegEx, to find certain patterns of words and characters in each document to remove or replace. The Count Vectorizer and TF-IDF both will interpret messy items found in internet posts like URL’s and Emojis as a token, so it is important to only feed them alphanumeric text. For example, the line of code I used to remove URL’s looks something like this:

sub['text'] = sub['text'].map(lambda x: re.sub(r"((http|ftp|https):\/\/)?[\w\-_]+(\.[\w\-_]+)+([\w\-\.,@?^=%&:/~\+#]*[\w\-\@?^=%&/~\+#])?",

' ',

x)

As you can see, interpreting RegEx is not very straightforward, but it is a very effective tool in cleaning text. RegEx is extremely versatile, however, as it was also used in this project to remove certain words, phrases, and sequences of characters that skewed my results. In the case of this URL cleaner, it also looks for unique patterns in strings of characters.

So now that we have assembled our vectorizers and have cleaned our text of undesirable features, it’s time to model!