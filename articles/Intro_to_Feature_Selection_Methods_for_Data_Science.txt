Intro to Feature Selection Methods for Data Science

Authored by: Ryan Farmar, Ning Han, Madeline McCombe

Photo by Eugenio Mazzone on Unsplash

What is feature selection?

Well, let’s start by defining what a feature is. A feature is an X variable in your dataset, most often defined by a column. Many datasets nowadays can have 100+ features for a data analyst to sort through! That is a ridiculous amount to process normally, which is where feature selection methods come in handy. They allow you to reduce the number of features included in a model without sacrificing the predictive power. Features that are redundant or irrelevant can actually negatively impact your model performance, so it is necessary (and helpful) to remove them. Imagine trying to learn to ride a bike by making a paper airplane. I doubt you’d get very far on your first ride.

Benefits of feature selection

The main benefit of feature selection is that it reduces overfitting. By removing extraneous data, it allows the model to focus only on the important features of the data, and not get hung up on features that don’t matter. Another benefit of removing irrelevant information is that it improves the accuracy of the model’s predictions. It also reduces the computation time involved to get the model. Finally, having a smaller number of features makes your model more interpretable and easy to understand. Overall, feature selection is key to being able to predict values with any amount of accuracy.

Overview

There are three types of feature selection: Wrapper methods (forward, backward, and stepwise selection), Filter methods (ANOVA, Pearson correlation, variance thresholding), and Embedded methods (Lasso, Ridge, Decision Tree). We will go into an explanation of each with examples in Python below.

Wrapper methods

Wrapping methods compute models with a certain subset of features and evaluate the importance of each feature. Then they iterate and try a different subset of features until the optimal subset is reached. Two drawbacks of this method are the large computation time for data with many features, and that it tends to overfit the model when there…