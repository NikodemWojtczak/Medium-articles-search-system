Facebook’s latest Semi-Weak Supervised Learning framework is a novel approach to leveraging unlabelled datasets for Computer Vision. https://ai.facebook.com/blog/billion-scale-semi-supervised-learning/

Researchers at Facebook have been exploring the use of Instagram hashtags as a means to weakly-supervised pre-training for ImageNet classification models. In this research, “Weakly-Supervised” learning describes the use of noisy labels for supervised learning, i.e. hashtags on Instagram pictures. This is advantageous because big data is king in computer vision, and they are able to utilize 1 Billion of these Instagram images for pre-training (ImageNet contains 1.2 Million images for the sake of reference).

Their models based on this weakly-supervised learning paradigm have clocked in state-of-the-art ImageNet classification results at 85.4% top-1 accuracy. They currently hold the ImageNet trophy at 86.4% by extending weak-supervised learning with resolution augmentation.

Facebook’s latest framework Semi-Weak Supervised Learning is the latest installment of this idea, achieving impressive results and naturally giving rise to efficient models. As the amount of data continues to explode, research such as Facebook’s Semi-Weak Supervised Learning is finding new ways to make use of massive unlabelled datasets and solve Computer Vision tasks such as ImageNet classification.

Semi- and Weak- Supervised Learning

Facebook presents a unique take on “Semi-Supervised Learning” in this research. They describe model distillation as a semi-supervised learning task, which isn’t incorrect, just not conventional of the literature. Model distillation refers to using a larger capacity teacher network to produce a softmax distribution over class labels. The lower capacity student network is then trained on this distribution. Model distillation is one of the most powerful techniques for model compression, powering models such as HuggingFace’s DistilBERT.

Semi-supervised learning describes the paradigm of constructing supervised learning signals from unlabeled data. This is the idea of techniques like Word2Vec, DeepWalk, or Wav2Vec in which a part of the input is masked out and the model is tasked with predicting what has been removed. This idea extended to images is described as “inpainting”. However, more common is to do something like artificially rotating images and then having the model predict the rotation angle. This is the mechanism powering self-supervised models such as “Self-Supervised GAN”.

In Facebook’s “Semi-Weak supervised learning” framework, a teacher model is initially trained with a weakly-supervised dataset (supervised learning of Instagram hashtags), fine-tuned on ImageNet, then the teacher model predicts the class distribution over the original weakly-supervised dataset, the student model trains itself to predict the same distribution, and finally, the student model is fine-tuned on the ImageNet dataset.

Class Imbalance in Unlabeled Datasets

Class imbalance describes datasets that are heavily skewed towards one class in the training data. For example, a training set containing 80% dogs and 20% cats will produce a model biased towards labeling images as dogs. These massive unlabeled datasets such as the Instagram pictures naturally contain class-imbalanced and long-tailed distributions. For example, there are tons of dog pictures on Instagram, how many leaf beetles are there?

Researchers at Facebook overcome this problem by using a top-K scoring system of the predicted labels from the teacher network and using this parameter to balance the number of images. As K gets larger, the images towards the end of the distribution become noiser with respect to their class label. In the leaf beetle case, there may only be say 8K beetle images, so if K is extended to 16 or 32K, the images from 8,001 to 16,000 will be incorrectly labeled. However, the idea is that the teacher model has enough sense of bettle images such that image 8,001 which isn’t a beetle, will still be something semantically / visually similar.

Inference Acceleration for Large-Scale Model Distillation

High-level idea of NVIDIA’s TensorRT inference accelerator. https://developer.nvidia.com/tensorrt

Another interesting component that arises from this framework is the importance of inference acceleration when the teacher is predicting labels on the unlabeled dataset. The teacher network has to make approximately 1 billion predictions to produce the distilled labels for the student network. Usually models trained with model distillation don’t address the inference bottleneck, however in the case of labeling 1 billion images this way, it is clearly important. This presents an additional contribution of inference accelerators such as NVIDIA’s TensorRT and models optimized for small inference latency.

What is the best source of Weakly-supervised Datasets?

Photo by NeONBRAND on Unsplash

It is interesting to think about the Instagram hashtag labeling. Is this the best source out there for weakly-supervised datasets? YouTube videos contain tags provided by the uploader, but this kind of labeling would be significantly noiser than Instagram hashtagging.

Distilled Architecture Search

Facebook’s recent paper varies the student network architecture from ResNet-18 to ResNet-50 and incrementally larger versions of ResNeXt. Amongst a swarm of research papers exploring neural architecture search in all sorts of contexts, there has been relatively little investigation into the distillation teacher-student architecture relationship. Transferring from larger to smaller capacities of ResNet variants seems intuitive. This idea might be better manifested in a more structured progression such as from EfficientNet-E7 to E5. Additionally, performance boosts could likely be achieved be searching for a specific architecture that is best fitted to learning from the teacher’s predicted label distribution.

Can Generative Models further increase the size of pre-training datasets?

Generated face images from NVIDIA’s Progressively Growing GAN models. What happens if these models are married with the 1 billion Instagram images used for pre-training? https://arxiv.org/pdf/1710.10196.pdf

The idea of using GANs to generate new training data from existing datasets seems very promising. This has been proven successful in the case of extremely limited datasets such as medical image analysis, but has not yet worked for conventional tasks such as ImageNet classification or COCO object detection. With the “Semi-Weak” supervised training framework, can GANs or Variational Auto-encoders be applied to the 1 billion unlabeled images to produce 2 or 10 billion images? Will this result in novel images that improve the pre-training of Computer Vision models?

Conclusion

Facebook’s Semi-Weak supervised learning framework is a very interesting perspective on semi-supervised learning, weak-supervised learning, class imbalance in model distillation, and inference acceleration for model distillation. Their approach of using Instagram images seems like it will naturally scale up to 10s of billions of images as more people use Instagram and the datasets naturally get larger. The top-K scoring method is a great strategy to address class imbalance evident in massive unlabelled datasets. It is interesting to think if Generative Models can further increase the same of these unlabeled datasets. Thank you for reading, if you are interested further in the details of this paper, please check out the video below!