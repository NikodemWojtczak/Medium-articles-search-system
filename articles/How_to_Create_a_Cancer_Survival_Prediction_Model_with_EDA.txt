How to Create a Cancer Survival Prediction Model with EDA

A Thorough Walkthrough of Exploratory Data Analysis Techniques with Haberman’s Cancer Survival Dataset using Python to Create a Simple Predictive Cancer Survival Model Jessica Phillip · Follow Published in Towards Data Science · 16 min read · Dec 30, 2019 -- Listen Share

Illustration by John Flores

The impetus for this blog and the resultant cancer survival prediction model is to provide a glimpse into the potential of the healthcare industry. Healthcare continues to learn valuable lessons from the current success of machine learning in other industries to jumpstart the utility of predictive analytics (also known as “health forecasting” ) and to improve patient diagnosis, care, chronic disease management, hospital administration and supply chain efficiency. [1]

This classification project is an introduction to the exploratory data analysis (EDA) of the Haberman dataset in order to determine the attributes necessary to develop a simple predictive cancer survival model. The model’s forecast will determine, what is the likelihood a patient will survive beyond 5 years after surgery? This write up ultimately automates the prognosis for patients based on three attributes; age, year of surgery and the number of positive axillary nodes removed from the patient during surgery.

1. What is Exploratory Data Analysis?

In the field of machine learning, exploratory data analysis (EDA) is a philosophy or rather an approach for analyzing a dataset. It is a technique for summarizing, visualizing and becoming intimately familiar with the important characteristics of a dataset. EDA is useful in order to maximize insights, uncover underlying structure, extract important variables, detect outliers and anomalies as well as test unconscious/unintentional assumptions.

And while this process can be a bit tedious, one does not simply, [insert classic LOTR meme], skip the EDA process to rush into the machine learning stage. In fact, EDA techniques are the precursor to machine learning as it’s used to answer questions like:

How to define feature variables that can potentially be used for machine learning? How to choose the most suitable algorithms for your data set?

In essence, EDA is for unearthing what the data can tell us beyond formal modeling or general hypothesis testing to create applicable business insights, models and procedure enhancements that may have not been evident or worth investigating to business stakeholders. How? By finding patterns in the data and thus insights into something interesting that’s worth investigating.

2. Understanding the Dataset

The Haberman’s survival dataset contains cases from a study that was conducted between 1958 and 1970 at the University of Chicago’s Billings Hospital on the survival of patients who had undergone surgery for breast cancer.

You can easily download the dataset from the Kaggle link below:

Attribute Information:

Age: Age of patient at time of operation (numerical). Year: Patient’s year of operation to remove the nodes (year — 1900, numerical) Nodes: Number of positive axillary nodes detected (numerical) Status: Survival status (class attribute)

1 = the patient survived 5 years or longer

2 = the patient died within 5 years

Note: Just a reminder! It’s always a great idea to do a bit of background research on your dataset and the problem at hand before you delve into EDA. I did so with ‘nodes’ and here’s what I found.

Background: Positive axillary lymph nodes are small, bean-shaped organs located in the armpit (axilla), which act as filters along the lymph fluid channels. As lymph fluid leaves the breast and eventually returns to the bloodstream, the lymph nodes catch and trap cancer cells before they reach other parts of the body. Thus, having cancer cells in the lymph nodes under your arm suggests an increased risk of the cancer spreading. When lymph nodes are free of cancer, test results are negative. However, if cancer cells are detected in axillary lymph nodes they are deemed positive.

3. Environment Configuration: Importing Libraries and Loading Data File

Below, you’ll find details on how to set up your environment to repeat this EDA process and cancer survival model. The purpose for importing the required libraries for analysis are:

Pandas is used for manipulating the dataset

NumPy for imposing mathematical calculations and statistical on the dataset

Matplotlib and Seaborn are used for visualization

# Import the necessary packages

import pandas as pd

import matplotlib.pyplot as plt

import seaborn as sns

import numpy as np # Load the dataset

haberman = pd.read_csv("haberman.csv")

4. Data Details

# Print first 5 lines of dataset

haberman.head()

Output:

# Personal preference: convert year format from 'YY' to 'YYYY'

haberman['year'] = haberman['year'] + 1900 # Again, personal preference: convert survival status 1 and 2 to #'yes' and 'no', respectively

haberman['status'] = haberman['status'].map({1:'yes', 2: 'no'}) # Print first 5 lines of altered dataset

haberman.head()

Output:

# Print the number of rows and columns for the dataset

haberman.shape

Output:

(306, 4)

Observations:

The .csv file contains data from 306 patients stored in the rows and 4 columns describing the features of the data set.

# Print the column names in dataset and the data type

haberman.columns

Output:

Index([‘age’, ‘year’, ‘nodes’, ‘status’], type = ‘object’)

# Details about the dataset

haberman.info()

Output:

<class 'pandas.core.frame.DataFrame'>

RangeIndex: 306 entries, 0 to 305

Data columns (total 4 columns):

age 306 non-null int64

year 306 non-null int64

nodes 306 non-null int64

status 306 non-null int64

dtypes: int64(4)

memory usage: 9.6 KB

Observations:

There are no missing values in this data set. The 3 attributes are of int data type and the last attribute was converted to a categorical datatype. In the ‘status’ column, the initial int-type was mapped to ‘yes’, which means the patient survived beyond 5 years. And the value ‘2’ was mapped to ‘no’, which means the patient passed within 5 years.

# Statistically describe the dataset

haberman.describe()

Output:

Observations:

Count: Total number of data points present in each respective column was 306. Mean: The average value for each column in respect to age, year and nodes. Std (Standard Deviation): The measurement of how far the group of values are spread out from the average (mean), or expected value. Min (Minimum): The minimum value present in each column. 25% Quartile: 1/4th of the data points are below the provided value. 50% Quartile: 1/2 of the data points are below the provided value. 75% Quartile: 3/4th of the data points are below the provided value. Max (Maximum): The maximum value present per column.

# Count each type of status in the dataset

haberman["status"].value_counts()

Output:

Yes 225

No 81

Name: status, dtype: int64

Observations:

The value_counts function details how many data points for each class are present. This code snippet describes how many patients survived and how many did not after a period of 5 years. Out of 306 patients, 225 survived and 81 did not live beyond 5 years.

# Create a 'yes status' dataset to store 'yes' values for the patients that survived

status_yes = haberman[haberman["status"] == 'yes']

status_yes.describe()

Output:

# Create a 'no status' dataset to store values for the patients that did not survive

status_no = haberman[haberman["status"] == 'no']

status_no.describe()

Output:

Observations:

On Age: The difference between the mean age and year of no vs yes dataset isn’t statistically significant. Note: There is however, a trend that describes a general increase in the age of the patients who did not survive surgery after 5 years. This increase can be seen in the minimum age, the percent quartiles and the maximum age of the non-survived patients. On Year: The difference between the mean year and of the no vs yes dataset isn’t statistically significant. On Nodes: There is a noticeable increase in the mean number of nodes found for the ‘yes dataset’ and the ‘no dataset’. There is also an increase in the maximum number of nodes found in the ‘no dataset’. The percent quartiles were also higher for the no dataset than the yes dataset. Note: For example, for the 75% of patients who died within 5 years after surgery they were found to have 11 nodes or more compared to only 3 nodes for the patients that did survive.

Model Insight:

On average, those who survived have about 2.5 times less nodes than those who did not survive. For those who survived they held an average of 2.79 nodes versus the 7.46 nodes for the patients that did not survive.

5. Univariate Analysis

Univariate analysis is the simplest form of analyzing data. This process does not deal with causes or relationships, as only one variable is involved. Instead, it’s major motive is to describe; it takes data, summarizes that data and finds patterns that exists within a single feature.

5.1 Probability Density Functions

Probability Density Functions (PDF) are a statistical measure used to gauge the likely outcome of a discrete value. PDF’s are plotted on a graph typically resembling a bell curve, with the probability of the outcome lying below the curve.

Here the height of the bar denotes the percentage of the data points under the corresponding group.

What is the intuition behind PDF?

# Create a function for PDF analysis

def histoPDF(featureVar, classVar):

sns.FacetGrid(data = haberman, hue = classVar, height = 5) \

.map(sns.distplot, featureVar) \

.add_legend();

plt.title("Histogram for " + featureVar)

plt.ylabel("density")

plt.plot('histogram_2.png') # PDF histogram for age v. status

histoPDF('age', 'status')

Output:

Observations:

Major overlapping is observed, which suggests that age isn’t a major determining factor in the patients likihood of survival. Differences between the age of the yes dataset and no dataset are barely observable given the amount of overlap in the PDF. Perhaps another statistical method can unearth a pattern between age and survival status.

Model Insights:

Ages 30–40 had a higher chance of survival, whereas ages 40–60 did not. For ages 60+ the chances of survival were about 50/50.

# PDF histogram for year v. status

histoPDF('year', 'status')

Output:

Observations:

Major overlapping continues again suggesting that the year of the patient’s surgical procedure did not affect their survival rate/outcome after 5 years. There was a spike in the death rate for patients whose surgery was in year 1965 and a decrease for procedures done in 1960. Patient’s likelihood of survival was up between 1960–1962.

# PDF histogram for nodes v. status

histoPDF('nodes', 'status')

Output:

Observations:

Complete separation would be ideal to distinguish the exact number of nodes for patients who survived. Patients with 0 nodes or 1 node are more likely to survive. There are very few chances of surviving if there are 25 or more nodes. This plot has shown that the number of nodes seem to influence the survival rate of patients more so than age and year of operation.

Model Insight:

Patient non-survival increasingly likely after 5 nodes.

6. Cumulative Distribution Function

CDF creates a plot of the empirical cumulative distribution function. Use the CDF plot to determine the percent of data that is at or below a given value on the x-axis.

# CDF analysis

count1, bin_edges1 = np.histogram(status_yes['nodes'], bins = 10, density = True)

pdf1 = count1/(sum(count1))

print(pdf1, bin_edges1);

cdf1 = np.cumsum(pdf1)

plt.plot(bin_edges1[1:], pdf1)

plt.plot(bin_edges1[1:], cdf1, label = 'Yes')

plt.xlabel('nodes') print("---------------------------------------------------------") count2, bin_edges2 = np.histogram(status_no['nodes'], bins = 10, density = True)

pdf2 = count2/(sum(count2))

print(pdf2, bin_edges2);

cdf2 = np.cumsum(pdf2)

plt.plot(bin_edges2[1:], pdf2)

plt.plot(bin_edges2[1:], cdf2, label = 'No')

plt.xlabel('nodes')

plt.legend() plt.show()

Output:

Observations:

Approximately 83.55% of patients who survived had nodes in the 0 to 4.6 range as per the CDF summary stats.

7. Box & Whisker Plots and Violin Plots

A box and whisker plot — also called a box plot — displays the five-number summary of a set of data. The five-number summary is the minimum, first quartile, median, third quartile, and maximum. The box extends from the lower to upper quartile values of the data, with a line at the median. The whiskers extend from the box to show the range of the data. Outlier points are those past the end of the whiskers.

Violin plot is the combination of a box plot and probability density function (CDF). Violin Plots allow to visualize the distribution of a numeric variable for one or several groups. It’s a close form of the boxplot, but allows a deeper understanding of the density.

# Create box and whisker plot for each feature

plt.figure(1)

plt.figure(figsize = (15, 5))

plt.subplot(131)

sns.boxplot(x = 'status', y = 'age', data = haberman)

plt.subplot(132)

sns.boxplot(x = 'status', y = 'year', data = haberman)

plt.subplot(133)

sns.boxplot(x = 'status', y = 'nodes', data = haberman)

plt.show()

Output: