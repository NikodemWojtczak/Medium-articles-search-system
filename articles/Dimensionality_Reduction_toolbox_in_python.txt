This article is derived from my work with my friend Hervé Trinh.

In recent years, the volume of data has exploded by more than 80%. This has led to the emergence of many models of Machine Learning, since it is easier to train these models with an important dataset.

However, the weight of the data can have a significant impact on the time of implementation of an algorithm since the complexity increases with the size of the data. In parallel, dimensionality can complicate the viewing of information contained in a database.

Dimensionality reduction is the process of reducing the total number of features in our feature set using strategies like feature selection or feature extraction.



For example, a base that contains the characteristics of a car will be difficult to view as they are numerous. One could imagine merging the mileage and the age of the car to form the characteristic wear, provided that they are correlated.

There are a multitude of algorithms for the reduction of dimensionality, there are mainly two categories, linear methods and nonlinear methods.

The techniques I will share will be with python. Be sure to have python installed on your machine.

To begin, we import the necessary libraries.

Let’s import the mnist data for processing.

A very popular technique of linear data transformation from higher to lower dimensions is Principal Component Analysis, also known as PCA. Let’s try to understand more about PCA and how we can use it for feature extraction in the following sections.

Principal Composant Analysis

Principal component analysis is a statistical method that uses the process of linear, orthogonal transformation to transform a higher-dimensional set of features that could be possibly correlated into a lower-dimensional set of linearly uncorrelated features. These transformed and newly created features are also known as Principal Components or PCs. In any PCA transformation, the total number of PCs is always less than or equal to the initial number of features. The first principal component tries to capture the maximum variance of the original set of features.

The previous code will both reduce the size of the mnist database and rebuild it in its original dimension. We can now plot the data in small size.

Incremental PCA

The incremental principal component analysis is a variant of the ACP It only keeps the most significant singular vectors to project the data into a space to

reduced size.

Kernel PCA

KPCA making it possible to perform complex nonlinear projections for dimensionality reduction.

For example, the following code uses Scikit-Learn’s KernelPCA class to perform kPCA with an RBF kernel.

Sparse PCA

Sparse PCA uses the links between the ACP and the SVD to extract the main components by solving a lower-order matrix approximation problem.

Singular Value Decomposition [SVD]

SVD can be applied even on rectangular matrices; whereas, eigenvalues are defined only for square matrices. The equivalent of eigenvalues obtained through the SVD method are called singular values, and vectors obtained equivalent to eigenvectors are known as singular vectors. However, as they are rectangular in nature, we need to have left singular vectors and right singular vectors respectively for their dimensions.

Gaussian Random Projection [GRP]

In the random projection, data with a very large dimension (d) are projected

in a two-dimensional space (kd) with a random matrix.

Sparse Random Projection [SRP]

The fragmented random matrix is ​​an alternative to the dense random projection matrix with traditional methods of dimension reduction. It ensures similar embedding quality while maximizing memory efficiency and allowing faster calculation of projected data.

MultiDimensional Scaling [MDS]

MDS is a method to visualize the di-similarity similarity between samples. MDS returns an optimal solution to represent the data in a smaller dimension space, in case the number of dimensions k is predefined.

ISOMAP

It is a nonlinear dimensionality reduction method based on spectral theory that attempts to preserve geodetic distances in the lower dimension.

MiniBatch Dictionary Learning

Dictionary-based learning solves a problem of matrix factorization which amounts to finding a dictionary that can give good results under the condition of parsimony of the code.

Independent Composent Analysis [ICA]

Independent component analysis is a method primarily used for signal processing to linearly separate mixed data.

T-distributed Stochastic Neighbor Embedding [T-SNE]

T-SNE reduces dimensionality while trying to keep similar instances close and dissimilar instances apart. It is mostly used for visualization, in particular to visualize clusters of instances in high-dimensional space.

Locally Linear Embedding [LLE]

LLE works by first measuring how each training instance linearly relates to its closest neighbors (c.n.), and then looking for a low-dimensional representation of the training set where these local relationships are best preserved (more details shortly). This makes it particularly good at unrolling twisted manifolds, especially when there is not too much noise.

Autoencoders

Autoencoders are artificial neural networks capable of learning efficient representations of the input data, called codings, without any supervision (i.e., the training set is unlabeled).