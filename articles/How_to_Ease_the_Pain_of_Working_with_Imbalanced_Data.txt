How to Ease the Pain of Working with Imbalanced Data

A summary of methods and resources for creating a model using an imbalanced dataset Andrea Brown · Follow Published in Towards Data Science · 5 min read · Jul 18, 2019 -- Listen Share

You’ve finally collected and cleaned your data and have even completed some exploratory data analysis (EDA). All that hard work has finally paid off — time to start playing with models! However, you quickly realize that 99% of your binary labels are of the majority class while only 1% are of the minority class. Using accuracy as the primary evaluation metric, your models are classifying all predictions as the majority class and are still 99% accurate. You do a little Googling to find the right search terms for Stack Overflow, and then you sigh deeply as you read about the pains of working with imbalanced data…

What is Imbalanced Data?

Fraud detection is commonly used as an example of imbalanced datasets. Maybe only 1 out of every 1,000 transactions is fraudulent, which means only 0.1% of labels are of the minority class. Disease data is also likely to be imbalanced. In environmental health, a typical threshold for an elevated risk of developing cancer is 1 in 10^6, or one in a million people. For my project, I was working with lawsuit data, and the companies that had been sued were minimal compared to the dataset as a whole. In summary, imbalanced data is exactly what it sounds like: the labels for the minority class are far and few between, making the accuracy metric somewhat useless for evaluating model results.

What Should I do Differently?

Use Different Metrics

Another key difference is to use alternative metrics to the traditional accuracy metric. Since I was primarily interested in identifying True Positives and avoiding False Negatives rather than maximizing accuracy, I used the area under the curve receiving operator characteristic (AUC ROC) metric. This TDS post has a great explanation of the AUC ROC metric. To summarize, AUC ROC = 0.5 essentially means the model is equivalent to a random guess and AUC ROC = 1 implies the model can perfectly differentiate between the minority and majority classes.

I also relied on the confusion matrix to maximize the number of True Positives and False Positives and minimize the number of False Negatives. Since I was working with lawsuit data, I decided to consider a False Positive result as an indication of a high likelihood of being sued in the future. Therefore, False Positive results were also very important to me. I liked the code in this Kaggle kernel to visualize the confusion matrix.

Summary of True Positives and False Negatives using the Confusion Matrix

I also calculated the more traditional precision and recall metrics (equations below for quick reference) to compare different learning algorithms, and I added the less traditional miss rate (false positive rate) to the list of metrics to compare as well.

Precision = TP / (TP + FP) Recall (True Positive Rate) = TP / (TP + FN) Miss Rate (False Positive Rate) = FN / (FN +TP)

Methodology

There are two primary classes of methodologies to working with imbalanced data: (1) at the algorithm level and (2) at the data level. I’ll summarize the methods for each in the following sections.

Algorithm-Related Methods

Cost-Sensitive Learning

Considering solutions to an imbalanced dataset at the algorithm level requires an understanding of algorithms that enable cost-sensitive learning. In my case (and likely for disease and fraud detection as well), identifying True Positives was the primary goal of the model, even at the expense of selecting some False Positives. This is where cost-sensitive learning comes in handy. Cost-sensitive learning takes into account the different types of misclassification (False Positives & False Negatives).

Logistic Regression

A classic logistic regression algorithm is a robust model for imbalanced datasets. The logistic regression algorithm includes a loss function that calculates the cost for misclassification. Using SciKit-Learn, the loss function can be manipulated with a penalty weight that includes either ‘L1’ or ‘L2’ Regularization depending on the solver used.

Support Vector Machines

In SciKit-Learn, the Support Vector Classifier includes a ‘class_weight’ parameter that can be used to give more weight to the minority class. “The ‘balanced’ mode “uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data.” This paper further details developing a cost-sensitive SVM.

Naive Bayes

SciKit-Learn includes a Complement Naive Bayes algorithm that is a cost-sensitive classifier that “uses statistics from the complement of each class to compute the model’s weights.” Optimizing the model weights is an effective way of handling imbalanced datasets.

Ensemble Method — Boosting

Boosting algorithms are ideal for imbalanced datasets “because higher weight is given to the minority class at each successive iteration.” For example, “AdaBoost iteratively builds an ensemble of weak learners by adjusting the weights of misclassified data during each iteration.”

Data-Related Methods

Re-sampling

Solving a class imbalance problem at the data level typically involves manipulating the existing data to force the dataset used to train the algorithm to be balanced. This method is called re-sampling, and typical re-sampling techniques include:

Over-sampling the minority class,

Under-sampling the majority class,

Combining over- and under-sampling, or

Creating ensemble balanced sets.

Over-sampling the Minority Class

Over-sampling involves balancing the dataset by creating synthetic data to increase the number of outcomes in the minority class. A common method for over-sampling is called the Synthetic Minority Oversampling Technique (SMOTE), which uses k-nearest neighbors to create synthetic data.

Taking this a step further, SMOTEBoost combines over-sampling with boosting. SMOTEBoost is “an over-sampling method based on the SMOTE algorithm that injects the SMOTE method at each boosting iteration.”

Under-sampling the Majority Class

Under-sampling involves decreasing the data included in the majority class to balance the training dataset. Note that this decreases the size of the dataset. A common technique for under-sampling is random undersampling (RUS), which randomly and selects a subset of the majority class. This Medium post goes into more detail on the various under-sampling methods.

RUSBoost combines under-sampling with boosting. Similar to SMOTEBoost, “RUSBoost achieves the same goal by performing random undersampling (RUS) at each boosting iteration instead of SMOTE.”

Ensemble Method — Bagging

Bagging is an example of an ensemble technique at the data level. Bagging involves “building multiple models (typically of the same type) from different subsamples of the training dataset.” Bagging can reduce variance and prevent overfitting in algorithms such as the Random Forest Decision Trees algorithm.

Conclusion

For my dataset, the best approach was to use combine an ensemble method of boosting with over-sampling of the minority class (SMOTEBoost) and use the logistic regression algorithm to achieve an AUC ROC value of 0.81. With additional time spent on feature engineering, I could increase the AUC ROC value even more.