Arguably the best starting point for regression tasks are linear models: they can be trained quickly and are easily interpretable.

Linear models make a prediction using a linear function of the input features. Here we’ll explore some popular linear models in Scikit-Learn.

The full Jupyter notebook can be found here.

Data

Here we’ll use the SciKit-Learn diabetes dataset to review some popular linear regression algorithms.

The dataset contains 10 features (that have already been mean centered and scaled) and a target value: a measure of disease progression one year after baseline.

We import the data and prepare for modeling:

from sklearn.datasets import load_diabetes

from sklearn.model_selection import train_test_split # load regression dataset

diabetes, target = load_diabetes(return_X_y=True)

diabetes = pd.DataFrame(diabetes) # Prepare data for modeling

# Separate input features and target

y = target

X = diabetes # setting up testing and training sets

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)

Evaluation Metric: R²

R-Squared, or the coefficient of determination, is how much variance in the target variable that is explained by our model.

Values can range from 0 to 1. Higher values indicate a model that is highly predictive. For example, a R² value of 0.80 means that the model is accounting for 80% of the variability in the data.

In general, the higher the R² value the better. Low values indicate that our model is not very good at predicting the target. One caution, however, is that a very high R² could be a sign of overfitting.

We’ll use the following function to get cross validation scores for our models: