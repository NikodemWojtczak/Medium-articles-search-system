A Simple CNN: Multi Image Classifier

Using Tensorflow and transfer learning, easily make a labeled image classifier with convolutional neural network Iftekher Mamun · Follow Published in Towards Data Science · 11 min read · Apr 7, 2019 -- 10 Listen Share

Computer vision and neural networks are the hot new IT of machine learning techniques. With advances of neural networks and an ability to read images as pixel density numbers, numerous companies are relying on this technique for more data. For example, speed camera uses computer vision to take pictures of license plate of cars who are going above the speeding limit and match the license plate number with their known database to send the ticket to. Although this is more related to Object Character Recognition than Image Classification, both uses computer vision and neural networks as a base to work.

A more realistic example of image classification would be Facebook tagging algorithm. When you upload an album with people in them and tag them in Facebook, the tag algorithm breaks down the person’s picture pixel location and store it in the database. Because each picture has its own unique pixel location, it is relatively easy for the algorithm to realize who is who based on previous pictures located in the database. Of course the algorithm can make mistake from time to time, but the more you correct it, the better it will be at identifying your friends and automatically tag them for you when you upload. However, the Facebook tag algorithm is built with artificial intelligence in mind. This means that the tagging algorithm is capable of learning based on our input and make better classifications in the future.

We will not focus on the AI aspect, but rather on the simplest way to make an image classification algorithm. The only difference between our model and Facebook’s will be that ours cannot learn from it’s mistake unless we fix it. However, for a simple neural network project, it is sufficient.

Since it is unethical to use pictures of people, we will be using animals to create our model. My friend Vicente and I have already made a project on this, so I will be using that as the example to follow through. The GitHub is linked at the end.

The first step is to gather the data. This in my opinion, will be the most difficult and annoying aspect of the project. Remember that the data must be labeled. Thankfully, Kaggle has labeled images that we can easily download. The set we worked with can be found here: animal-10 dataset. If your dataset is not labeled, this can be be time consuming as you would have to manually create new labels for each categories of images. Another method is to create new labels and only move 100 pictures into their proper labels, and create a classifier like the one we will and have that machine classify the images. This will lead to errors in classification, so you may want to check manually after each run, and this is where it becomes time consuming.

Now that we have our datasets stored safely in our computer or cloud, let’s make sure we have a training data set, a validation data set, and a testing data set. Training data set would contain 85–90% of the total labeled data. This data would be used to train our machine about the different types of images we have. Validation data set would contain 5–10% of the total labeled data. This will test how well our machine performs against known labeled data. The testing data set would contain the rest of the data in an unlabeled format. This testing data will be used to test how well our machine can classify data it has never seen. The testing data can also just contain images from Google that you have downloaded, as long as it make sense to the topic you are classifying.

Let’s import all the necessary libraries first:

import pandas as pd

import numpy as np

import itertools

import keras

from sklearn import metrics

from sklearn.metrics import confusion_matrix

from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img

from keras.models import Sequential

from keras import optimizers

from keras.preprocessing import image

from keras.layers import Dropout, Flatten, Dense

from keras import applications

from keras.utils.np_utils import to_categorical

import matplotlib.pyplot as plt

import matplotlib.image as mpimg

%matplotlib inline

import math

import datetime

import time

Defining Dimensions and locating images:

#Default dimensions we found online

img_width, img_height = 224, 224



#Create a bottleneck file

top_model_weights_path = ‘bottleneck_fc_model.h5’ # loading up our datasets

train_data_dir = ‘data/train’

validation_data_dir = ‘data/validation’

test_data_dir = ‘data/test’



# number of epochs to train top model

epochs = 7 #this has been changed after multiple model run

# batch size used by flow_from_directory and predict_generator

batch_size = 50

In this step, we are defining the dimensions of the image. Depending on your image size, you can change it but we found best that 224, 224 works best. Then we created a bottleneck file system. This will be used to convert all image pixels in to their number (numpy array) correspondent and store it in our storage system. Once we run this, it will take from half hours to several hours depending on the numbers of classifications and how many images per classifications. Then we simply tell our program where each images are located in our storage so the machine knows where is what. Finally, we define the epoch and batch sizes for our machine. For neural networks, this is a key step. We found that this set of pairing was optimal for our machine learning models but again, depending on the number of images that needs to be adjusted.

Importing transfer learning model VGG16:

#Loading vgc16 model

vgg16 = applications.VGG16(include_top=False, weights=’imagenet’) datagen = ImageDataGenerator(rescale=1. / 255)

#needed to create the bottleneck .npy files

This is importing the transfer learning aspect of the convolutional neural network. Transfer learning is handy because it comes with pre-made neural networks and other necessary components that we would otherwise have to create. There are many transfer learning model. I particularly like VGG16 as it uses only 11 convolutional layers and pretty easy to work with. However, if you are working with larger image files, it is best to use more layers, so I recommend resnet50, which contains 50 convolutional layers.

For our image classifier, we only worked with 6 classifications so using transfer learning on those images did not take too long, but remember that the more images and classifications, the longer this next step will take. But thankfully since you only need to convert the image pixels to numbers only once, you only have to do the next step for each training, validation and testing only once- unless you have deleted or corrupted the bottleneck file.

Creation of the weights and feature using VGG16:

#__this can take an hour and half to run so only run it once.

#once the npy files have been created, no need to run again. Convert this cell to a code cell to run.__ start = datetime.datetime.now()



generator = datagen.flow_from_directory(

train_data_dir,

target_size=(img_width, img_height),

batch_size=batch_size,

class_mode=None,

shuffle=False)



nb_train_samples = len(generator.filenames)

num_classes = len(generator.class_indices)



predict_size_train = int(math.ceil(nb_train_samples / batch_size))



bottleneck_features_train = vgg16.predict_generator(generator, predict_size_train)



np.save(‘bottleneck_features_train.npy’, bottleneck_features_train)

end= datetime.datetime.now()

elapsed= end-start

print (‘Time: ‘, elapsed)

Since we are making a simple image classifier, there is no need to change the default settings. Just follow the above steps for the training, validation, and testing directory we created above. However, you can add different features such as image rotation, transformation, reflection and distortion.

Once the files have been converted and saved to the bottleneck file, we load them and prepare them for our convolutional neural network. This is also a good way to make sure all your data have been loaded into bottleneck file. Remember to repeat this step for validation and testing set as well.

Creating a bottleneck file for the training data. (Same step for validation and testing):

#training data

generator_top = datagen.flow_from_directory(

train_data_dir,

target_size=(img_width, img_height),

batch_size=batch_size,

class_mode=’categorical’,

shuffle=False)



nb_train_samples = len(generator_top.filenames)

num_classes = len(generator_top.class_indices)



# load the bottleneck features saved earlier

train_data = np.load(‘bottleneck_features_train.npy’)



# get the class labels for the training data, in the original order

train_labels = generator_top.classes



# convert the training labels to categorical vectors

train_labels = to_categorical(train_labels, num_classes=num_classes)

Creating our Convolutional Neural Network code:

#This is the best model we found. For additional models, check out I_notebook.ipynb start = datetime.datetime.now()

model = Sequential()

model.add(Flatten(input_shape=train_data.shape[1:]))

model.add(Dense(100, activation=keras.layers.LeakyReLU(alpha=0.3)))

model.add(Dropout(0.5))

model.add(Dense(50, activation=keras.layers.LeakyReLU(alpha=0.3)))

model.add(Dropout(0.3))

model.add(Dense(num_classes, activation=’softmax’)) model.compile(loss=’categorical_crossentropy’,

optimizer=optimizers.RMSprop(lr=1e-4),

metrics=[‘acc’]) history = model.fit(train_data, train_labels,

epochs=7,

batch_size=batch_size,

validation_data=(validation_data, validation_labels)) model.save_weights(top_model_weights_path) (eval_loss, eval_accuracy) = model.evaluate(

validation_data, validation_labels, batch_size=batch_size, verbose=1) print(“[INFO] accuracy: {:.2f}%”.format(eval_accuracy * 100))

print(“[INFO] Loss: {}”.format(eval_loss))

end= datetime.datetime.now()

elapsed= end-start

print (‘Time: ‘, elapsed)

Now we create our model. First step is to initialize the model with Sequential(). After that we flatten our data and add our additional 3 (or more) hidden layers. This step is fully customizable to what you want. We made several different models with different drop out, hidden layers and activation. But since this is a labeled categorical classification, the final activation must always be softmax. It is also best for loss to be categorical crossenthropy but everything else in model.compile can be changed. Then after we have created and compiled our model, we fit our training and validation data to it with the specifications we mentioned earlier. Finally, we create an evaluation step, to check for the accuracy of our model training set versus validation set.