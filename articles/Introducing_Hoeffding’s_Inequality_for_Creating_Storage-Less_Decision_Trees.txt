Introducing Hoeffding’s Inequality for Creating Storage-Less Decision Trees

© by my lovely wife Tinati Kübler

The Task

Imagine that you have a huge labeled dataset and you want to build a model for a prediction task. This can be Twitter, for example, where you have more Tweets (the features) than you can count including the corresponding number of likes (labels). Now you want to build a model that can predict whether a tweet will be liked more than 100 times or not, trained on all tweets written in the last year, i.e. we want to solve a classification task. A label of 1 means that the tweet has more than 100 likes, 0 otherwise.

Photo by Sara Kurfeß on Unsplash

You decide to use a Decision Tree or even something smart derived from it, like a Random Forest or Gradient Boosting. But Decision Trees, models based on them, and even other models share the following disadvantage:

You need the training data readily available in memory.

The Problem

In the best case, the complete training data fits into your local machine’s memory. However, you realize that the tweets dataset is larger than 8–32GB, so you are out of luck. Maybe you have access to a cluster with 512GB of RAM, but this is not large enough, too.

How large is the dataset actually? Let us do a rough estimation. Twitter itself and several other sources (here and here) report that there are around 6,000 Tweets per second. This means around 6,000 * 60² * 24 * 365 = 189,216,000,000‬ tweets per year. Let us assume that each tweet has a size of 140 bytes, one byte for each character in the bag-of-words encoding. We ignore that each tweet might have some metadata attached and that we could also use bigrams, trigrams, etc. This is a whopping 140 * 189,216,000,000‬ / 10⁹= 26,490 GB of tweet data for a single year!