Real Estate Investing in Texas

Introduction

For this project, I performed a time series analysis using Zillow’s historical median housing prices for the United States. The aim was to provide investors with the best zip codes to buy and develop homes in the state of Texas.

Let’s get started!

The Data

Figure 1: Zillow Median Housing Prices

The dataset contained descriptive variables like state, city, region ID, and county. It also contained a variable called SizeRank, which after data exploration, ranked zip codes by urbanization. The more urbanized area, the higher the size rank. The data also included the median housing prices from April 1996 to April 2018. These are the values that I used to perform time series analysis. Before diving into that analysis, some pre-requisites needed to be met to be considered for investment.

ZipCode Selection

The investor had several preferences to invest:

State: The investor wanted to invest specifically in the state of Texas because the state’s population has grown by 10 % in the past five years and the median age of people living in the state is 33.9 compared to the US median age of 37.4.

Urbanization: Zipcodes should be in the top 20 % according to the SizeRank variable

Median Housing Price: The zipcode’s average housing price should be between 1.5 deciles below and 1 decile above the zipcode’s 1-year median value. A one-year parameter was chosen because it is most realistic about the prices a real estate investor will face shortly.

Risk: Because the investor had a risk-averse strategy, zip codes were chosen according to a coefficient of variation below 60 percentile.

Returns: Once all the zip codes were filtered according to the investor’s preference, I chose the zip codes with the highest ROI.

When all the zip codes were filtered and ranked, I ended up the following five zip codes:

Figure 2: Final zip codes

The zipcodes pertained to the following cities in Texas:

Figure 3: Locations that suited the investor’s needs

Now that the 5 zip codes were obtained, I proceeded to perform a time series analysis to rank these zip codes according to forecasted 10-year returns.

Time Series Analysis

Now a time series can be plotted and we can visually inspect the data.

Figure 4: Exploring price change over time

The median housing prices had a positive trend and the prices are probably not stationary because the next period prices depended on the previous period price. So, I calculated the monthly returns, performed a model fit for each zip code and calculated forecasts using the monthly returns. The returns are more likely to be stationary and have a constant mean of zero. Next, I proceeded to calculate and plot the monthly returns for the five zip codes.

# new column for monthly returns for zc in range(len(dfs_ts)): dfs_ts[zc][‘ret’] = np.nan *len(dfs_ts) for i in range(len(dfs_ts[zc])-1): dfs_ts[zc][‘ret’][i+1] = (dfs_ts[zc].value.iloc[i+1]/dfs_ts[zc].value.iloc[i]) — 1



#plot the monthly returns for each zipcode for i in range(len(dfs_ts)): dfs_ts[i].ret.plot(figsize=(11, 5), color = 'b') plt.title(f'Zipcode: {dfs_ts[i].ZipCode[0]}') plt.xlabel('Date') plt.ylabel('Returns %') plt.legend(loc='best') plt.show()

Figure 5: Waxachie’s Monthly Returns

From the above plot, I noticed that there was no clear trend in the data and it appeared that the data was stationary, which is an assumption to build the model. A helpful visualization to check for stationary is the rolling mean and standard deviation which shouldn’t display a trend for the data to be stationary.

for i in range(len(dfs_ts)):

rolmean= dfs_ts[i].ret.rolling(window=12, center=False).mean()

rolstd = dfs_ts[i].ret.rolling(window=12, center=False).std()

fig = plt.figure(figsize=(11, 5))

orig = plt.plot(dfs_ts[i].ret,color='blue',label='Orginal')

mean = plt.plot(rolmean, color='red', label='Rolling Mean')

std = plt.plot(rolstd, color='black', label='Rolling Std')

plt.legend(loc='best')

plt.title(f'Rolling Mean & Standard Deviation for Zipcode: {dfs_ts[i].ZipCode[0]}')

plt.show()

Figure 6: Rolling Mean

While the data does not appear to be stationary, it is not enough to just rely on the visualization and proceed to fit time series models. Therefore, it is necessary to perform an Augmented Dickey-Fuller test for stationary.

Figure 7: Dickey-Fuller Results

From the results above, two of the five zip codes resulted in non-stationary at a 95 % confidence level. Therefore, these four zipcodes will need the ‘I’ parameter in the ARIMA model is going to be set to 1.

ARIMA Model

ARIMA stands for AutoRegressive Integrated Moving Average. So far, only the value for the ‘I’ parameter is known. Now it is time to search for the AR (p) and MA (q) parameters to best fit each zip code.

The AR & MA parameters are estimated using the autocorrelation function (ACF) and partial autocorrelation function (PACF) plots of stationary time series.

def acf_pacf(df, alags=48, plags=48):

'''Creates the ACF and PACF plots to

observe possible parameters for ARIMA model'''

#Create figure

fig,(ax1, ax2) = plt.subplots(2,1, figsize=(13,8))

#Create ACF plot

plot_acf(df, lags=alags, zero=False, ax=ax1)

#PACF plot

plot_pacf(df, lags=plags, ax=ax2)

plt.show()

def seasonal_plot(df, N=13, lags=[12,24,36,48,60,72]):

#differencing the rolling mean to find seasonality in acf plot

fig,(ax1, ax2) = plt.subplots(2,1,figsize=(13,8))

rolling = df - df.rolling(N).mean()

plot_acf(rolling.dropna(), lags=lags, ax=ax1)

plot_pacf(rolling.dropna(), lags=lags, ax=ax2)

plt.show();

Figure 8: ACF & PACF plots

Plots were created for each of the zip codes to find the best estimates for the AR and MA parameters. For most of the zipcodes, the plots followed a decaying pattern. Therefore, the models will likely include both parameters.

To further aid in finding the parameters, a nested for-loop was performed using the Akaike Information Criterion or AIC. AIC is a value provided by statsmodels that estimates the relative quality of a statistical model and provides a means for model selection. A model with many features that fits the data well will be assigned a larger AIC value than a model with fewer features to achieve the same goodness of fit. Therefore, we are interested in the lowest AIC value possible.

# Define the p, d and q parameters to take any value between 0 and 3

p = d = q = range(0,2) # Generate all different combinations of p, d and q triplets

pdq = list(itertools.product(p, d, q))

pdqs = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]

ns = []

for comb in pdq:

for combs in pdqs:

try:

mod = sm.tsa.statespace.SARIMAX(TS_75165,

order=comb,

seasonal_order=combs,

enforce_stationarity=False,

enforce_invertibility=False)

output = mod.fit()

ans.append([comb, combs, output.aic])

#print('ARIMA {} x {}12 : AIC Calculated ={}'.format(comb, combs, output.aic))

except:

continue

ans_df = pd.DataFrame(ans, columns=['pdq','pdqs', 'aic'])

ans_df.loc[ans_df['aic'].idxmin()]

From this estimate, the ARIMA parameters (p, I, q)obtained were (1, 1,1) and the seasonal components(P, D,Q,S) were (0,0,0,12). This tells us that the data has a seasonality of 12 periods. This was repeated for the other zip codes. The parameters were passed onto a model and summary is as follows:

Figure 9: Summary of Athe RIMA model

From the above plots, we can see that the KDE curve is relatively close to the N(0,1) curve which indicates that the residuals are normally distributed. The Q-Q plot also shows us that the residuals are normally distributed because the blue dots(the residuals) are along the red line(normal distribution). The graph at the top left doesn’t show any obvious seasonality and appears to be white noise. This is confirmed with the graph at the bottom right because the time series residuals have a low correlation with lagged versions of themselves.

The next step after fitting the model is to validate the model by comparing the predicted value to real values to the accuracy of our model.

def forecast(df, pred_date, start_date):

#make these global so they can be accessed later

global pred

global pred_conf



#get predictions from specific date and calculate confidence intervals

pred = output.get_prediction(start=pd.to_datetime(pred_date), dynamic=False)

pred_ci = pred.conf_int()



#plot real vs predicted values

rcParams['figure.figsize'] = 15,6



#plot observed values

ax = df[start_date:].plot(label='Observed')



#plot predicted values

pred.predicted_mean.plot(ax=ax, label='One-Step Forecast', alpha=.9)



#plot range of confidence intervals

ax.fill_between(pred_ci.index,

pred_ci.iloc[:,0],

pred_ci.iloc[:, 1], color = 'g', alpha=.5)

#axes labels

ax.set_xlabel('Date')

ax.set_ylabel('Monthly Returns')

plt.legend()

plt.show()



values_forecasted = pred.predicted_mean

values_truth = df



rmse = np.sqrt(mean_squared_error(values_truth[pred_date:], values_forecasted))

print('RMSE: '+str(rmse))

print('mean: '+str(df[pred_date:].mean()))

I decided to leave the last three years as the test set and train the model on everything before that point. Once a model was created, I calculated the RMSE on the train data set and then performed a one-step forecast on the test set to calculate the goodness of fit of the model.

Figure 10: One-step Forecast

As we can see from the graph above, the one-step forecast seems to align with the true values very well. It will also be useful to quantify the accuracy of our forecasts. We will use the MSE or mean squared error, which summarizes the average error of our forecasts.

With dynamic forecasting, we only use information from the time series up to a certain point, and after that, forecasts are generated using values from previous forecasted time points.

Forecasting

Once the model was trained, I used the model to generate the monthly returns for the next 10 years. With this information, I calculated the total returns for periods of 1, 3, 5, and 10 years.

Figure 11: 10-year forecast for Waxahachie

Findings

After performing the times series analysis on the five zip codes and forecasting the total returns for up to ten years, I recommended to the investor the following zip codes:

76028: Burleson, TX (448 % 10-year return) 75104: Cedar Hill, TX (187 % 10-year return) 75165: Waxahachie, TX (127 % 10-year return)

Having more domain knowledge in real estate investing will improve analysis. These models can be improved by taking other external variables into account. These other variables can include tax rates, crime rates, school district ratings, and interest rates. It may to interesting to compare a time series analysis on just housing prices and the monthly returns discovered in this analysis. The results from both would improve the decision-making process.

Please note that this project was just an exercise in time series modeling and shouldn’t be taken as an actual real estate investment.