Accuracy Performance Measures in Data Science: Confusion Matrix

A brief look into various ways by which you can assess your performance measures using Confusion Matrix for Data Science models Dhruv Sharma · Follow Published in Towards Data Science · 6 min read · Sep 17, 2019 -- Listen Share

Photo by Eleonora Patricola on Unsplash

In a previous article, I had briefly explained the intricate workings and trappings of a k-NN Model, let us now try to look briefly into first implementing the model and assess the adult income dataset, and then analyzing accuracy by various performance measures.

A brief look into the k-NN algorithm shows us that the algorithm is a classification algorithm, basically performed by identifying the nearest neighbors and then their classification into various classes. It can be used for binary or multi-class problems.

Photo by Alexander Mils on Unsplash

Adult Income Dataset

The model, that we will try to implement here is the Adult Income dataset. The dataset can be found at Kaggle, or on my GitHub repository. This dataset contains the income for adults across the United States and is one of the most common problems to solve using the k-NN algorithm. Here the income was originally continuous but has been made into binary, by using a filter of $50,000. Incomes greater than $50,000 are given a 1, while incomes lesser than that are given a 0. The dataset also has 14 other predictors. A brief, first few observations of the dataset shows the following:

Result from head() on the dataset

We can see from above, that the dataset has important descriptors that can be used to explain why a person’s income would be more or less than $50,000. We can also observe that the distribution of income levels is quite skewed, that is, there are more people with income less than $50,000 than people with income more than it.

More incomes lesser than $50,000 than above it

After performing the necessary data manipulation operations of dropping nulls and scaling the features, we get our final partial dataset as:

Partial dataset after data cleaning has been performed

Model Implementation

Now that the data has been cleaned, we can implement the model on it, and then move on to performing accuracy by various measures. As previously discussed, the k-NN model is implemented in various steps, which are:

Steps for conducting a k-NN analysis, source: Recent Trends in Big Data Using Hadoop

So, let us take the default number of neighbors (k=5), given by scikit-learn, and let us also take the baseline conditions, we can execute the model as:

And then we can assess how our model is performing using various type of assessment measures available to us.

Confusion Matrix

The most common type of metric available to us is the confusion matrix, which is also called the confidence matrix. The confusion matrix is a matrix that looks like:

Sample Confusion Matrix

What we can see from above is that the confusion matrix is a matrix between actual values vs predicted values. It is generally used for classification purposes, where it is necessary to predict the target as a 1 or 0. When we observe the actual value as absent, we give it a 0, and 1 otherwise. The same is done for predicted values as well. So, how is this important?

Well, we can tell a lot of things from this matrix, such as:

Our confusion matrix looks like:

Confusion Matrix for the Adult Income dataset

Accuracy: This is the rate of the classifier being correct, so basically take a sum of True Positive and True Negative values and then divide by total. So it means that there are a total of 14,653 values, out of which we have 10,109 True Positives and 2045 True Negative values. Therefore the accuracy of our model will be (10109 + 2045)/14653 = 82.94%. We can say that our model has good accuracy. It is also called Hit Ratio since it is the measure of total hits vs. all values.

This is the rate of the classifier being correct, so basically take a sum of True Positive and True Negative values and then divide by total. So it means that there are a total of 14,653 values, out of which we have 10,109 True Positives and 2045 True Negative values. Therefore the accuracy of our model will be (10109 + 2045)/14653 = 82.94%. We can say that our model has good accuracy. It is also called Hit Ratio since it is the measure of total hits vs. all values. Misclassification Rate (MISC) : This is the rate of values that, as the name suggests, were misclassified. It is also called as the Miss Ratio since it is the count of values that were missed. Therefore, if we subtract Accuracy from 100%, we will get the misclassification rate. Our MISC value here is 0.17 or 17%. It means that in our case, we can say that when the misclassification rate is 17% that out of 100 people in the dataset, 17 people were incorrectly classified.

: This is the rate of values that, as the name suggests, were misclassified. It is also called as the Miss Ratio since it is the count of values that were missed. Therefore, if we subtract Accuracy from 100%, we will get the misclassification rate. Our MISC value here is 0.17 or 17%. It means that in our case, we can say that when the misclassification rate is 17% that out of 100 people in the dataset, 17 people were incorrectly classified. Precision : This is the rate of values that measures the accuracy of positive predictions. So when we divide True Positives, by total positives, we get the precision value. Therefore our precision here is (2045)/(2045 + 1013) = 66.87%. It means that in our case, we can say that when precision is 66.81% that out of 100 people who were predicted to have income more than $50,000, 67 people were correctly classified.

: This is the rate of values that measures the accuracy of positive predictions. So when we divide True Positives, by total positives, we get the precision value. Therefore our precision here is (2045)/(2045 + 1013) = 66.87%. It means that in our case, we can say that when precision is 66.81% that out of 100 people who were predicted to have income more than $50,000, 67 people were correctly classified. Recall : This is the rate of values that measures positive instances that were correctly identified by the classifier. It is also called sensitivity, or the true positive rate. Thus recall is (True Positive)/(True Positive+False Negative) or in our case 2045/(2045 + 1486) = 57.91% . It means that in our case, we can say that when the recall is 57.91% that out of 100 people who have income more than $50,000, 57.91 or 58 people were correctly classified.

: This is the rate of values that measures positive instances that were correctly identified by the classifier. It is also called sensitivity, or the true positive rate. Thus recall is (True Positive)/(True Positive+False Negative) or in our case 2045/(2045 + 1486) = 57.91% . It means that in our case, we can say that when the recall is 57.91% that out of 100 people who have income more than $50,000, 57.91 or 58 people were correctly classified. Specificity : This is the rate of values that measures negative instances that were correctly identified by the classifier. It is also called the True Negative rate. Specificity is then, (True Negatives) / (True Negatives + False Positives) or in our case, 10109/(10109 + 1013) or 90.89%. It means that in our case, we can say with 90.89% specificity that out of 100 people who did not have income more than $50,000, 90.89 or 91 people were correctly classified.

: This is the rate of values that measures negative instances that were correctly identified by the classifier. It is also called the True Negative rate. Specificity is then, (True Negatives) / (True Negatives + False Positives) or in our case, 10109/(10109 + 1013) or 90.89%. It means that in our case, we can say with 90.89% specificity that out of 100 people who did not have income more than $50,000, 90.89 or 91 people were correctly classified. F-1 Score: It is the harmonic mean of precision and recall. The normal mean gives equal preference to all values, while F-1 score gives much more weight to low values. F-1 Score is basically the below:

F-1 Score

In our case, F-1 Score is given as:

Precision, Recall and F1- Score values for 1 and 0 respectively

Receiver Operating Characteristics and Area Under the Curve: Receiver Operating Characteristics curve, also known as the ROC Curve, is basically a plot of True Positive Rate vs False Positive Rate. False Positive rate is the ratio of negative instances that were incorrectly classified. It can also be defined as 1- True Negative Rate, which is also the Specificity. Therefore this curve can also be thought of as a curve between Sensitivity and 1- Specificity. In Python, it can be implemented using the scikit-learn or the matplotlib library. The area under the curve is also another important measure here. The more it is closer to 1, the better the classification has been performed. The curve for our case looks like:

Code to draw the ROC Curve, Source: Hands-on ML by Aurelien Geron

ROC Curve and the AUC at bottom

Cumulative Accuracy Profile: This curve is similar to the ROC Curve, however, it is a chart of accuracy. It basically plots accuracy and helps to understand and conclude about the robustness of the model.

Code for making the curve:

Code for drawing the CAP Curve

CAP Curve

Having said the above, it is important to these note that there are multitudes of other information that you can use to make it sure that your model has the required accuracy and performance. These include Coefficient of Variation, Root Mean Square Error, and others but this article was for Confusion Matrix only.