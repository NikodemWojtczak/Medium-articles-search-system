Everything You Ever Wanted to Know About K-Nearest Neighbors

Photo by Nina Strehl on Unsplash

K-Nearest Neighbors is one of the simplest and easiest to understand machine learning algorithms. It can be used for both classification and regression tasks but is more common in classification, so we will start with that use case. The principles, though, can be used in both cases.

Here is the algorithm:

Define k Define a distance metric — usually Euclidean distance For a new data point, find the k nearest training points and combine their classes in some way — usually voting — to get a predicted class

That’s it!

Some of the benefits:

It doesn’t require any training in the traditional sense. You just need a fast way to find the nearest neighbors.

Easy to understand and explain.

Some of the negatives:

You need to define k, which is a hyper-parameter, so it can be tuned with cross-validation. A higher value for k increases bias and a lower value increases variance.

You have to choose a distance metric and could get very different results depending on the metric. Again, you can use cross-validation to decide on which distance to use.

It doesn’t really offer insights into which features might be important.

It can suffer from high dimensional data due to the curse of dimensionality. Basically, the curse of dimensionality means that in high dimensions, it is likely that close points are not much closer than the average distance, which means being close doesn't mean much. In high dimensions, the data becomes very spread out, which creates this phenomenon. There are so many good resources for this online, that I won’t go any deeper.

Basic assumption:

Data points that are close share similar values for our target class or value.

Voting Methods