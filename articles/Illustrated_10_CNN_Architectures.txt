What architecture is this? ğŸ¤”

Member-only story

Illustrated: 10 CNN Architectures

(TL;DR â€” jump to the illustrations here)

Changelog:

5 Jan 2022 â€” Fix typos and improve clarity

28 Nov 2020 â€” Updated â€œWhatâ€™s novelâ€ for every CNN

17 Nov 2020 â€” Edited no. of layers in last dense layer of Inceptionv1 from 4096 to 1000

24 Sep 2020 â€” Edited â€œWhatâ€™s novelâ€ section for ResNeXt-50

How have you been keeping up with the different convolutional neural networks (CNNs)? In recent years, we have witnessed the birth of numerous CNNs. These networks have gotten so deep that it has become extremely difficult to visualise the entire model. We stop keeping track of them and treat them as black-box models.

Fine, maybe you donâ€™t. But if youâ€™re guilty too then hey, youâ€™ve come to the right place! This article is a visualisation of 10 common CNN architectures, hand-picked by yours truly. These illustrations provide a more compact view of the entire model, without having to scroll down a couple of times just to see the softmax layer. Apart from these images, Iâ€™ve also sprinkled some notes on how they â€˜evolvedâ€™ over time â€” from 5 to 50 convolutional layers, from plain convolutional layers to modules, from 2â€“3 towers to 32 towers, from 7â¨‰7 to 5â¨‰5â€” but more on these later.

By â€˜commonâ€™, I am referring to those models whose pre-trained weights are usually shared by deep learning libraries (such as TensorFlow, Keras and PyTorch) for users to use, and models that are usually taught in classes. Some of these models have shown success in competitions like the ImageNet Large Scale Visual Recognition Challenge (ILSVRC).

The 10 architectures that will be discussed and the year their papers were published.

Pre-trained weights are available in Keras for 6 of the architectures that we will talk about. Adapted from a table in the Keras documentation.

The motivation for writing this is that there arenâ€™t many blogs and articles out there with these compact visualisations (if you do know of any, pleaseâ€¦