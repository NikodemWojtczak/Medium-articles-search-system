A Guide to Integrating Text Analytics into Tableau Lovelytics · Follow Published in Towards Data Science · 12 min read · Oct 11, 2019 -- 3 Listen Share

Data is often dirty and messy. Sometimes, it doesn’t even come in the right form for quick analysis and visualization. While Tableau (and Prep) had several tools to deal with numeric, categorical, and even spatial data, one consistent missing piece was handling unstructured text data.

Not anymore.

In the latest edition of Tableau Prep (2019.3), released just a few weeks ago, Prep now natively supports custom R or Python scripts. By using TabPy, one can use the entire suite of R and Python libraries on any datasets without having to leave the Tableau universe. From running machine learning models to calling geospatial APIs, the possibilities are pretty much endless.

In this article, we take a crack at something new and exciting: applying natural language processing techniques to unstructured text data using Tableau Prep and Python. Before we dive into that, we start with an in-depth guide on how to set everything up.

Part 1: Setting up Python for Tableau Prep

The first big question is: how do you get Python to work with Tableau Prep? Installation is pretty easy and straightforward — just follow the instructions on the website here. Our suggestion is to use Anaconda to install the package, rather than pip, to avoid potential dependency errors.

(We ran our code on a MacBook so that’s what we’re using as the reference in our guide, sorry Windows users!)

There are two really useful things to note that will hopefully spare you the frustration we experienced:

After installing TabPy with conda , you have to downgrade your version of tornado to 5.1.1, or you will run into an asynchronous error when trying to run TabPy. This is also listed on the (very short) FAQ section. Running the command tabpy in Terminal did not work for me. Instead I used cd to change the directory to the TabPy folder (try finding it somewhere like anaconda3/envs/[my_env]/lib/python3.7/site-packages/tabpy_server ). Then, I ran the startup file by keying in sh startup.sh .

That’s it! But the second big question is: how exactly does Tableau Prep use the Python Script?

A simple diagram to explain what is going on behind the scenes

Essentially, Tableau Prep exports the data as a Pandas dataframe, applies a specified function within the custom Python script to the dataframe, and returns a new dataframe which is passed back into Tableau Prep.

It’s helpful to keep the whole flow in mind, especially when writing the customized function in the Python script. In particular, note that your function has to take in a Pandas dataframe and return another Pandas dataframe.

Now onto the fun part: writing the Python script!

Your Python script needs at least two functions — one for what you are trying to do in Python, and another to define the output schema. Let’s start with the latter.

get_output_schema() helps Tableau Prep understand the structure of the data that is being returned to it from Python. You need this function if the dataframe you are returning to Tableau Prep has different columns from the original data. It takes the following form:

There are six data types, all pretty self-explanatory and shown in the function above. Note that they must be mapped correctly to the actual data type of each column in the dataframe for this to work.

As for your own custom function, here is a very simple one which does some basic data processing:

As outlined in the diagram earlier, the function takes in the raw data as a Pandas dataframe, and returns the cleaned data as a Pandas dataframe. You can write any function here, as long as you import the required libraries within the script itself.

You can also include more than one function in your script, but each scripting tool in Tableau Prep can only call one function at a time. If you plan to use multiple functions for different datasets, you can simply put them all into the same Python script but call only the relevant function when you need it. This saves you the hassle of creating multiple scripts.

Put these two functions together into a single script, and you’re done. Now, we finally get to run it on Tableau Prep!

First, we open up a new Tableau Prep workflow by bringing in a CSV file.

This CSV file only has three columns — Category , Title , and Text . Then, we add the “Script” tool to the workflow, which brings us to this page:

For now, we get an error because the TabPy server hasn’t been set up yet. Open up Terminal, navigate to the folder tabpy_server , and run the command sh startup.sh . The following should appear:

Remember the port number here, then go back to Tableau Prep, click on the button which says ‘Connect to Tableau Python (TabPy) Server’, and key the following in:

Key in the port number that was specified in Terminal! It’s not always 9004.

Click ‘Sign In’, and it takes you back to the main Tableau Prep window. Select your Python script file and then enter the name of your function into the box right below the button. The workflow runs automatically as long as your script works fine.

Everything is standard Tableau Prep stuff from here on. Of course, data cleaning is the least exciting thing to do in Python, especially since Tableau Prep already does it so well. Now that we’ve learned how to do the basic stuff, we can take the training wheels off and start integrating more advanced functionalities into the workflow.

Part 2: Natural Language Processing in Tableau

Most text data is unstructured, and NLP can help us to analyze all this data.

What is natural language processing (NLP)? Put simply, NLP seeks to parse and analyze what and how we communicate. While code is always precisely and clearly defined, how we write or speak is constrained only by grammatical rules. Context and meaning give shape to our writing and speech, and that ambiguity makes it difficult for computers to understand exactly what we mean.

Unsurprisingly, NLP has become a fairly common part of everyday life. Spam filters, translation, and virtual assistants are some of the many examples of how NLP is being applied in the real world. There is so much potential in this field because of how it bridges the gap between what we find intuitive and what the computer can understand.

That’s why bringing text analytics into Tableau is such a game-changer. With so much unstructured text data out there, being able to visualize such data effectively will help us better perceive general trends and investigate important outliers.

For this article, we work with a publicly available dataset containing 2225 BBC news articles (many thanks to Greene and Cunningham for freely providing this dataset!). Each of these articles are assigned one of five categories, and individually come in a .txt file.

We process this script using Python in order to generate a CSV file for Tableau Prep. Here’s the code we used:

Now that we have the data in the right format, let’s look at how we want to analyze the unstructured text. Let’s look at two news articles to get a sense of what the underlying data is like.

Here’s one from the ‘business’ category:

Peugeot deal boosts Mitsubishi Struggling Japanese car maker Mitsubishi Motors has struck a deal to supply French car maker Peugeot with 30,000 sports utility vehicles (SUV). The two firms signed a Memorandum of Understanding, and say they expect to seal a final agreement by Spring 2005. The alliance comes as a badly-needed boost for loss-making Mitsubishi, after several profit warnings and poor sales. The SUVs will be built in Japan using Peugeot’s diesel engines and sold mainly in the European market. Falling sales have left Mitsubishi Motors with underused capacity, and the production deal with Peugeot gives it a chance to utilise some of it. In January, Mitsubishi Motors issued its third profits warning in nine months, and cut its sales forecasts for the year to March 2005. Its sales have slid 41% in the past year, catalysed by the revelation that the company had systematically been hiding records of faults and then secretly repairing vehicles. As a result, the Japanese car maker has sought a series of financial bailouts. Last month it said it was looking for a further 540bn yen ($5.2bn; £2.77bn) in fresh financial backing, half of it from other companies in the Mitsubishi group. US-German carmaker DaimlerChrylser, a 30% shareholder in Mitsubishi Motors, decided in April 2004 not to pump in any more money. The deal with Peugeot was celebrated by Mitsubishi’s newly-appointed chief executive Takashi Nishioka, who took over after three top bosses stood down last month to shoulder responsibility for the firm’s troubles. Mitsubishi Motors has forecast a net loss of 472bn yen in its current financial year to March 2005. Last month, it signed a production agreement with Japanese rival Nissan Motor to supply it with 36,000 small cars for sale in Japan. It has been making cars for Nissan since 2003.

And here’s one from the “tech” category:

The year search became personal The odds are that when you fire up your browser, you go straight to your favourite search engine, rather than type in a web address. Some may see this as the height of laziness, but in an era of information overload, search has become a vital tool in navigating the net. It is symptomatic of how the way we use the internet is changing. And as Google has shown, there is money in offering a service that people cannot live without. There is no shortage of companies vying for the loyalty of web searchers, offering a wealth of different services and tools to help you find what you want. Over the past 12 months, giants of the technology world such as Microsoft and Yahoo have sought to grab a slice of the search action. “User experience has contributed to people searching more,” said Yonca Brunini of Yahoo. As people become more familiar with the internet, they tend to spend more time online and ask more queries, she said. “The other second thing is broadband,” Ms Brunini told the BBC News website. “This will do to internet what colour has done to TV.” But search is hardly a new phenomenon. It has been around since the early days of the net. Veteran surfers will remember old-timers like Hotbot and Altavista. “Search was always important,” said Urs Holzle, Google vice-president of operations. “We trumpeted that in 1999. It is even truer now as there are more users and more information.” “People didn’t realise that search was the future. The financials have something to do with it.” Google has shown web commerce can work through its targeted small adverts, which appear at the top and down the right-hand side of a page and are related to the original search. These small ads helped Google reach revenues of $805.9m for the three months to September. Others have woken up to the fact that you can make money out of web queries. “Once you see there is a market, Microsoft is bound to step to it. If Microsoft sees search as important, then nobody queries it,” said Mr Holzle. Microsoft is just one of the net giants muscling in on search. Yahoo, Ask Jeeves, Amazon and a handful of smaller outfits are all seeking to capture eyeballs. Web users face a plethora of choices as each company tries to outflank Google by rolling out new search products such as desktop search. It reflects how the battlefield has shifted from the net to your PC. Search is not just about finding your way around the web. It is now about unlocking information hidden in the gigabytes of documents, images and music on hard drives. For all these advances, search is still a clumsy tool, often failing to come up with exactly what you had in mind. In order to do a better job, search engines are trying to get to know you better, doing a better job of remembering, cataloguing and managing all the information you come across. “Personalisation is going to be a big area for the future,” said Yahoo’s Yonca Brunini. “Whoever cracks that and gives you the information you want is going to be the winner. We have to understand you to give you better results that are tailored to you.” This is perhaps the Holy Grail of search, understanding what it is you are looking for and providing it quickly. The problem is that no one yet knows how to get there.

What kinds of data do we want to extract from these news articles? Some things come to mind:

Who or what is mentioned in the article? Is the article written in a positive or negative tone? What is the general topic being discussed?

Coincidentally, each of the three questions corresponds to a specific sub-field in NLP. The first is entity recognition, the second is sentiment analysis, and the third is topic modeling. In this article, we’ll work with just the first two.

To do that, we rely on two well-known NLP libraries in Python: Spacy and TextBlob. Make sure you install both packages before proceeding!

Now, we simply have to write our script using these libraries!

This article isn’t meant to be a guide to NLP on Python (there are many articles and websites which already do that), so we will skip over explaining how both packages work. We’ve left comments in the code to give a good idea about what we’re doing at each part.

In the previous section, we went into the nitty-gritty of how to integrate Python scripts into the Tableau Prep workflow, so using the script above should be easy! We imported the BBC data CSV file, entered the port number, selected the Python script, and entered the function name.

Within 2 minutes, we see…

… that the Python script works perfectly!

One of the best parts about Tableau Prep is the ability to look at the data in aggregate. We can see immediately that most articles have less than 3750 characters and typically have a polarity score of around 0 (-1 being negative and 1 being positive), which aligns fairly well with our impression of BBC News as a concise and neutral news source.

We also have data on the people, countries, and organizations which are mentioned in each article. While there are some errors (“Climate” is not an organization), the Spacy library was broadly accurate in identifying these named entities.

Let’s dive a bit deeper into what exactly was returned for the two articles we looked at earlier. For the article on Peugeot:

And for the article on personalized search:

In both cases, Spacy correctly identified the organizations, persons, and countries being mentioned in the article. Notice also how the personalized search article scores higher in both Text_Subjectiveness and Text_Polarity . This aligns with what we read: it was more focused on building a narrative about how personalized searches became more common, and was fairly supportive of the trend. In contrast, the Peugeot article was simply describing industry movements.

By integrating text analytics into Tableau Prep, we were able to extract useful information out of the unstructured text data contained in the news articles. And there’s much more that we could have done — topic modeling, document similarity, emotion detection, and so on.

Final Thoughts

We hope this guide has been useful in explaining how to integrate custom Python scripts into Tableau Prep, especially in the field of natural language processing. Once you get the hang of it, writing custom scripts for API calls, geospatial analysis, or even machine learning is really straightforward.

And when that happens, the sky’s the limit.

— Shaun Khoo