Deep Dive into SF Crime

San Francisco is famous for many things: its vibrant tech environment, the iconic Golden Gate, charming cable cars and (arguably) the world’s best restaurants. It is also the heart of LGBT and hipster culture which makes it an extremely attractive tourist and migration destination. However, with thriving tourism, rising wealth inequality, and thousands of homeless people there is no scarcity of crime in the city. In this post, I invite you to dive into the San Francisco Crime data to get some insights into the SF crime environment and engineer features for your own crime classification model.

Exploratory Analysis

You can download San Francisco Crime classification data from Kaggle. The dataset contains nearly 800,000 observations of crime reports from all the city’s neighborhoods in 2003–2015. It includes the following variables:

San Francisco Crime Rates Dataset Variable Description

Let’s first explore our target variable and find out which 10 crime types are the most common in San Francisco. We will sort the categories by the number of incidences and then use the horizontal bar chart to present our findings:

# Get 10 most common crimes

most_common_cat = train['Category'].value_counts()[0:9].sort_values()

most_common_cat.values





categs = most_common_cat.index

y_pos = np.arange(len(categs))

counts = most_common_cat.values



plt.barh(y_pos, counts, align='center', alpha=0.5)

plt.barh(y_pos, counts, align='center', alpha=0.5)

plt.yticks(y_pos, categs)

plt.xlabel('Number of Incidences')

plt.show()

Most Common Crime Types

It’s quite comforting to know that violent crimes are not at the top of the crime incidence list. However, property crimes seem to be quite common. Let’s now explore which districts have the highest number of registered crimes.

To do so, we will use Folium which is an easy to use tool that creates interactive maps. To run the chunk of code below, you’d need to install Folium by running pip install folium in your terminal or directly inside the notebook by adding “!” in front of the command. Then you’d need to download the JSON file as specified in this cell:

by_zone = train.apply(pd.Series.value_counts).reset_index()



# Load SF data

!wget --quiet https://cocl.us/sanfran_geojson -O sf_neighborhoods.json

sf_zones = r'sf_neighborhoods.json'



SF_COORDINATES = (37.76, -122.45)





# Create an empty map zoomed in on San Francisco

sf_crime_map = folium.Map(location=SF_COORDINATES, zoom_start=12)





sf_crime_map.choropleth(

geo_data=sf_zones,

data=by_zone,

columns=['index', 'PdDistrict'],

key_on='feature.properties.DISTRICT',

fill_color='YlOrRd',

fill_opacity=0.7,

line_opacity=0.2,

legend_name='San Fransisco Crime by Neighborhood'

)



sf_crime_map

The output of this cell is an interactive map with the number of crime incidences sorted by police department districts:

San Francisco Crime Rate by Neighborhood

Feel free to experiment with this plot by plotting individual crime categories or checking the change in the distribution over time.

Next, let’s look at the distribution of crimes over weekdays. I first built a crosstable to get the crime counts per weekdays. After that, I normalized the counts and visualized them using a heatmap from the seaborn library:

# Extract the most common crimes from the data

most_commons = train[train['Category'].apply(lambda x: x in categs)]



# Build a cross table to get the number of each crime type per day of week

cat_per_week_common = pd.crosstab(most_commons['Category'], most_commons['DayOfWeek'])

# Calculate percentages of crimes

cat_per_week_common = cat_per_week_common.div(cat_per_week_common.sum(axis=1), axis=0)

# Rearrange columns

cat_per_week_common = cat_per_week_common[['Monday',

'Tuesday', 'Wednesday',

'Thursday', 'Friday',

'Saturday','Sunday']]

# Transform into a heat map

fig = plt.figure(figsize=(10,10))

ax = sns.heatmap(cat_per_week_common,

cmap="BuPu", linewidths=.5)

plt.xticks(fontsize=12,rotation=45,ha='right')

plt.yticks(fontsize=12)

plt.xlabel("")

plt.ylabel("")

Most Common Crimes per Day of Week

As the graph above illustrates, most crime categories such as assaults and acts of vandalism happen during the weekend when people go out. Some other crime types are more often registered during weekdays which might be associated with police working hours.

Finally, let’s see the success rate of the SF police department. Specifically, how many violent crimes are getting resolved. Let’s first subset violent crimes from the list of categories. I picked the ones that I consider interesting, but feel free to explore other categories as well! After that, we can create the arrest variable which will divide all the possible resolutions into two classes: prosecuted or not. I’m making an assumption that ‘NOT PROSECUTED’ and ‘NONE’ are the only two resolutions that correspond to the negative class. We will calculate the proportion of solved cases and plot them using a horizontal plot. This time, we will plot proportions, not absolute counts:

# Pick crime types of interest

violent = train[train.Category.isin(['ASSAULT', 'BURGLARY',

'KIDNAPPING', 'ROBBERY',

'SEX OFFENSES FORCIBLE'])].copy()

# Create Arrest variable

violent['Arrest'] = np.where(violent['Resolution'].isin(['NONE', 'NOT PROSECUTED']), 0,1)



# Calculate counts

arrest_counts = violent['Category'][violent.Arrest==1].value_counts()[0:9]

total_counts = violent['Category'].value_counts()[0:9]

arrest_counts = arrest_counts/(total_counts).sort_index()

total_counts = total_counts/(total_counts).sort_index()



# Plot values

total_counts.plot.barh(color='crimson', label= 'Unsolved')

arrest_counts.plot.barh(color='mediumseagreen', label='Solved')

plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)

plt.xlabel('Proportion')

plt.show()

Solved and unsolved violent crimes in SF

We can see that the police success rate is below 50% for all the violent crimes we picked. In the case of burglary, it is just 17%.

Feature Engineering

Our dataset has many observations but only a limited number of features. In this section, we will create two sets of features: temporal and spatial. Temporal features can be extracted from the Dates variable. Apart from the obvious features such as month, day, and hour, I extracted business hours, weekends, and national holidays. To get access to all the US holidays, simply import USFederalHolidayCalendar from pandas.tseries.holiday. Here’s the function that will help you to extract all the features:

def time_engineer(data):

'''

Extract temporal features from dates.

'''

# Turn strings into timestamp objects

data.Dates = pd.to_datetime(data.Dates)



# Extract years, months, times of the day, and weeks of year

data['Year'] = data['Dates'].dt.year

data['Month'] = data['Dates'].dt.month

data['Day'] = data['Dates'].dt.day

data['Hour'] = data['Dates'].dt.hour

data['WeekOfYear'] = data['Dates'].dt.weekofyear



# Add a dummy for public holidays

cal = calendar()

holidays = cal.holidays(start=data['Dates'].min(), end=data['Dates'].max())

data['Holiday'] = data['Dates'].dt.date.astype('datetime64').isin(holidays).astype('bool')



# Add times of a day

data['Night'] = np.where((data['Hour']< 6), 1, 0)

data['Morning'] = np.where((data['Hour']>=6) & (data['Hour']<12), 1, 0)

data['Afternoon'] = np.where((data['Hour']>= 12) & (data['Hour']<18), 1, 0)

data['Evening'] = np.where((data['Hour']>= 18) & (data['Hour']<24), 1, 0)

data['BusinessHour'] = np.where((data['Hour']>= 8) & (data['Hour']<18), 1, 0)



# Add seasons

data['Spring'] = np.where((data['Month']>=3) & (data['Month']<6), 1, 0)

data['Summer'] = np.where((data['Month']>=6) & (data['Month']<9), 1, 0)

data['Autumn'] = np.where((data['Month']>=9) & (data['Month']<12), 1, 0)

data['Winter'] = np.where((data['Month']<=2) | (data['Month']==12), 1, 0)



# Encode weekdays

data_dummies = pd.get_dummies(data['DayOfWeek'])

data = pd.concat([data, data_dummies], axis=1)



# Create a dummy for weekends

data['Weekend'] = np.where((data['DayOfWeek']=='Saturday') & (data['DayOfWeek']=='Sunday'), 1, 0)



# Encode districts

data_dummies = pd.get_dummies(data['PdDistrict'])

data = pd.concat([data, data_dummies], axis=1)

data = data.drop(columns=['PdDistrict'])

# Drop categorical variables and variables that are not in test set

# School valiables contain too many NaNs

data.drop(columns=(['Address', 'Dates', 'Descript', 'DayOfWeek',

'Resolution', 'Enrolled In Public School',

'Enrolled In Private School', 'Not Enrolled In School']))

return data

Word Cloud of Temporal Features

Extracting spatial features is a bit more tricky since they are based on the uszipcode database that can be loaded as a package. In the notebook, you will find all the functions that are necessary to clean up the zipcode data and extract relevant demographics features. Be aware that the feature engineering process takes a lot of time (a couple of hours), mostly because there is a need to impute zip codes for every latitude and longitude.

Your final dataset should include 89 variables that contain information about the temporal and spatial aspects of crime incidences. Now, feel free to play around with this data and train your own model that will score at the top of the Kaggle leaderboard! To get you started, here’s a simple function that trains a model, makes predictions for the test set and calculates the logloss as specified in the challenge:

# Try out different models

models = [LogisticRegression, RandomForestClassifier, KNeighborsClassifier] def run_model(model, X_train, y_train, X_test, y_test):

model = model()

model.fit(X_train, y_train)

y_preds = model.predict(X_test)

return (log_loss(y_test, y_preds))



results = [run_model(model) for model in models]

Conclusion

In this post, we looked into the San Francisco Crime Classification dataset and learned how to produce data visualizations to explore different aspects of the data. We also used geographical location and dates to engineer spatial and temporal features. In a different post (hopefully) we will explore model and feature selection techniques, hyperparameter tuning, and some popular dimensionality reduction methods! Stay tuned :)

Full notebook: https://github.com/ritakurban/Practical-Data-Science/blob/master/SF_crime.ipynb