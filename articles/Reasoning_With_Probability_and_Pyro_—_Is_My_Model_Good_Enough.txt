Check accuracy again, but only on 100 test observations

>>> Accuracy is 86.00%

We got a slightly lower number, but still comparable.

That said, while the numbers are close, having faith in the model based on 100 examples is harder than having faith based on 14,000 examples.

Intuitively, our confidence in the test set’s score increases as the size of the test set increases. Pyro allows us to numerically estimate our confidence and how much room for error we have.

The Pyro Model

The Pyro workflow is very unique and requires three components

A model function which simulates our underlying model (not the decision tree, but the process which gives us correct or incorrect labels). This function starts with a prior distribution A kernel which measures the likelihood of the observed examples as they are produced by the model function A sampler which builds the posterior distribution

First we need to define our observations for Pyro. In our case, our observations are the cases where we see correct and incorrect classification. Let’s consider both the small test set (100 observations) and the large test set (14,000 observations).

Now we have to define the model function. The function accepts our observations and tries to sample from a prior distribution and calculate the likelihood of the prior based on our observations. The kernel will then update the prior to a more likely posterior distribution.

It doesn’t look like much, but in this model two important events happen.

First, we have used the `pyro.sample` function to register a parameter named ”p” as a learnable value for Pyro. The use of either `pyro.sample` or `pyro.param` registers the resulting value with Pyro’s internal store (a special dictionary-like object) as learnable values. In our case we’ve said ”p” is a learnable distribution.

We’ve also registered every observation as a learnable value which should comply with the observation we provide to it.

The kernel we will register this model with (Hamiltonian Monte Carlo kernel) will look at all learnable values defined in this model and will attempt to adjust the learnable distributions such that they increase the likelihood of provided observations.

The sampler we will use (Markov Chain Monte Carlo) will run the HMC kernel to find a posterior distribution.

>>> Sample: 100%|█████████████████████████████████████████| 200/200 [03:46, 1.13s/it, step size=1.51e+00, acc. prob=0.916]

Now we’ve run our sampler, we can do several things with our resulting posterior probability. First, we may want to visualize the probability ”p” we’ve defined. We can use do so by sampling from our sampler.

The posterior distribution for our accuracy score given 100 examples

>>> mean std median 2.5% 97.5% n_eff r_hat

>>> p 0.86 0.04 0.86 0.79 0.93 185.83 1.00



>>> Number of divergences: 0

Not such a great result… 100 observations is not really enough to settle on a good outcome. The distribution does not seem to center around a particular value, and when we ask for the 95% credibility interval our true value can lie anywhere between 79% and 93%. This may or may not be accurate enough for our purposes.

Let’s see how confident we can be in our model when we use all 14,000 observations

Modifying Our Model

If we run all 14,000 observations through the same model, it would take a very long time to run.

This is because we cycle through each observation in our code.

Pyro contains a more convenient, vectorized, method of approaching our model.

First, we redefine our model function such that it accepts NO observations, but rather it returns its own observations

Now, we defined a second function that takes as input a model function, and observations, and utilizes `pyro.poutine` to run the model function in a conditioned environment. It’s important our observations here have the same name (“obs”) as they do in the model function.

Finally, we re-run the MCMC sampler, but now with our conditioned model, and we send our model function, as well as our observations, as arguments

>>> Sample: 100%|█████████████████████████████████████████| 200/200 [01:47, 1.86it/s, step size=7.87e-01, acc. prob=0.984]

The posterior distribution for our accuracy score given 14,000 examples

>>> mean std median 2.5% 97.5% n_eff r_hat

>>> p2 0.87 0.00 0.87 0.87 0.87 9.88 1.07



>>> Number of divergences: 0

Now we can get a much tighter fit around the 87% mark. Just to compare the two distributions, we could plot them together.

The two posterior distributions for our accuracy based on the number of support examples

I hope you all enjoyed reading this brief intro to Pyro and its capabilities. I certainly look forward to trying new things and working with probabilistic programming more!