Note: This is part-6 of a series of articles on ‘Security and Privacy in Artificial Intelligence & Machine Learning’. Here are the links to all articles (so far):

Photo by Jason Blackeye on Unsplash

In the previous article of the series, we looked at the nature and extent of damage that attackers can inflict if they, too, start leveraging the capabilities of AI/ML something that is but inevitable. In this part we will shift our attention towards another aspect by taking a closer look at privacy in the context of machine learning.

The advancements in ML/AI have thrown a big wrench in the works as far as privacy is concerned. Each new scenario — however compelling from the functionality and utility standpoint — seems to bring in scary new ways to impact and compromise data privacy.

Let us examine the many ways privacy surfaces in the context of AI/ML — governance, individual and organizational motivations, technical mechanisms for privacy assurance, regulatory aspects, etc.

Privacy governance in the brave new world

When introducing the privacy governance challenges in view of the immensely lucrative possibilities of AI/ML scenarios, few things can serve better than a close look at the ‘Cambridge Analytica’ controversy.

If you haven’t heard of Cambridge Analytica, firstly, welcome back to Earth! :-). The gist of the matter is that in 2014 a professor of psychology from Cambridge University published a ‘psychometry’/‘personality profile’ application called ‘This is your digital life’ on the Facebook app platform. The app, at least on paper, took consent from users to collect some of their profile/personal information. Being a personality test, it became quite popular and, by 2016, about 50 million US *citizens* had signed-on and taken the test (giving the author access and insights into the personality of 50 million citizens!).

However, there were a few problems with how and why this was done. First, the app collected way more personal data than it had obtained consent for. Second, the professor was in a business arrangement with a company called Cambridge Analytica (CA) that specialized in consulting for large election campaigns. This professor (who happened to be partly Russian, partly American), sold the data for about 200 million dollars to CA. Further to this, one of the big investors in Cambridge Analytica was a Republican senator who was also a key aide in President Trump’s election campaign. The rest is for us to connect the dots.

The snip below is from an email exchange between the professor and the team he worked with at CA (of which Chris Wylie became a whistleblower).

“This is a start to get you thinking about what you may want…”

Not only does the CA controversy highlight how our longstanding institutions like democracy can be ‘mass social-engineered’ using techniques from the big data analytics/AI&ML world, it also makes an excellent study of the failure of governance across several systems and organizational boundaries. It serves as a grave reminder of how protection of our privacy sits atop flimsy promises which can get washed away in a gush of compelling business value propositions.

As AI & ML-based business opportunities expand and become more pervasive, privacy and security professionals in all walks of life are likely to face similar dilemmas of where to draw a line in terms of gathering and retaining personal data and being transparent about its use. It was never a black or white thing to begin with — but now there will likely be many more shades of gray.

As we see this unfold it is important for the hapless consumer to remember this:

On the internet *you* are the product!

While on the topic, another thing to be aware of is that the trail of ‘likes’ we leave on social networking platforms has been proven to be a rich source of information about us. This study demonstrated that with knowledge of as few as 10 likes a computer model can judge or understand someone’s personality as well as a work colleague, at 70 likes it can outperform a friend, at 150 likes a family member, and at 300 a spouse!!

Big data, privacy and anonymity

Although AI/ML push the challenges to a whole new level, concerns about privacy have been rife since the early days of big data itself (i.e., well before machine learning came back into mainstream).

A classic problem that many big data scenarios brought in was the challenge of ‘inference control’ — the ability to share extracts from large scale datasets for various studies/research projects without revealing privacy sensitive information about individuals in the dataset. Before delving into privacy aspects in the machine learning context, let us explore the techniques that were developed and employed over the years when mining large datasets.

Privacy concerns have been around since early days of Big Data

Databases containing data about people usually have columns/attributes that — from a privacy standpoint — can be of one of the following types:

(a) Personally Identifiable Information (PII) — these are columns which can pretty much directly link to or identify a person (e.g., social security number, email address, etc.).

(b) Quasi-Identifiers (QI) — these are columns which may not be useful by themselves but can be combined with other QIs, query results and some external information to identify an individual (e.g., ZIP code, age, gender, etc.).

(c) Sensitive Columns — these are attributes that are not PII or QI but constitute data about the person that needs to be protected for various reasons (e.g., salary, HIV diagnosis, live geo-location, etc.).

(d) Non-sensitive Columns — these are the remaining attributes that do not fit (a), (b) or (c) above (e.g., country).

Types of attributes from a privacy standpoint

Clearly, in the presence of QIs, just removing PII columns from a dataset is not enough for privacy protection. For instance, if basic demographic data (which qualifies as QI) is present in a dataset, then it can be combined with other public data sources — such as a voter registration list — to identify the individuals with great accuracy. This approach was used by researchers a few years ago when they found that the ‘anonymized’ dataset shared for the Netflix Prize competition could be compromised by associating with some data from another public data source — viz. movie ratings by users on IMDB. In that situation, the researchers leveraged just a few data points available publicly from IMDB to mine the Netflix dataset and revealed the entire movie watching history of individuals (something that is considered sensitive and protected under US privacy regulations).

So what might work?

Say hello to ‘k-anonymity’, ‘l-diversity’ and ‘t-closeness’.

K-Anonymity is used to provide a guarantee that any arbitrary (QI-based) query on a large dataset will not reveal information that can help narrow a group down below a threshold of ‘k’ individuals. That is, the technique provides an assurance that there will remain an ambiguity of ‘at-least-k’ records for anyone mining for privacy sensitive attributes from a dataset using their knowledge of QIs of specific individuals.

In this context, subsets of a dataset with the same value of one or more QIs are called ‘equivalence groups’ (or ‘equivalence classes’). For example, ‘all females under age 30 in a certain ZIP code’ could constitute an equivalence group. The goal is to ensure that an attacker cannot leverage queries such as this to narrow down and gain sensitive information about a specific person. Essentially, ‘k-anonymity’ ensures that all possible equivalence groups of a dataset have at least ‘k’ records.

This is accomplished using techniques such as (a) purging records or columns where some records may be purged altogether or certain QIs (e.g., gender) may be replaced with ‘*’ in others or (b) generalization where specific QI values are replaced by ranges (e.g., 63 years → 55–70 years) or numbers replaced by categorical values (e.g., 87% → ‘Passed’) — thereby diluting the precision of QI-based queries against the dataset.

A ‘k-anonymized’ result set (k=3)

Attacks on k-anonymity — The techniques used in ‘k-anonymity’ can be subject to attacks if the results from different subsets of the dataset (possibly released for different purposes) are unsorted. For instance, a subset released for a less sensitive scenario may have a helpful QI which an attacker happens to link to an individual using other information. The attacker can then look at a more sensitive subset of the same data and associate the individual with the sensitive information (e.g., a disease) if the rows happen to be in the same order. A mitigation for this is to randomize the order of each released subset.

Another class of attacks comes into play if there is not enough diversity in the records containing a sensitive attribute within each equivalence group. In that case, an attacker can use some background information on individuals to infer sensitive data about them. In the extreme case, if all records in an equivalence group have the same value of the sensitive attribute then the attacker can make an inference even without background information. E.g., if the anonymized dataset reveals that all records for individuals in a certain age range (e.g., 20–30 yrs) have a specific disease and if an attacker knows a particular person (e.g., a neighbor in that age group) is in the dataset then they can conclude that this person has the same disease.

Thus, while k-anonymity offers resistance against ‘membership inference’ attacks, it does not protect against ‘attribute inference’ attacks (which emerge from homogeneity of equivalence groups).

k-anonymized data lacking attribute diversity can leak sensitive info

L-Diversity tries to address this by ensuring that equivalence groups have ‘attribute diversity’. That is, it ensures that subsets of the dataset that have the same value of a QI (e.g., same age group) have ‘sufficient diversity’ of the sensitive attribute (at least ‘l’ different values). Furthermore, it tries to attain this across *all possible* equivalence groups. Note that ‘l-diversity’ works in hand in hand with ‘k-anonymity’ — it adds ‘attribute inference’ protection to a dataset that is protected for ‘membership inference’ by ‘k-anonymity’.

While ‘l-diversity’ covers for the weak points of ‘k-anonymity’, it is complex and difficult to accomplish across datasets with many QIs (as that leads to lots of equivalence groups to address). Furthermore, attackers can exploit (a) semantic relationships amongst attribute values or (b) situations where attribute values have very different levels of sensitivity to extract private information. It also makes an assumption that the attacker does not know the actual global distributions of a sensitive attribute within the complete dataset (which is generally not true).

T-Closeness mitigates these weaknesses by consciously keeping the distribution of each sensitive attribute in an equivalence group ‘close’ to its distribution in the complete dataset. In ‘t-closeness’, the distance between the distribution of a sensitive attribute in an equivalence group and the distribution of that attribute in the whole table (or population) is no more than a threshold ‘t’. Moreover, it uses the notion of Earth Mover’s Distance (EMD) to represent ‘distance’ between distributions for its advantages of annulling the effects of differing attribute sensitivities.

Enter AI/ML

When we introduce the algorithms and techniques of AI/ML into the big data picture, challenges shoot up to a whole different level. This is because — unlike traditional query/rules-based data extraction/triangulation approaches— ML & AI scenarios can combine hundreds of input dimensions/features in arbitrarily complex ways and, consequently, can compromise privacy in hitherto unknown and unfathomable manners.

The techniques from (just) big data days struggle in this ecosystem. As an example, ‘k-anonymity’ relies on the ability to find ‘k nearby records’ to return for a query — something that is a known challenge/nebulous concept in hyper-dimensional spaces. For example, an online shopping site stores hundreds of attributes about its customers. When each record has so many dimensions, the concept of ‘near neighbors’ becomes hard to pin down — forget being able to do it for all equivalence groups!

Let us now look at some underlying principles and current efforts that help.

Incorporating Privacy into Machine Learning

Differential Privacy

Much of the recent work analyzing privacy guarantees of AI/ML algorithms has leveraged a concept called Differential Privacy (DP).

Differential Privacy provides a mathematical framework that can be used to understand the extent to which a machine learning algorithm ‘remembers’ information about individuals that it shouldn’t — thus offering the ability to evaluate ML algorithms for privacy guarantees they can provide. This is invaluable because we want models to learn general concepts from a dataset (e.g., people with salary higher than X are 90% more likely to purchase drones than people with salary less than Y) but not specific attributes that can reveal the identity or sensitive data of individuals that made up the dataset (e.g., John’s salary is X). Moreover — in a departure from the attack model assumed by ‘k-anonymity’ and it’s peers — differential privacy makes no assumptions about the level or extent of background knowledge available to the attacker.

DP — the adversary cannot tell if the training set contained a specific record

Even though the term ‘differential privacy’ has become somewhat of a buzzword (especially after Apple announced that this approach underpins their privacy protection efforts on various iOS devices), it is foundational in the sense that significant effort is being invested by researchers all over various AI/ML domains to ensure that algorithms and building blocks of machine learning can be made ‘differentially private’.

Differential privacy achieves its objective by addition of a controlled amount of ‘noise’ during processing so as to generate enough ambiguity downstream that privacy-impacting inferences cannot be made based on predictions from the system. Moreover, this is done while still ensuring that the predictions are accurate enough to be practical. It works on the notion of a fixed ‘privacy budget’ and provides a basis for evaluating privacy loss from various operations on the data — ultimately specifying how that loss may be bounded while accomplishing acceptable tradeoffs with loss in utility of the data.

The mathematical gist of Differential Privacy (Image from this deck.)

In the iOS use cases (which employ differential privacy), Apple uses three key mechanisms to protect privacy while still learning general patterns of user behavior in various contexts. For instance, in the quest for ‘which emojis are trending?’, firstly, hashing is used to ensure that their backend service does not get any direct identifying data about the user. Secondly, sub-sampling is employed by reporting only a random subset of the each user’s activity (i.e., instead of sending every data point that might be of interest, only a subset is sent). Lastly, even for the data that is sent, noise is injected so that a specific user’s pattern of keystrokes is only approximately accurate when the backend sees it. The privacy aware backend algorithms are able to ‘filter out’ the noise while aggregating data across devices to learn the general patterns of user behavior and improve user experience on the basis of that. So, for instance, if it is determined that a particular emoji is gaining popularity, it is added in the quick access emojis set seen by all users. Likewise, once it is determined that a particular ‘local’ dictionary word has been added by many users, it is added to the dictionary of all users.

The PATE Framework

Differential privacy provides a general technique that can be applied in various ways in AI/ML contexts. The Private Aggregation of Teacher Ensembles (PATE) Framework applies differential privacy to provide an overall privacy guaranty on the model being trained from user data. The key intuition in the PATE framework is that “if two models trained on separate data agree on some outcome then it is less likely that sharing that outcome to the consumer will leak any sensitive data about a specific user”.

The framework divides the private data into subsets and independently trains different models (called ‘teachers’) on each of the subsets. The overall prediction is generated by combining the individual predictions of this ‘ensemble’ of teacher models. This in itself does not add any privacy ingredient into the mix. That is accomplished by two important steps. First, noise is added when combining the outcomes of individual teachers so that the combined result is a ‘noisy aggregation’ of individual teacher predictions. Second, these noisy predictions from the teacher ensemble are used as ‘labeled training data’ to train a downstream ‘student’ model. It is this student model that is exposed to end users for consumption. This article by the authors of the PATE framework goes into more detail.

Federated Learning

Federated Learning takes a somewhat different approach to preserve privacy in crowd-sourced learning scenarios. The key idea is that ‘why even bring all data together if, instead, we can devise ways in which we can learn from subsets (islands) of data and then effectively aggregate our learnings?’.

This makes learning possible in several interesting situations. For instance, a group of hospitals may be interested in applying ML techniques to improve healthcare of patients but (a) individual hospitals may not have sufficient data to do so by themselves and (b) they may not want to risk releasing their data for central aggregation and analysis. This is an ideal scenario for applying federated learning.

Another common scenario is the ability to use machine learning to improve user experience across a device platform without moving user activity data from individual devices to a central service (described here). In the latter case, each device downloads a (regularly improving) model from a central location and applies ML locally to make ‘micro-improvements’ to the model to be submitted back to the central server. This approach is gaining traction also because of the increasing richness of data available locally on each device (so learning locally is highly beneficial with fewer privacy constraints) and the much more powerful device-side processors compared to yesteryears (making compute intensive ML algorithms feasible to run).

Homomorphic Encryption

Lastly, efforts have also been under way for using cryptography to provide privacy guarantees in machine learning by leveraging an encryption technique called Homomorphic Encryption (HE).

When data is encrypted using traditional techniques, it becomes impossible to do any meaningful computation on it in the encrypted form. With the widespread adoption of cloud computing, one often encounters scenarios where a party possessing sensitive data wants to outsource some computation on that data to a third party which it does not trust with the plaintext data. Homomorphic encryption basically provides the ability to perform various meaningful operations on encrypted data without having direct access to the encryption keys or the plain text data itself. Using homomorphic encryption, the service can perform the requested computation on the encrypted data and return the (encrypted) result back to a client. The client can then use the encryption key (which was never shared with the service) to decrypt the returned data and get the actual result.

Homomorphic encryption is by itself an evolving field. Performance is a big concern and certain restrictions — such as the limitation to compute only polynomial functions (many activation functions in ML are non-polynomial) and only additions and multiplications of integers modulo-n (most learning algorithms require floating point computations) — mean that there are still many challenges to overcome. However, with the rising popularity of machine learning as a service (MLaaS), there is a lot of interest in improving techniques that leverage homomorphic encryption to perform ‘encrypted’ machine learning. Both variants are being explored — ability to perform learning/prediction over encrypted data and ability to use an encrypted model for learning with plaintext data.

Privacy regulations and machine learning

GDPR & Machine Learning

The General Data Protection Regulation (GDPR), which went into effect from May 2018, is the latest attempt from EU to get global organizations to treat personal data seriously. Very briefly, the GDPR pushes the bar for privacy protection much higher for any entity/organization processing (or outsourcing processing of) data about EU citizens. There are stringent requirements about consent handling, data protection, breach disclosure and erasure — with severe fines upon willful violations. Moreover, unlike its predecessor, the Data Protection Directive (DPD) which was a ‘directive’ (to be used as guidelines by member nations to formulate laws), the GDPR is a ‘regulation’ (a law in and by itself).

The GDPR went into effect from May 2018

Some clauses in the GDPR have stirred up a lot of discussion and debate regarding their potential implications for AI/ML. Let us look at a salient few briefly.

Interpretation of ‘potentially identifiable’ data

In the ‘scope of applicability’ section, GDPR states that the regulation applies to “all data about EU citizens that could potentially identify a data subject”. This in itself brings up an interesting conundrum. Given the immense power and perceptiveness of AI/ML algorithms, would it be possible to categorically exclude any system that processes data about people (in any form and for any purpose) from being under purview of GDPR? Can you be certain there isn’t the remotest possibility that the data you are processing about people cannot be used in any way to identify any of the subjects?

Moreover, one of the most valuable activities that helps a data science project progress leading to innovation and interesting insights is exploratory data analysis (EDA). The GDPR states that a data processor should not use the data for any purpose beyond the original intent without securing further consent from the data subject. This would at best significantly slow down exploratory efforts.

Explicit bar on subjecting people to ‘autonomous’ decisions

The GDPR explicitly debars data subjects being made subject to outcomes of autonomous decision making. But that’s the whole essence of so many mainstream use cases of AI/ML, right? Does this clause mean that one cannot apply ML to data about EU citizens? Does showing an ad when someone’s browsing a site constitute being subject to automated decision making? There are many such issues subject to interpretation. When it comes to the use of AI/ML, there seems to be an out though and what most organizations might bank on. Basically, this prohibition does not apply if the autonomous decision making is (a) necessary for meeting contractual obligations or needed for legal reasons or (b) when the data subject has explicitly consented to it.

Right for explanations

This is a tricky one. GDPR requires that sufficient explanations should be offered to data subjects who may wish to understand why a certain (automated) decision was made based on use of their data. The intent of this is to promote transparency and ensure that such systems are unbiased.

However, the complexities of machine learning models and the serious difficulty of being able to explain how they arrive at their predictions or outcomes makes this a daunting task. It is not fully clear how ‘fine-grained’ these explanations need to be to be acceptable under the regulation. For instance, if using a deep neural network (DNN), would a data processor need to explain the inner workings of stochastic gradient descent (SGD) and show the weights vector of various layers and explain the logic of convolutions and dropout, etc. Most experts of deep learning say that many times even they do not understand why it all works — but it does! There is also the aspect of IP protection that might kick in at some point in all this.

While everyone waits for ‘case laws’ to appear and clarify the exact position on this, there’s work under way to make machine learning explainable. One example is the approach outlined in Locally Interpretable Model-Agnostic Explanations (LIME). LIME basically treats the potentially very complex actual model as a black-box and attempts to recreate a simpler (easier to explain) representation purely by studying the inputs provided and the decisions made by the more complex model. Moreover, it attempts to do so ‘locally’ in the sense that it takes the specific prediction that an explanation is sought for and makes perturbations to the input vector to see which aspects of the input had significant effects or more weightage in the decision that was made. This acknowledges the fact that — while a thorough explanation of every aspect of a model may be daunting — we can still strive to understand specific decisions.

Right to be forgotten

According to GDPR, a data subject may request that a processor remove all data they may have stored about the subject (amounting to withdrawal of a past consent to use personal data). This provision is designed to empower a data subject about controlling long-term use and retention of their data by various data processors. (It was originally introduced by recognizing a fundamental need to protect rehabilitated individuals from being tainted permanently by the availability of information about their past crimes on the internet.)

When a data subject’s data is used towards machine learning, an interesting question arises when they withdraw consent or request data deletion. Should the data processor be required to retrain previously trained models without the specific subject’s data? Apparently not! The GDPR is clear that the revocation does not impact or apply to past processing that was done when the original consent was in effect. So that should be a relief. However, as we saw in the previous section, it is possible that models may remember things they are not supposed to — thus making recreation of potentially identifiable data starting with the trained model a possibility. This means that although you may think you have deleted a data subject’s data but it may still be recoverable! Currently, this risk appears to have been written off as one of academic interest.

What Next?

The preceding discussion about GDPR and the intent behind many of its stipulations bring up interesting issues regarding social implications of AI/ML. Considerations such as fairness and bias, ethics and morality of algorithmic decision making can stir our sense of security in society at the deepest levels. In the next article, we will explore those areas and perhaps look at “security” in the context of AI/ML in the broadest sense that we have done so far.