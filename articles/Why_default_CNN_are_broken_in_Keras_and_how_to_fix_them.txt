Initialization method

Initialization has always been a important field of research in deep learning, especially with architectures and non-linearities constantly evolving. A good initialization is actually the reason we can train deep neural networks.

Here are the main takeaways of the Kaiming paper, where they show the conditions that the initialization should have in order to have a properly initialized CNN with ReLU activation functions. A little bit of math is required but don’t worry, you should be able to grasp the outlines.

Let’s consider the output of a convolutional layer l being:

Then, if the biases are initialized to 0, and under the assumption that the weights w and the elements x both are mutually independent and

share the same distribution, we have:

With n, the number of weights in a layer (i.e. n=k²c). By the following variance of the product of independent property:

It becomes:

Then, if we let the weights w such that they have a mean of 0, it gives:

By the König-Huygens property:

It finally gives:

However, since we are using a ReLU activation function, we have:

Thus:

This was the variance of the output of a single convolutional layer but if we want to take all of the layers into account, we have to take the product of all of them, which gives:

As we have a product, it is now easy to see that if the variance of each layer is not close to 1, then the network can rapidly degenerate. Indeed, if it is smaller than 1, it will rapidly vanish towards 0 and if it is bigger than 1, then the value of the activations will grow indefinitely, and can even become a so high number that your computer cannot represent it (NaN). So, in order to have a well-behaved ReLU CNN, the following condition must be carefully respected:

Authors have compared what happens when you train a deep CNN initialized to what was the standard initialization at that time (Xavier/Glorot) [2] and when initialized with their solution.

Comparison of the training of a 22-layer ReLU CNN initialized with Glorot (blue) or Kaiming (red). The one initialized with Glorot doesn’t learn anything

Does this graph seems familiar ? Exactly what I witnessed and shown you at the beginning ! The network trained with Xavier/Glorot initialization doesn’t learn anything.

Now guess which one is the default initialization in Keras ?

That’s right ! By default in Keras, convolutional layers are initialized following a a Glorot Uniform distribution:

So what’s happening if now we change the initialization to the Kaiming Uniform one ?