Uncover Hidden Gems with Exploratory Data Analysis

Photo by Ilze Lucero on Unsplash

One of the key attributes of a data scientist is an inquisitive mind. In this article, we will explore an interesting data set using some popular Python tools, but will also try to think about the data in unconventional terms. For areas like fraud detection, connecting the data in unexpected ways can tease out patterns, even if the data may seem incomplete. Ideally, you will come up with insights that will open up new business opportunities.

While looking around Kaggle for supply chain data sets, I came upon one called Supply Chain Data, with a subtitle of Import and Shipment Data. Scanning down the page, I see the data source is called walmart-import-data.csv. Hmmm, could be interesting. Uh oh, “About this file” says "No description yet". This is where the sleuthing starts — we don't even have a data dictionary.



Usually when you are doing exploratory data analysis, it is because you have a specific question that you are trying to answer using the data, e.g. how many cases of pineapple were shipped from the Bahamas last month, but here we will let our imagination run wild and see what secrets the data might reveal.

To keep this article readable, only some key code fragments are shown below. The Python Jupyter notebook is on GitHub here, and the Kaggle kernel is here. For clarity, I will also use hindsight liberally to shortcut some things that I found by trial and error, e.g. how I know that about 9% of the rows in the original data file are all NA. I originally found that certain columns had a lot of NaN values, but on further inspection saw that entire rows were NaN.

Preliminaries

To be able to follow along with the Jupyter notebook, you will need to clone or download the repository from GitHub here. Alternatively, you can run the Kaggle kernel for this article.

If you go the first route, after you have cloned the repository, you will need to download the data file from Kaggle here and move the file walmart-import-data-full.csv to the data/raw subdirectory in the repository.

First Steps

It is not clear why this data set was posted, and it isn’t very recent (the latest entry is dated 2013–09–09). For our purposes it is fine, and the techniques will apply if a more recent dataset turns up. What kind of insights can we get? Let's get started by loading the data.

xdata = pd.read_csv(raw_data_path / 'walmart-import-data-full.csv', low_memory=False)

# About 9% of the rows are all NA; drop them

xdata.dropna(axis=0, how='all', inplace=True)

We specify low_memory=False to avoid the warning about certain columns containing data of mixed types. If we go beyond our Exploratory Data Analysis (EDA) phase, we could spend some time homogenizing those columns and create a custom dtypes parameter for read_csv. As usual, we take a peek at the first few rows to get a general sense of the data set:

xdata.head(3)

The first few rows and 13/34 columns of the data

Looking at xdata.shape shows that we have 175713 rows and 34 columns in the dataset with NA rows removed. From xdata.columns we can get a rough sense of what’s in the columns, which have reasonably meaningful names. How about looking at ‘COUNTRY OF ORIGIN’? It might be interesting to see where Walmart’s goods were coming from in 2013. The describe function is very useful for EDA:

xdata['COUNTRY OF ORIGIN'].describe().to_dict() {'count': 174093, 'unique': 71, 'top': 'China', 'freq': 125366}

We see that the top country of origin is China, accounting for 72% of the shipments (by count). However, there are 71 unique countries, so who else is selling to Walmart?

Visualization of Walmart’s Global Suppliers

We could look at a histogram, but it would have more visual impact to see it on a map.

To do this, we can make a choropleth map, where different shades of colors for each country will show the percentage of shipments. We will use Folium which uses Python and the Leaflet.js library to create interactive maps for data visualization.

A Folium Choropleth map needs a data frame and two columns: one that is used to index into the JSON mapping data and the other that contains a value that is used to pick a shading color on the map.

Although I’m sure there are many more efficient ways of doing this, since this is exploratory data analysis, and the data set is not that large, we can use groupby, describe and xs to quickly make a dataframe with the columns needed to make a Folium map. If you are having difficulty seeing how I ended up with the expression below, break it into parts and display each part. I don’t really care about the ‘WEIGHT (KG)’ column right now, just that we can use its ‘count’ field.

countries_of_origin = xdata.groupby('COUNTRY OF ORIGIN').describe().xs('WEIGHT (KG)', axis=1).copy()

countries_of_origin.reset_index(level=0, inplace=True) coo_record_count = sum(countries_of_origin['count'])

countries_of_origin['pct'] = countries_of_origin['count'].apply(lambda cnt: 100*cnt/coo_record_count)

countries_of_origin['logpct'] = countries_of_origin['count'].apply(lambda cnt: np.log(100*cnt/coo_record_count))

# We will use the sorting later

countries_of_origin.sort_values(by==['count'])

The first three entries in countries_of_origin, by count

Now that we have the data in a useful format, let’s make a map with Folium:

# Initialize the map:

m = folium.Map(location=[30, 0], zoom_start=2, no_wrap=True) # geo_data is the map data # Add the color for the chloropleth:

_ = folium.Choropleth(

geo_data=folium_world_data.as_posix(),

name='Choropleth',

data=countries_of_origin,

columns=['COUNTRY OF ORIGIN', 'pct'],

key_on='feature.properties.name',

fill_color='YlGn',

fill_opacity=0.7,

line_opacity=0.2,

nan_fill_color='white',

legend_name='Country of Origin (%)'

).add_to(m)



# Save to html

map_path = reports_path / 'walmart_folium_chloropleth_pct.html'

m.save(map_path.as_posix())

m

Countries of origin for Walmart shipments, by count

This isn’t very useful, because China dominates so thoroughly that everything else is close to zero. The white areas are countries that are not shipping to Walmart at all (at least in this data). The yellow areas ship to Walmart, but in single digit percentages.

There was a reason that I added two columns to countries_of_origin above. Let’s plot the map again, but this time use the log of shipment percentage. I will omit the code for brevity, but I just replaced the second element in the columns parameter with ‘logpct’.

Countries of origin for Walmart shipments, by log(count)

This is more instructive, since it shows relative positioning amongst the runners-up. Looking at this map made me curious about the countries that were almost zero but not quite. Visually, Saudia Arabia stands out as a curiosity to me.

xdata[xdata['COUNTRY OF ORIGIN']=='Saudi Arabia']['PRODUCT DETAILS'].describe().to_dict() {'count': 3,

'unique': 3,

'top': '320 BAGS, 19,200 KG OF ETHIOPI A ARABICA COFFEE SIDAMO GRAD- 2 TOP QUALITY REF P2006A, PA CKAGE: JUTE BAGS OF 60 KGS EA CH DELIVERY TERMS: FOB DJIBOU TI NET SHIPPED WEIGHT FDA R EGISTRATION NO. 11826035706 S',

'freq': 1}

Aha, it seems to be coffee transshipped from Ethiopia. A glance at the other two items shows granite slabs and personal effects, not the expected pertroleum products. One other thing of note is that the ‘PRODUCT DETAILS’ entry has some weird word break issues (which is not limited to this entry).

Looking at the tail end of the sorted dataframe, I see Korea, which I would imagine should have more than one shipment given how big they are in the electronics space.

countries_of_origin.sort_values(by=['count'], axis=0, ascending=False).tail(4)

Sure enough, South Korea is number three by count, which we see if we replace tail(4) with head(3) above, which means that the lone shipment for Korea is a data entry error. See also China Taiwan and Taiwan. Some feedback to the owners of this data would be to require standard country names when inputting data.

Before we move away from the mapping section, I wanted to point out one oddity of Folium that took me a bit of time to figure out. To set the key_on parameter of the choropleth function correctly, I looked at the Folium data file folium/examples/data/world-countries.json to find the names in the data structure, giving me key_on=’feature.properties.name’.

Weighty Matters

Now that we have identified which countries ship the largest number of items, let’s look at the countries that ship the heaviest items. The Pandas Grouping Cookbook was very helpful in figuring out the expressions below.

top_by_weight = countries_of_origin.sort_values(by=['max'], axis=0,

ascending=False)[0:10]['COUNTRY OF ORIGIN'].values tw3 = xdata.groupby(['COUNTRY OF ORIGIN']).apply(

lambda subf:

subf.sort_values('WEIGHT (KG)',

ascending=False).head(3)) cols_of_interest = ['COUNTRY OF ORIGIN', 'WEIGHT (KG)',

'PRODUCT DETAILS', 'ARRIVAL DATE'] https://www.wolframalpha.com/input/?i=3e%2B07+kg

# ≈ (0.7 to 1) × mass of a Handy size cargo ship

mass_handy_size = 3.0e+07 #kg # ≈ (0.7 to 1) × mass of a Handy size cargo shipmass_handy_size = 3.0e+07 #kg tw3.loc[(tw3['COUNTRY OF ORIGIN'].isin(top_by_weight)) & (tw3['WEIGHT (KG)']>mass_handy_size)][cols_of_interest]

Recall that countries_of_origin was constructed using groupby and ‘WEIGHT (KG)’, so max here refers to the weight. I have to admit that I was quite surprised at the countries that showed up at the top, but even more surprised when I saw what those products were.

The shipment from Venezuela was the heaviest, by two orders of magnitude. Because the dataframe display truncates the ‘PRODUCT DETAILS’ column, let’s make sure we are not talking about olive oil here…

list(xdata[xdata['WEIGHT (KG)']>1e+09]['PRODUCT DETAILS'])[0] '5,550.023 METRIC TONS (49,849 BBLS) LIGHT VIRGIN NAPHTHA'

Petroleum naphtha is an intermediate hydrocarbon liquid stream derived from the refining of crude oil. Looking through the product details, we see that the heaviest shipments are petroleum products (ALKYLATE, GASOLINE, NAPTHA), building materials (ROCK and CEMENT), and GRANULAR UREA (!?). Beats the piss out of me what they do with that…

The hidden gems here for me were the various unexpected countries that supplied petroleum products, and that one of the expected oil exporters (Kuwait) shipped UREA instead. One other interesting factoid, courtesy of Wolfram Alpha, is that the NAPHTHA shipment from Venezuela weighs almost as much as the Great Pyramid of Giza.

Pack It In

Since we have the fields ‘WEIGHT (KG)’ and ‘CONTAINER COUNT’ we can figure out what percentage of shipments would fit in standard shipping containers.

Given that the ‘WEIGHT (KG)’ covers almost 10 orders of magnitude, and also includes some zero values, we will find it useful to add a column, ‘log_weight_kg’.

xdata['log_weight_kg'] = xdata['WEIGHT (KG)'].apply(lambda x: np.log(max(1, x)))

With xdata[‘WEIGHT (KG)’].describe(), we see that the mean weight of a shipment is 48815.86 Kg, which Wolfram Alpha helpfully says is about half of the cargo mass capacity of a Boeing 747–200F aircraft. Plotting a histogram of the ‘log_weight_kg’ column gives us:

where the red dashed line is the median.

Next, let us look at shipments that could potentially be shipped in a standard 20 or 40 foot shipping container, by weight. Here I am going to make the assumption that ‘WEIGHT (KG)’ is the net weight of the shipment.

Add a ‘kg_per_container’ to the dataset, where we normalize missing values so that we have at least one container for each shipment.

xdata['kg_per_container'] = xdata.apply(calc_kg_per_container, axis = 1)

containerizable = xdata[xdata['kg_per_container'] <= NW_KG_DRY_CONTAINER_40FT]

We consider it ‘containerizable’ if it meets the weight requirement for a 40 ft container, and a simple calculation shows that this is true for 99.6% of the shipments. By narrowing our attention to this vast majority of shipments, setting aside the bulky outliers, we can get a more fine-grained picture of these smaller shipments.

The weight per container basically steadily declines, apart from a hill at around 19000 Kg/container. This might be an interesting area for further research.

Another Dimension

To highlight the importance of creating a data dictionary for each dataset, we now consider the ‘M.UNIT’ column. I am assuming that ‘M.UNIT’ is the units for the ‘MEASUREMENT’ column (if so, this relationship should be recorded in the data dictionary).

xdata['M.UNIT'].value_counts().to_dict() {'CM': 93812, 'X': 47585, 'CF': 321, 'M': 128, 'F': 24, 'CC': 11, 'MM': 4, 'SS': 3, 'XX': 3, 'FF': 1}

We can take a guess that ‘CM’ means cubic meters, ‘CF’ is cubic feet, but what about the rest? To harp on my original point, we should not have to guess; these should be given in the data dictionary. Since it appears we must guess, let us start out with the less common measurements.

low_units = ['F', 'CC', 'MM', 'SS', 'XX', 'FF']

cols_of_interest = ['COUNTRY OF ORIGIN', 'WEIGHT (KG)', 'QUANTITY', 'CONTAINER COUNT',

'MEASUREMENT', 'M.UNIT', 'PRODUCT DETAILS'] # , 'ARRIVAL DATE'

pd.set_option('display.max_colwidth', 125)

pd.set_option('display.max_rows', 50)

xdata[xdata['M.UNIT'].isin(low_units)][cols_of_interest]

The first few entries of the less common M.UNIT collection

For the six less common ‘M.UNIT’ values in low_units, the only pattern I could readily discern is that if ‘M.UNIT’ is either ‘F’, ‘FF or ‘MM’’, then ‘MEASUREMENT’ is zero, so perhaps these are ignorable. The others will have to remain a mystery until we can talk with the data owners.

We can get a quick overview of the distribution over all the units using seaborn to make a stripplot. We limit ourselves to a subset of the data that might fit in a 40 ft container.

xic = xdata[(xdata['CONTAINER COUNT']>0) & (xdata['WEIGHT (KG)']>0) & (xdata['kg_per_container']<=NW_KG_DRY_CONTAINER_40FT)] g = sns.stripplot(x='MEASUREMENT', y='M.UNIT', data=xic)

sns.despine() # remove the top and right line in graph

g.figure.set_size_inches(6,4)

plt_path = figures_path / 'measure_stripplot.png'

plt.savefig(plt_path.as_posix(), format='png'

Distribution of MEASUREMENT values by M.UNIT

We will look at the distribution of ‘CM’ in more detail below.