It often pops up on lists of common interview questions for data science positions. Explain the difference between precision and recall, explain what an F1 Score is, how important is accuracy to a classification model? It’s easy to get confused and mix these terms up with one another so I thought it’d be a good idea to break each one down and examine why they’re important.

Accuracy

The formula for accuracy is pretty straight forward.

But when dealing with classification problems we are attempting to predict a binary outcome. Is it fraud or not? Will this person default on their loan or not? Etc. So what we care about in addition to this overall ratio is number predictions that were falsely classified positive and falsely classified negative, especially given the context of what we are trying to predict. A 99% accuracy rate might be pretty good if we are trying to predict something like credit card fraud, but what if a false negative represents someone who has a serious virus that is apt to spreading quickly? Or a person who has cancer? That’s why we have to breakdown the accuracy formula even further.

Where TP = True Positive, TN = True Negatives, FP = False Positives and FN = False Negatives.

Precision & Recall

Before getting into precision and recall, a quick note on Type I and Type II errors. These terms that are not unique to classification problems in machine learning, they’re also extremely important when it comes to statistical hypothesis testing.

Type I Error: False positive (rejection of a true null hypothesis)

Type II Error: False negative (non-rejection of a false null hypothesis)

So with that in mind we can define precision as the percentage of relevant results, while recall is characterized as the percentage relevant results that are correctly classified by the model you’re running. Now obviously these definitions aren’t all that intuitive, so let’s take a look at a few visualizations and see if we can wrap our heads around it.

Ok, so this is starting to make a little more sense I think. When it comes to precision we’re talking about the true positives over the true positives plus the false positives. As opposed to recall which is the number of true positives over the true positives and the false negatives. Below are the formulas and as you can see they are not too complicated.

I think the most intuitive visualization for interpreting the performance of a statistical classification model is a confusion matrix. It’s a two by two table in which each row represents an instance in the predicted class while the columns represent the instances of the actual class.

Below is an actual confusion matrix from a project I did on prediction of hard drug use based on surveyed data using several classification models including Logistic Regression, XGBoost and Random Forest. Here is a link to that project on my GitHub.

F1-Score

Finally we have the F1-score, which takes both precision and recall into account to ultimately measure the accuracy of the model. But what’s the difference between this metric and accuracy? Well as we talked about in the beginning, false positives and false negatives can be absolutely crucial to the study, while true negatives are often less import to what ever problem you’re trying to solve especially in a business setting. The F1 score tries to take this into account, giving more weight to false negatives and false positives while not letting large numbers of true negatives influence your score.

Hopefully this blog clears up any confusion you might have had when it comes to these four metrics and you realize that accuracy is not necessarily the end-all be-all of measurement for machine learning classification models. It’s really going to depend on what kind of problems you are trying to solve.