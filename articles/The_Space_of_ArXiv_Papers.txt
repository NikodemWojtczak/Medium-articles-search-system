This summer, my team and I joined forces with researchers from the NVIDIA AI Technology Center to design and develop an Artificial Intelligence (AI) Research Assistant using machine learning techniques, at the Singapore University of Technology and Design.

The motivation behind this project? Having the AI Assistant handle portions of the academic research process, reducing the cognitive load of human researchers and freeing up valuable time and resources for them to perform higher-order tasks such as ideation, experimental design, and writing.

In this article, we’ll talk about some of the big ideas we gleaned and how we might possibly think about designing AI systems to construct spaces of knowledge, navigate and manipulate data, and deliver insights rather than an information overload.

Designing the space of knowledge

It turns out that a substantial portion of the research process, particularly the early stages, have a high potential for automation. A study we conducted involving 64 researchers revealed two key insights into the pain points of early-stage research: (1), The space of information was simply too difficult to navigate, and (2), the structure of this space was highly complex, forcing navigation via recursive citation paths between research papers - without the guarantee of ever finding something worthwhile.

But, what if we could take this seemingly inevitable, intractable problem for humans, and translate it into an automatable task? Could we easily localize ourselves in that space of information, and make it easier to explore areas near us? That was our aim.

We need to translate the problem into an engineering one — how can we construct and analyze the space of research knowledge, and ensure that it’s easily traversable, unbiased, and ultimately beneficial to the human researcher? We first need a huge, structured source of research knowledge, and for a really good exemplar we look to ArXiv, Cornell’s e-print archive, to form our knowledge base.

Papers submitted to ArXiv are often destined for publication at conferences or in journals, and potentially undergo a painfully slow peer review process. It’s no wonder that for ArXiv is quickly becoming the de-facto standard for quickly staking claim on research ideas, informal reviewing, and to democratize access to scientific work for some fields of science. Verified researchers self-submit pre-prints to ArXiv, and are encouraged to submit them in the (La)TeX format. If we examine the ArXiv bulk source file access page, we’ll see that large collections of these articles in .tex format are available for download.

A really short brief on LaTeX

For starters, a simple rendered document and its LaTeX source is shown below.

A PDF document compiled from its LaTeX source (shown below).

\documentclass{article}

\begin{document}



\title{Introduction to \LaTeX{}}

\author{Author's Name}



\maketitle



\begin{abstract}

The abstract text goes here.

\end{abstract}



\section{Introduction}

Here is the text of your introduction.



\begin{equation}

\label{simple_equation}

\alpha = \sqrt{ \beta }

\end{equation}



\subsection{Subsection Heading Here}

Write your subsection text here.



\section{Conclusion}

Write your conclusion here.



\end{document}

A TeX file contains basic instructions for typesetting, and can be rendered to various formats, most frequently PDFs. LaTeX contains a set of convenience macros that make it easy to define and re-use writing essentials, such as titles, headers, equations, sections, and footers.

The structure of research papers

Having documents stored in TeX means that all you need to recreate these pretty, structured, and formatted research papers in their full glory are their source .tex files, along with any graphics or media files that are referenced by the source.

If you’re a machine learning practitioner, you’ll likely have already recognized that these abstracts, titles, sections, subsections, etc., and their text and graphical content, form label-data pairs, all of which have already been assigned for us! We can now look at all of ArXiv as a structured dataset of labels and text (or images and other media). For example, if we’re interested in classifying strings of text in the paper, we can train our classification models on samples that look like this:

Label: \section{Results} -> 'results'

Data: "The results of our method...

Having research papers that are properly structured also enables us to try and guess what parts of research papers are important — if we know that the abstract of a paper is a good summary of the full text, then we can train models to attempt to recreate the summary given the full text of the research paper.

To take this idea further, some ArXiv papers contain structured abstracts, which are little labelled summaries of each section (e.g. Data, Method, Results, Conclusion) of the paper within the abstract section, essentially providing pairs of longer text, in their normal sections, with their summaries, found in the abstract. Not surprisingly, they have been used recently to train abstractive summarization models!

Contrast this with the deluge of design problems that emerge if we try to analyze PDF, or scanned, documents: Should we use OCR, or should we try and reconstruct labels from images? Or should we try and transform the PDF back into LaTeX? Which conversion is potentially lossier? It becomes clear that sticking with LaTeX, where possible, gives us the cleanest data from the source distributions (authors!) that generated it, with absolutely no noise.

The space of research papers

Okay, so we now know what a single research paper looks like, but they look disparate: there doesn’t seem to be a way to properly compare these papers! How would the AI Assistant construct a representation of knowledge from the research papers that it has, and simplify it so that topical queries, like “Boiling eggs”, or structured questions, like “What are the key differences between Convolution and Correlation?”, can start to make sense? What does it mean to ask the Assistant a question, and how will it navigate this space to try and return an answer?

Let’s assume the year is 2028 and you’re a functioning human-level AI Assistant. You’re trying to answer the convolution versus correlation question to help a fellow researcher out. Let’s also assume you already understand that you’re tasked to compare two things, and with the sense of urgency implied by “key differences”, you know you must only return the top-k largest differences. Now you have two keywords, “convolution”, and “correlation”. You know that you must look at documents which mention either or both of the words, and process the similarities between the usage of the two, and by doing so you’ll be able to discover the differences via a subtractive, logical approach. Easy, isn’t it!

Now let’s phase back into present-day 2019, and imagine you’re the one building the Assistant. You’re stuck — you have this massive collection of documents, but you don’t have a way of processing similarities, or the relevance, between documents, the first step toward making a comparison between “convolution” and “correlation”. Or do you?

There are some possibilities. If we look at our entire collection of ArXiv papers, we might realize that they’re just really made up of a lot of words. We might guess that having similar words, styles, or content across two text documents means that it is highly likely that the two are relevant to each other. This brings us to two popular methods of assessing relationships between words in documents: (1) ranking methods, and (2) vector embeddings.

A ranking method like TF-IDF or BM25 uses the occurrence of words across documents as markers of similarity. TF-IDF considers documents similar only if they share distinctive words frequently. Vector embeddings are typically trained on a large corpus (like our collection of articles), and during inference, generate vectors for a given word, which can then be manipulated numerically — using cosine distance, for instance. Additionally, context-sensitive embeddings like ELMo and BERT let us distinguish between the exact same words used differently.

What both of these methods are letting us do is to traverse the space between articles by specifying some form of similarity between them. And by extension, we gain the ability to compare documents against one another, to say that this is more relevant than that. And that’s arguably the first step to learning the ability to prioritize information, to rank knowledge according to criteria, to compare research papers, topics, and queries across a wide range of metrics.

If the text content of our research collection wasn’t already rich enough, ArXiv also stores metadata about each paper. All articles come with metadata files that minimally contain author names, article titles, topic categories, plaintext abstracts, and dates, providing additional dimensions, or metrics, to evaluate the relevance and similarity of sets of research documents. And who says that research ideas, or knowledge, are insular within documents, or that word similarity needs to be the basis of the space? What other kinds of spaces can we construct with the features we’re able to extract? What about…

Citations

Bibliographies are huge clues as to whether a research paper is similar to another. Imagine building a huge graph of knowledge, where the nodes are the individual research papers, and directed edges carry information about which paper cited which. Might we be able to learn weights for these edges based on the content in the papers? This is a whole different — and an arguably more structured — representation of the space of research! What’s important is that we’re now able to visualize real connections between papers if we imagine this huge network of citations, making it easier to construct search algorithms to traverse them like the graph problems we’re so used to. Along with a text-feature representation of similarity, by adding the idea of citations into our space of ArXiv we now have dimensions encoding the external relationship between papers, ones in the domain of authorship and the transmission of ideas.

It’s not easy recovering the full network of references, though. Papers that are themselves not on ArXiv often break citation paths as we try to recursively generate this network. What if you needed to cite a very, very old paper? Or, what if that particular paper was on ViXra.org instead?

Issues with fair knowledge

Exciting as trying to engineer knowledge might be, there’s a glaring issue when we attempt to equate our painstakingly-collected dataset with knowledge, and when we fail to consider how ArXiv is merely a sample of all available research information. It’s important to note that by just using TeX articles from ArXiv, we’re categorically biasing AI Assistants towards quantitative research from specific, verified researchers across 8 fields of study, submitted from 1 Jan 2000 to present.

Learning procedures also need to account for statistical imbalances, like the number of submissions between fields or the seasonality of submissions. We’re also presented with biases over larger timescales: the flux of language in documents over long periods of time is being actively researched in the Digital Humanities — how might changes in sentence structure and word usage over time factor into our similarity and summarization algorithms, and how might we adapt them to account for these concerns?