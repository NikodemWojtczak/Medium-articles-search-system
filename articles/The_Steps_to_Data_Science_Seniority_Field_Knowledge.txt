Internal Knowledge

Your ability to get things done is correlated with your familiarity with the tools that ease the burden of programming AI models and the knowledge about when to use them. You can say that a good Data Scientist is an expert applier.

There are three steps in the Data Scientist modeling development process that are directly affected by your field knowledge: data representation, model building, and impact measurement.

Data Representation

The first thing you need to do when creating your models is to define how your raw information will become pretty, semantically-charged numbers. Temporal Series of KPIs? Live feeds from surveillance cameras? Big chunks of texts from social media? You should be able to deal with those challenges even if they are out of your area of expertise.

You need to know exactly how to deal with each type of data and what models are adequate for your chosen data representation.

According to this Medium article from Sequoia, one of the main factors that transform a Junior into a Senior is their ability to be a multi-domain expert and functional leader. For that to happen, you need to be free from modal restraints: image, text, frequencies… It’s all data.

Get to know the basics of every possible area. Bag-of-Words and Word2Vec for text, SIFT/SURF and ImageNet-trained convolutional networks for images, spectrograms and MFCCS for audio. Your feature extraction toolbox should grow constantly.

Model Building

While modeling, training, and testing complex algorithms are part of the Data Scientist toolkit, they are not the main dish of the Data Scientist full course.

You may have a preferred area, such as Natural Language Processing, Neural Machine Translation, Object Detection or Facial Recognition, but you don’t need to be well-versed in all manner of neural network architecture known to man.

You need to know the basics to model for every task. How to design your inputs and outputs, what loss function is best for your training, how to measure class unbalance and, most importantly, how to identify the core of the problem.

Most AI challenges can be neatly segregated in distinct areas. Knowing general solutions for problems in each area is the first step to seniority.

Offer category classification? Can be simple ( multi-class classification ) or complex ( extreme classification ). Start with hierarchies of small classifiers to simplify the problem.

) or complex ( ). Start with hierarchies of small classifiers to simplify the problem. Market share prediction? Time series forecasting . Arrange your data in a sequence, try to predict n+1 . Check LSTM for a simple deep learning deploy.

. Arrange your data in a sequence, try to predict . Check for a simple deep learning deploy. User profiling? Classification if you have a good amount of labeled data, Clustering if you don't. Use manual labels as roots for your clusters. Check cluster metrics. See if you can't use graph-related algorithms.

if you have a good amount of labeled data, if you don't. Use manual labels as roots for your clusters. Check cluster metrics. See if you can't use graph-related algorithms. Track players in a soccer game? Object detection and object tracking. Labeled data is essential for good detection. A pre-trained YOLO network should be your first test, Mobilenet if speed and memory are a problem. Tracking depends on how many frames you can classify per second.

You don’t need the implementation details of everything in your head, that’s a task for Google and Mendeley. You just need to know how to efficiently research how to develop them and use that to implement these algorithms.

Impact Measurement

Now, for all models you create, you will feel a tingling sensation when looking at their metrics. If you leave it alone, it'll eat your sleep hours and leave you reeling, gasping, taken over the edge into madness.

Is this model good enough for this task? Is it even useful for my company?

Chances are that your whole model will be scrapped. That's just the reality of data. Things move so fast you don't even remember the last time an ML project was rolled into production.

But the model itself was never your biggest asset. It's the insights you gained with it that are valuable.

Let's go back a few paragraphs and reuse that user profiling problem. Your project was to segment your userbase in order to create targeted ads. Marketing was expecting big, abstract groupings based on previous population studies and your model created small, dense clusters. They are not happy about it, and you are called to explain these results.

Now, your model was objectively bad in the eyes of everyone, but you know better.

Data was insufficient. You propose a new framework of user information collection to address the lack of specific attributes that should give Marketing the exact insight it desires.

The objective is too abstract/fundamentally wrong. You share your view that users are better modeled in smaller communities and pinpoint groups with a high amount of successful and reoccurring transactions.

The model is not completely unusable. There are certain user groups in your findings that weren't noticed until just now. You posit that your model can be used specifically to find those users and target them accordingly.

Specific parts of the model can be used for other purposes. The user vector representation you created can be added to product recommendation tasks to increase carousel relevance.

All that from a failed project.

The only whetstone that will adequately sharpen this specific skill of value prospection is experience. A systemic view of the company will allow you to see possible connections; knowledge of what can be reused will allow you to create these links.

Improve your critical thinking. Make it a daily practice. Quarter problems and solutions in atomic pieces. Be watchful of issues in other areas that may be enhanced with data.