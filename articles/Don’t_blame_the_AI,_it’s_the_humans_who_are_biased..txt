These datasets include things like your name or personal information, a photograph, documents, or other historical artifact. Because these data have been collected by humans they are subjective and often unrepresentative[3]. As outlined in my earlier post, there are problems with how women (and minorities) have been represented (or suppressed) in historical records. Women and people of color may be disproportionately affected by AI bias, due to intersections of economics and gender, race, and ethnicity within the datasets[1,4]. Buolamwini and Gebru[5], in their analysis of commercially available facial recognition AI, find that gender of white men was properly identified by the computer more than 99% of the time, but accuracy is barely 65% for black women.

How societal bias makes its way into AI is complex, but after researching this topic at length, I find they can roughly group into 3 main issues. Bear with me as a non-engineer in my attempt to articulate them.

First, the data used to train the AI system may itself carry systemic social biases. Some even have outlined how our language itself carries biases, impacting many data points at the root. Imagine a hypothetical scenario of a university using AI to help rank freshman applicants, using previous student records as its training data (e.g., high school GPA, SAT scores, or other state testing results correlated with college success). There are widely discussed inequities in the U.S. school system and standardized testing thatâ€¦