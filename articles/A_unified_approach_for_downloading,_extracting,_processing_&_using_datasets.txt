Salient Features

200 image classes

Training dataset of 100,000 images

Validation dataset of 10,000 images

Test dataset of 10,000 images.

All images are of size 64×64.

wnids.txt contains the list of class names (nXXXXXX)

words.txt contains friendly names associated with class names [we are not going to use for this exercise]

val_annotations.txt contains the list of images. For each image there is a corresponding column that provides the associated lable (i.e. one from wnids.txt)

train folder contains a folder per class and in that folder is a directory called images that contains the 500 jpeg files for that class. Every class inside train has nXX_boxes.txt that contains the bounding box information about the images. We are not going to process the bounding box information for this exercise.

tfds.core.GeneratorBasedBuilder

The main workhorse behind the implementation of various datasets in tensorflow_datasets is this class. As the name implies it expects you to generate the examples of your datasets and it would write it in the tfrecords for you.

tensorflow_datasets expect you implement a class that inherits from tfds.core.GeneratorBasedBuilder and implement three methods — _info , _split_generators and _generate_examples .

The _info method is where you are going to return the metadata about your dataset. Here is a snippet that shows it for tiny-imagenet.

The most important element of tfds.core.DatasetInfo returned from _info is the specification of features. If you are already familiar with tfrecords you will notice that it is different from the one that you are used to see. As a matter of fact tensorflow_datasets provides some efficient wrappers for building the features that greatly simplify lot of things. Here in this example, I am using tfds.features.Image whereas if I was manually building the tfrecords I would have to use multiple fields for e.g. one for storing raw data, one for shape, another one image format etc. On top of that I would have to read the image from my script. As you will see later in the implementation of the _generate_examples function, we do not need to do any of that.

I also quite appreciate the fact that the DatasetInfo has fields for description as well as citations. These metadata fields come in handy for generating the documentation as well & more importantly acknowledgement of people’s hard work in creating the dataset in the first place.

The next method to implement is _split_generators which in many ways is the workhorse for this implementation.

The first line in this method i.e. extracted_path = dl_manager.extract(dl_manager.download(_URL)) is the one that is responsible for downloading and extracting it. The _URL in this case points to http://cs231n.stanford.edu/tiny-imagenet-200.zip. The return value i.e extracted_path contains the path to the place where the zip has been extracted. Now this was the first source of confusion and problem for me. When I called download_and_prepare it would show me the progress bar for downloading but when I would look at the location it would look like

As shown in the image above default path where the zip is downloaded and extracted is ~/tensorflow_datasets/downloads/extracted . Now since the directory is ending with .zip extension I thought that it is a file and extraction is failing. Ended up spending 15 minutes trying to figure out what is wrong and finally paid attention to the fact that it is a directory.

But what if the dataset was divided amongst many zip/tar/rar/gz files ?… Worry not !, you are covered. dl_manager.download() can take a dictionary as an argument and return value in that case would be a dictionary that would contain the corresponding values. Below listing shows an example for that use case -

def _split_generators(self, dl_manager):

# Equivalent to dl_manager.extract(dl_manager.download(urls))

dl_paths = dl_manager.download_and_extract({

'foo': 'https://example.com/foo.zip',

'bar': 'https://example.com/bar.zip',

})

dl_paths['foo'], dl_paths['bar']

I called this method the workhorse in my implementation because this is where I am reading the metadata files (wnidx.txt, val_annotations.txt etc) and building dictionaries that contains the list of images for the various classes and assigning a numerical label for the classes.

Please note that the organization of train and validation data in tiny-imagenet is different and hence I have two different methods to process the information however I am consolidating them in the same dictionary format. You can see the implementation details at — https://github.com/ksachdeva/tiny-imagenet-tfds/blob/master/tiny_imagenet/_imagenet.py

The interesting code statement for you in _split_generators is the return value where I am returning the various split generators. The code should be clear enough to speak for itself. The only part to pay attention to is the gen_kwargs key of tfds.core.SplitGenerator . Essentially whatever you would pass as the value of this field would be passed as the argument for _generate_examples .

Now, you would implement this much of the dataset class and when would issue download_and_prepare you would get an error notification saying that it could not find the checksum file. This was the second step that took me for a little spin and mostly I spent more time chasing this issue than implementing the entire package.

Here is what is happening. For every remote archive file that you download (in this case http://cs231n.stanford.edu/tiny-imagenet-200.zip) you are required to provide the checksum information. The checksum information is to be stored in a file with .txt extension. Since we are developing our own python package (i.e. not inside the tensorflow_datasets ) you need to also tell the tensorflow_datasets where is the directory where you would have the file that contains the checksum. Here is what you need to add to your module in order to accomplish this -

checksum_dir = os.path.join(os.path.dirname(__file__),'url_checksums/') checksum_dir = os.path.normpath(checksum_dir) tfds.download.add_checksums_dir(checksum_dir)

The other aspect is what should be the name of the checksum file ? After bit of trial & error, I figured that it should be the same name as your class name but with snake casing i.e. for TinyImagenetDataset you would name your file as tiny_imagenet_dataset.txt This part is not very clear from their guide, at least I could not find it specified anywhere and had to figure it out myself.

Still the work is not done yet; from where would you get the checksum itself ? Compute it manually ? Fortunately here the framework will help. All you need to do is that the first time you would invoke download_and_prepare from a test/example that is using your dataset class you should pass following argument -

dl_config = tfds.download.DownloadConfig(register_checksums=True)

download_and_prepare(download_config=dl_config))

You will now see that the download would succeed with out any checksum error and your checksum file (tiny_imagenet_dataset.txt in this case) would have an entry. From this point onwards you can remove the download_config parameter.

Time to look at _generate_examples now.

As evident from the code, this is a generator function yielding two return values. First one is the key that is a unique identifier for the sample that would be written in the tfrecords and the second is the dictionary that corresponds to the schema you specified in the _info method. As I mentioned earlier you do not need to read the image files anymore rather you simply supply the path to your image file and the tensorflow_datasets would make sure to read it and embed it in the tfrecords

Packaging your implementation (Optional)

If you are building a private dataset most likely it would be just a python package in your repository. However if you are sharing it with in your organization and has some sort of python package repository you may want to build a wheel file.

There is nothing specific about building a python pip package here except that you would want to make sure that you do include the checksum file in your final build.

You can look at https://github.com/ksachdeva/tiny-imagenet-tfds/blob/master/setup.py as an example of how to do it.

Concluding Remarks

Based on my experience so far with tensorflow_datasets it does achieve its goal of providing the necessary abstractions and support for mundane and repetitive tasks while at the same time providing the flexibility to deal with variety of datasets. It is also easily possible to add the support for datasets that you may not want to disclose and the ones that reside outside of tensorflow_datasets .