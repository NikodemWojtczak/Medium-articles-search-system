Now let us plot the histograms for the numerical features. We’ll use two very handy data visualization libraries, Matplotlib and Seaborn (which builds upon Matplotlib):

import matplotlib.pyplot as plt

import seaborn as sns quan = list(data.loc[:, data.dtypes != 'object'].columns.values)

grid = sns.FacetGrid(pd.melt(data, value_vars=quan),

col='variable', col_wrap=4, height=3, aspect=1,

sharex=False, sharey=False)

grid.map(plt.hist, 'value', color="steelblue")

plt.show()

As a final glance we will have a look at basic linear correlations between the numerical features. First, let us visualize those with a Seaborn heatmap:

sns.heatmap(data._get_numeric_data().astype(float).corr(),

square=True, cmap='RdBu_r', linewidths=.5,

annot=True, fmt='.2f').figure.tight_layout()

plt.show()

In addition, we can also output the correlations of each feature with the dependent variable:

>>> data.corr(method='pearson').iloc[0].sort_values(ascending=False)

impressions 1.000000

budget 0.556317

days 0.449491

google_display 0.269616

google_search 0.164593

instagram 0.073916

start_month 0.039573

start_week 0.029295

end_month 0.014446

end_week 0.012436

facebook -0.382057

Here we can see that the number of impressions is positively correlated with the budget amount and the campaign duration in days, and negatively correlated with the binary option to use Facebook as a channel. However, this only shows us a pair-wise linear relationship and can merely serve as a crude initial observation.

Pre-process your data

Before we can begin to construct a predictive model, we need to make sure our data is clean and usable, since here applies: “Garbage in, garbage out.”

We are lucky to be presented with a fairly well-structured dataset in this case, but we should still go through a quick pre-processing pipeline specific to the challenge at hand:

Only keep rows where the dependent variable is greater than zero since we only want to predict outcomes larger than zero (theoretically values equal to zero are possible, but they won’t help our predictions). Check for columns with missing data and decide whether to drop or fill them. Here we’ll drop columns with more than 50% missing data since these features cannot add much to the model. Check for rows with missing values and decide whether to drop or fill them (doesn’t apply for the sample data). Put rare categorical values (e.g. with a share of less than 10%) into one “other” bucket to prevent overfitting our model to those specific occurrences. Encode categorical data into one-hot dummy variables since the models we will use require numerical inputs. There are various ways to encode categorical data, this article provides a very good overview in case you’re interested to learn more. Specify dependent variable vector and independent variable matrix. Split dataset into training and test set to properly evaluate your models’ goodness of fit after training. Scale features as required for one of the models we are going to build.

Here’s the full pre-processing code:

Train your models

Finally, we can proceed to build and train multiple regressors to ultimately predict the outcome (value of the dependent variable), i.e. the number of impressions for the marketing campaign in question. We will try four different supervised learning techniques — linear regression, decision trees, random forest (of decision trees) and support vector regression — and will implement those with the respective classes provided by the Scikit-learn library, which was already used to scale and split the data during pre-processing.

There are many more models we could potentially use to develop regressors, e.g. artificial neural networks, which might yield even better predictors. However, the focus of this article is rather to explain some of the core principles of such regression in a — for the most part — intuitive and interpretable way than to produce the most accurate predictions.

Linear regression

Constructing a linear regressor with Scikit-learn is very simple and only requires two lines of code, importing the required function from Scikit’s linear model class and assigning it to a variable:

from sklearn.linear_model import LinearRegression

linear_regressor = LinearRegression(fit_intercept=True, normalize=False, copy_X=True)

We want to keep the default parameters since we need an intercept (result when all features are 0) to be calculated and we do not require normalization preferring interpretability. The regressor will calculate the independent variable coefficients and the intercept by minimizing the sum of the squared errors, i.e. the deviations of the predicted from the true outcome, which is known as the Ordinary Least Squares method.

We can also output the coefficients and their respective p-values, the probability of the output being independent from (here also uncorrelated to) a specific feature (which would be the null-hypothesis of the coefficient equaling 0), and hence a measure of statistical significance (the lower the more significant).

Having visualized the correlations between the numerical features earlier during our “first glance”, we expect the features “budget”, “days” and “facebook” to carry relatively small p-values with “budget” and “days” having positive and “facebook” a negative coefficient. The statsmodels module provides an easy way to output this data:

model = sm.OLS(self.y_train, sm.add_constant(self.X_train)).fit()

print(model.summary())

The p-value here was calculated using a t-statistic or -score based on the t-distribution. The summary also gives us a first hint at the accuracy or goodness of fit for the whole model, scored by the coefficient of determination R-squared measuring the share of variance in the output explained by the input variables, here 54.6%.

However, in order to compare all models and to suit our particular challenge we will use a different scoring method which I call “Mean Relative Accuracy”, defined as 1 - mean percentage error = 1 - mean(|(prediction - true value) / true value|). This metric is obviously undefined if the true value is 0, but in our case this is not relevant as we check this condition in the pre-processing step (see above) and we will thus obtain good interpretability matching an intuitive definition of accuracy. We will calculate this score for all models using five-fold cross-validation, randomly splitting the dataset five times and taking the mean of each of the scores. Scitkit-learn also provides a handy method for this:

linear_score = np.mean(cross_val_score(estimator=linear_regressor,

X=X_train, y=y_train, cv=5,

scoring=mean_relative_accuracy))

The training score we obtain for the linear regressor is 0.18; ergo the best fit we were able to produce with this model results only in a 18% prediction accuracy. Let’s hope the other models are able to outperform this.

Decision Tree

Next up is a regressor made from a single decision tree. Here we will use a Scikit-learn function with a few more arguments, so-called hyperparameters, than for the linear model, including some we don’t know the desired settings for yet. That’s why we are going to introduce a concept called Grid Search. Grid Search, again available from Scikit-learn, lets us define a grid or matrix of parameters to test when training the prediction model and returns the best parameters, i.e. the ones yielding the highest score. This way we could test all available parameters for the Decision Tree Model, but we will focus on two of those, the “criterion” to measure the quality of a split of one branch into two and the minimum number of samples (data points) for one leaf (final node) of the tree. This will help us to find a good model with the training data while limiting overfitting, i.e. failing to generalize from the training data to new samples. From now on we will also set a random state equal to one for all stochastic calculations so that you will receive the same values coding along. The rest works similar to the linear regression we’ve built earlier:

tree_parameters = [{'min_samples_leaf': list(range(2, 10, 1)),

'criterion': ['mae', 'mse'],

'random_state': [1]}]

tree_grid = GridSearchCV(estimator=DecisionTreeRegressor(),

param_grid=tree_parameters,

scoring=mean_relative_accuracy, cv=5,

n_jobs=-1, iid=False)

tree_grid_result = tree_grid.fit(X_train, y_train)

best_tree_parameters = tree_grid_result.best_params_

tree_score = tree_grid_result.best_score_

The best parameters chosen from the grid we defined include the mean squared error as the criterion to determine the optimal split at each node and a minimum of nine samples for each leaf, yielding a mean relative (training) accuracy of 67% — which is already a lot better compared to the 18% from the linear regression.

One of the advantages of a decision tree is that we can easily visualize and intuitively understand the model. With Scikit-learn and two lines of code you can generate a DOT representation of the fitted decision tree which you can then convert into a PNG image: