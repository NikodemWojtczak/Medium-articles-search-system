VAE for CelebA Dataset

The CelebFaces Attributes Dataset (CelebA) is a large-scale face attributes dataset with more than 200K celebrity images, each with 40 attribute annotations. The images in this dataset cover large pose variations and background clutter. CelebA has large diversities, large quantities, and rich annotations, including

10,177 number of identities,

202,599 number of face images, and

5 landmark locations, 40 binary attributes annotations per image.

You can download the dataset from Kaggle here:

The first step is to import all our necessary functions and extract the data.

Imports

import shutil

import errno

import zipfile

import os

import matplotlib.pyplot as plt

Extract Data

# Only run once to unzip images

zip_ref = zipfile.ZipFile('img_align_celeba.zip','r')

zip_ref.extractall()

zip_ref.close()

Custom Image Generator

This step is likely something most readers have not used before. Due to the huge size of our data, it may not be possible to load the dataset into the memory of your Jupyter Notebook. This is a pretty normal problem to have when working on large datasets.

A workaround for this is to use a stream generator, which streams batches of data (images in this case) into memory sequentially, thereby limiting the amount of memory that is required for the function. The caveat to this is that they are a bit complicated to understand and code, as they require a reasonable understanding of computer memory, GPU architecture, etc.

# data generator

# source from https://medium.com/@ensembledme/writing-custom-keras-generators-fe815d992c5a

from skimage.io import imread



def get_input(path):

"""get specific image from path"""

img = imread(path)

return img



def get_output(path, label_file = None):

"""get all the labels relative to the image of path"""

img_id = path.split('/')[-1]

labels = label_file.loc[img_id].values

return labels



def preprocess_input(img):

# convert between 0 and 1

return img.astype('float32') / 127.5 -1



def image_generator(files, label_file, batch_size = 32):

while True:



batch_paths = np.random.choice(a = files, size = batch_size)

batch_input = []

batch_output = []



for input_path in batch_paths:



input = get_input(input_path)

input = preprocess_input(input)

output = get_output(input_path, label_file = label_file)

batch_input += [input]

batch_output += [output]

batch_x = np.array(batch_input)

batch_y = np.array(batch_output)



yield batch_x, batch_y



def auto_encoder_generator(files, batch_size = 32):

while True:

batch_paths = np.random.choice(a = files, size = batch_size)

batch_input = []

batch_output = []



for input_path in batch_paths:

input = get_input(input_path)

input = preprocess_input(input)

output = input

batch_input += [input]

batch_output += [output]

batch_x = np.array(batch_input)

batch_y = np.array(batch_output)



yield batch_x, batch_y

For more information on writing custom generators in Keras, a good article to check out is the one I referenced in the above code:

Load the Attribute Data

Not only do we have images for this dataset, but each image also has a list of attributes corresponding to aspects of the celebrity. For example, there are attributes describing whether the celebrity is wearing lipstick, or a hat, whether they are young or not, whether they have black hair, etc.

# now load attribute



# 1.A.2

import pandas as pd

attr = pd.read_csv('list_attr_celeba.csv')

attr = attr.set_index('image_id')



# check if attribute successful loaded

attr.describe()

Finish Making the Generator

Now we finish making the generator. We set the image name length to 6 since we have a 6 digit number of images in our dataset. This section of code should make sense after reading the custom Keras generator article.

import numpy as np

from sklearn.model_selection import train_test_split IMG_NAME_LENGTH = 6 file_path = "img_align_celeba/"

img_id = np.arange(1,len(attr.index)+1)

img_path = []

for i in range(len(img_id)):

img_path.append(file_path + (IMG_NAME_LENGTH - len(str(img_id[i])))*'0' + str(img_id[i]) + '.jpg') # pick 80% as training set and 20% as validation set

train_path = img_path[:int((0.8)*len(img_path))]

val_path = img_path[int((0.8)*len(img_path)):] train_generator = auto_encoder_generator(train_path,32)

val_generator = auto_encoder_generator(val_path,32)

We can now pick three images and check that attributes make sense.

fig, ax = plt.subplots(1, 3, figsize=(12, 4))

for i in range(3):

ax[i].imshow(get_input(img_path[i]))

ax[i].axis('off')

ax[i].set_title(img_path[i][-10:])

plt.show()



attr.iloc[:3]

Three random images along with some of their attributes.

Building and Training a VAE Model

First, we will create and compile a Convolutional VAE Model (including encoder and decoder) for the celebrity faces dataset.

More Imports

from keras.models import Sequential, Model

from keras.layers import Dropout, Flatten, Dense, Conv2D, MaxPooling2D, Input, Reshape, UpSampling2D, InputLayer, Lambda, ZeroPadding2D, Cropping2D, Conv2DTranspose, BatchNormalization

from keras.utils import np_utils, to_categorical

from keras.losses import binary_crossentropy

from keras import backend as K,objectives

from keras.losses import mse, binary_crossentropy

Model Architecture

Now we can create and make a summary of the model.

b_size = 128

n_size = 512

def sampling(args):

z_mean, z_log_sigma = args

epsilon = K.random_normal(shape = (n_size,) , mean = 0, stddev = 1)

return z_mean + K.exp(z_log_sigma/2) * epsilon



def build_conv_vae(input_shape, bottleneck_size, sampling, batch_size = 32):



# ENCODER

input = Input(shape=(input_shape[0],input_shape[1],input_shape[2]))

x = Conv2D(32,(3,3),activation = 'relu', padding = 'same')(input)

x = BatchNormalization()(x)

x = MaxPooling2D((2,2), padding ='same')(x)

x = Conv2D(64,(3,3),activation = 'relu', padding = 'same')(x)

x = BatchNormalization()(x)

x = MaxPooling2D((2,2), padding ='same')(x)

x = Conv2D(128,(3,3), activation = 'relu', padding = 'same')(x)

x = BatchNormalization()(x)

x = MaxPooling2D((2,2), padding ='same')(x)

x = Conv2D(256,(3,3), activation = 'relu', padding = 'same')(x)

x = BatchNormalization()(x)

x = MaxPooling2D((2,2), padding ='same')(x)



# Latent Variable Calculation

shape = K.int_shape(x)

flatten_1 = Flatten()(x)

dense_1 = Dense(bottleneck_size, name='z_mean')(flatten_1)

z_mean = BatchNormalization()(dense_1)

flatten_2 = Flatten()(x)

dense_2 = Dense(bottleneck_size, name ='z_log_sigma')(flatten_2)

z_log_sigma = BatchNormalization()(dense_2)

z = Lambda(sampling)([z_mean, z_log_sigma])

encoder = Model(input, [z_mean, z_log_sigma, z], name = 'encoder')



# DECODER

latent_input = Input(shape=(bottleneck_size,), name = 'decoder_input')

x = Dense(shape[1]*shape[2]*shape[3])(latent_input)

x = Reshape((shape[1],shape[2],shape[3]))(x)

x = UpSampling2D((2,2))(x)

x = Cropping2D([[0,0],[0,1]])(x)

x = Conv2DTranspose(256,(3,3), activation = 'relu', padding = 'same')(x)

x = BatchNormalization()(x)

x = UpSampling2D((2,2))(x)

x = Cropping2D([[0,1],[0,1]])(x)

x = Conv2DTranspose(128,(3,3), activation = 'relu', padding = 'same')(x)

x = BatchNormalization()(x)

x = UpSampling2D((2,2))(x)

x = Cropping2D([[0,1],[0,1]])(x)

x = Conv2DTranspose(64,(3,3), activation = 'relu', padding = 'same')(x)

x = BatchNormalization()(x)

x = UpSampling2D((2,2))(x)

x = Conv2DTranspose(32,(3,3), activation = 'relu', padding = 'same')(x)

x = BatchNormalization()(x)

output = Conv2DTranspose(3,(3,3), activation = 'tanh', padding ='same')(x)

decoder = Model(latent_input, output, name = 'decoder')



output_2 = decoder(encoder(input)[2])

vae = Model(input, output_2, name ='vae')

return vae, encoder, decoder, z_mean, z_log_sigma



vae_2, encoder, decoder, z_mean, z_log_sigma = build_conv_vae(img_sample.shape, n_size, sampling, batch_size = b_size)

print("encoder summary:")

encoder.summary()

print("decoder summary:")

decoder.summary()

print("vae summary:")

vae_2.summary()

Define the VAE Loss

def vae_loss(input_img, output):

# Compute error in reconstruction

reconstruction_loss = mse(K.flatten(input_img) , K.flatten(output))



# Compute the KL Divergence regularization term

kl_loss = - 0.5 * K.sum(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis = -1)



# Return the average loss over all images in batch

total_loss = (reconstruction_loss + 0.0001 * kl_loss)

return total_loss

Compile the Model

vae_2.compile(optimizer='rmsprop', loss= vae_loss)

encoder.compile(optimizer = 'rmsprop', loss = vae_loss)

decoder.compile(optimizer = 'rmsprop', loss = vae_loss)

Train the Model

vae_2.fit_generator(train_generator, steps_per_epoch = 4000, validation_data = val_generator, epochs=7, validation_steps= 500)

We randomly choose some images of the training set, run them through the encoder to parameterize the latent code, and then reconstruct the images with the decoder.

import random

x_test = []

for i in range(64):

x_test.append(get_input(img_path[random.randint(0,len(img_id))]))

x_test = np.array(x_test)

figure_Decoded = vae_2.predict(x_test.astype('float32')/127.5 -1, batch_size = b_size)

figure_original = x_test[0]

figure_decoded = (figure_Decoded[0]+1)/2

for i in range(4):

plt.axis('off')

plt.subplot(2,4,1+i*2)

plt.imshow(x_test[i])

plt.axis('off')

plt.subplot(2,4,2 + i*2)

plt.imshow((figure_Decoded[i]+1)/2)

plt.axis('off')

plt.show()

Random samples from training set compared to their VAE reconstruction.

Notice that the reconstructed images share similarities with the original versions. However, the new images are a bit blurry, which is a known phenomenon of VAEs. This has been hypothesized to be due to the fact that variational inference optimizes a lower bound to the likelihood, not the actual likelihood itself.

Latent Space Representation

We can choose two images with different attributes and plot their latent space representations. Notice that we can see some differences between the latent codes, which we might hypothesize as explaining the differences between the original images.

# Choose two images of different attributes, and plot the original and latent space of it



x_test1 = []

for i in range(64):

x_test1.append(get_input(img_path[np.random.randint(0,len(img_id))]))

x_test1 = np.array(x_test)

x_test_encoded = np.array(encoder.predict(x_test1/127.5-1, batch_size = b_size))

figure_original_1 = x_test[0]

figure_original_2 = x_test[1]

Encoded1 = (x_test_encoded[0,0,:].reshape(32, 16,)+1)/2

Encoded2 = (x_test_encoded[0,1,:].reshape(32, 16)+1)/2



plt.figure(figsize=(8, 8))

plt.subplot(2,2,1)

plt.imshow(figure_original_1)

plt.subplot(2,2,2)

plt.imshow(Encoded1)

plt.subplot(2,2,3)

plt.imshow(figure_original_2)

plt.subplot(2,2,4)

plt.imshow(Encoded2)

plt.show()

Sampling from Latent Space

We can randomly sample 15 latent codes and decode them to generate new celebrity faces. We can see from this representation that the images generated by our model is of great similar styles with those images in our training set and it is also of good reality and variations.

# We randomly generated 15 images from 15 series of noise information n = 3

m = 5

digit_size1 = 218

digit_size2 = 178

figure = np.zeros((digit_size1 * n, digit_size2 * m,3))



for i in range(3):

for j in range(5):

z_sample = np.random.rand(1,512)

x_decoded = decoder.predict([z_sample])

figure[i * digit_size1: (i + 1) * digit_size1,

j * digit_size2: (j + 1) * digit_size2,:] = (x_decoded[0]+1)/2 plt.figure(figsize=(10, 10))

plt.imshow(figure)

plt.show()

So it seems that our VAE model is not particularly good. With more time and better selection of hyperparameters and so on, we would probably have achieved a better result than this.

Now let us compare this result to a DC-GAN on the same dataset.