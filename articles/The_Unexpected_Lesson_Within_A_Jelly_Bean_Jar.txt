A good way to start this comparison is probably by providing a definition of what neurons do in Artificial Neural Networks. I found this description to be rather compelling and simple to understand⁴:

Each neuron receives one or more input signals x 1, x 2, …, x m and outputs a value y to neurons of the next layer and so forth. The output y is a nonlinear weighted sum of input signals.

Under this point of view then, neurons in an ANN are the individuals of a collective thinking. In fact, the de facto architecture of ANN’s is a collection of connected individual regressors³. The output of a neuron with n input neurons is defined by⁵ :

Each output h then is a function with parameters W and b of the sum of individual linear regressions from all inputs x, which in turn will be the input (after an activation function, usually non-linear³ ⁶) of the next layer. The neurons collectively and only collectively, solve tasks. Try building an ANN classifier for a complex task with one neuron, you must probably going to fail. This will be like Galton asking one single person to give an estimate of the ox’s weight. The estimation is probably going to be wrong. It is here where ANN’s really work collectively. This concept can be visualized in the next example:

Taken from here

In the image above the trained NN is taking as input 784 features from the image of a “2" and will classify it accordingly. The complexity of the system increases drastically with each added neuron, but in turn increases the amount of possible feature permutations that effectively pushes up the performance of the classifier. Add too many though and you will be a victim of overfitting⁷. I recommend you to visit this Google Playground to understand these and other concepts better where you can see the effect each added (or removed) neuron has on a simple classifier. Try training the model with only the first two features (X¹ and X²) and see the results. Now do it with more. can you find the minimum amount of neurons needed to get good results? Do you need many neurons/layers to do simple tasks? The answer is no. Will get back to this in a moment.

Going back to oxen and jelly beans, this will be like finding the minimum amount of individuals required for a very good estimation. Surely asking 10,000 people about the weight of the ox will reduce the error, but at 800 we are already 99% around the ground truth. Increasing the complexity of an algorithm is useful only when the desired output has not been satisfied. From here, computationally speaking will be best to reduce the amount of estimators to find the minimum required to reach the desired performance. The vox populi reduces the cost of the computation once this balance is found. To understand this, we can look at the next figure I quickly made in Python:

Distribution plots with μ = 1 and σ = 0.1 from 10 to 1000 points.

We can create a set of random normal distributions with μ = 1 and σ = 0.1 while increasing the amount of samples from 10 to 1000. Because we know that the mean ground truth is by design equal to 1, we can then compute the average across these distributions and see how close it gets to μ. As you might have guessed, the more data we have the better, meaning that our estimation gets closer and closer to our ground truth. After infinite samples we reach μ, but this is unpractical for obvious reasons. It might even be that that 1000 samples is too costly for whatever reason and we decide to use the set with 500 points for our analysis, which yields an error that satisfy our needs. It is our sweet spot: general enough to maximize performance, but specific enough to minimize error. Artificial Neural Networks follow a similar (albeit not identical, mind you) principle.