Convolutional neural networks are the workhorse behind a lot of the progress made in deep learning during the 2010s. These networks have revolutionized tasks such as image classification and object detection, but they also work remarkably well in other contexts such as text classification, speech recognition, or any domain where a filter can be used to detect similarities in regions of input data.

In this post I will go over how to build a basic CNN in from scratch using numpy. This exercise goes into the nuts and bolts for how these networks actually work. In practice, it is common to use deep learning frameworks such as Tensorflow or Pytorch. These frameworks are great, but it is impossible to understand what a convolutional neural network is actually doing at each step when all you have to do is type a few lines of code to create a CNN.

The Concept:

Feedforward neural networks are powerful tools, but they don’t work well on images. A feedforward neural network takes a 32x32x3 image — 32 pixels high, 32 pixels wide, and 3 pixels deep one for red, green, and blue— and classifies it. In order to run an image through a feedforward neural network the image is stretched out to be a 3072X1 (32 *32 *3 =3072) numpy array. It is then multiplied by weights, the loss is optimized, and the image is classified based on the score it has been assigned. This process works, but it is not capable of producing state of the art results because stretching the image into a one dimensional vector does not preserve the spatial structure of the image. This means that the features used in classification tasks are not as precise as they could be if the image had its spatial structure preserved across all three dimensions. The outcomes of deep learning tasks are dependent on the quality of the data that they are fed. Producing state of the art results requires a solution that is capable of extracting state of the art features from an image.

Convolutional neural networks are that solution. CNN’s produce state of the art results because they constantly preserve the spatial structure of an image. This means that better features will be extracted for the classification task at the end. A CNN takes a 32x32x3 image slides a filter of the same depth over the image to produce a 2D activation map which contains a score that measures the similarity between the filter and the image. The stack of activation maps is used for the next…