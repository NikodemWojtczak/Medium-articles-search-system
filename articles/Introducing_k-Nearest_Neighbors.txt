Introducing k-Nearest Neighbors

The k-nearest neighbors algorithm is based on a very simple premise: That things that are close together have a lot in common. This premise can be seen all over the world:

The closest neighbors of college students are typically other college students.

We tend to have more in common with our romantic partners, with whom we share a home, than with the people in the next town over.

In grocery stores, each type of produce is stored in a group rather than distributed around the entire store.

The k-nearest neighbors modelling approach uses this premise to assume that we can take knowledge we have of one point, and draw conclusions about the neighboring points. Since similar things are commonly clustered together we can use that proximity to make assumptions about the data points that we don’t know. In a grocery store, the item next to an apple is typically another apple. More specifically, the item next to a gala apple is typically another gala apple. Similar for people’s political orientations; as we continue to gather more and more into politically polarized towns and cities, if we know that one person in a city is a conservative or liberal, we can reasonably safely assume that their neighbors are also conservative or liberal.

Since it’s based on an assumption like this instead of a complex statistical model, k-nearest neighbors is the simplest predictive model.

How does the k-nearest neighbors model work?

In order to use the k-nearest neighbors algorithm, you need two things: