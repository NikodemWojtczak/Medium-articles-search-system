Google Cloud Platform’s AI Platform (formerly ML Engine) offers a hyperparameter tuning service for your models. Why should you take the extra time and effort to learn how to use it instead of just running the code you already have on a virtual machine? Are the benefits worth the extra time and effort?

One reason to use it is that AI Platform offers Bayesian Optimization out of the box. It is more efficient than a grid search and will be more accurate than a random search in the long run. They also offer early stopping and the ability to resume a completed training job. That allows you to use past data to run a few more well informed trails if you think you haven’t ran enough trials the first time.

All of that can help speed the hyperparameter tuning process up while doing a better job than either grid search or random search to find the best fit. When training on lot’s of data frequently or running lots of experiments it can definitely save money and time.

Stochastic Gradient Descent Visualized Source

As always, all my code can be found on my git hub page. After going through how to run a hyperparameter tuning job on GCP, I’ll compare the training time and cost for three predefined machines that they offer.

I’ll also show how to retrieve the best hyperparameters from the training run automatically from the api. You can then pass those directly into your production model or if you’re running experiments save them in a csv on Google Cloud Storage, a table in BigQuery or wherever you want for analysis later.

So what is Bayesian Optimization for hyperparameter tuning anyway? Hyperparameter tuning is an optimization problem which means the exact nature is unknown and expensive to compute so it would be beneficial to go about it in an informed way.

Bayesian optimization is an extremely powerful technique when the mathematical form of the function is unknown or expensive to compute. The main idea behind it is to compute a posterior distribution over the objective function based on the data (using the famous Bayes theorem), and then select good points to try with respect to this distribution. Source

That means that as as more trials run and the posterior distribution begins to improve, the hyperparameter values…