Section 1: No balancing

Here we plot the precision results of having no balancing at all:

Average Train Precision among C-V folds: 76.8 %

Average Test Precision among C-V folds: 71.98 %

Single Test set precision: 75.86 %

Single Test set Low Class(No-Fraud) Precision: 99.9 %

Single Test set High Class(Fraud) Precision: 75.9 %

Precision measurements of the CV folds seem quite unstable.

Some precision metrics are above their corresponding precision on the train set.

Big differences appear among the test set precision of the CV folds.

The average test set precision of the CV is not extremely away from the unknown’s fraud dataset, but still the difference is important.

Thus we shall examine balancing for our dataset.

Section 2: Balancing outside C-V (under-sampling)

Here we plot the precision results of balancing, with under-sampling, only the train subset before applying CV on it:

Average Train Precision among C-V folds: 99.81 %

Average Test Precision among C-V folds: 95.24 %

Single Test set precision: 3.38 %

Single Test set Low Class(No-Fraud) Precision: 100.0 %

Single Test set High Class(Fraud) Precision: 3.40 %

Like before:

- CV results seem mostly high, but unstable.

- Some precision metrics are above their corresponding precision on the train set.

- Big differences appear among the test set precision of the CV folds.

- CV results seem mostly high, but unstable. - Some precision metrics are above their corresponding precision on the train set. - Big differences appear among the test set precision of the CV folds. Additionally, any positive interpretation of some of the above high numbers is probably wrong,since the ‘unknown’ imbalanced test gives bad precision.

The above difference sources from the fact that:

- each fold’s test set is balanced,

- each fold’s model is fitted on balanced train fold data.

- each fold’s test set is balanced, - each fold’s model is fitted on balanced train fold data. Thus the model goes well in CV, since it is trained & tested on similar class distributions(balanced).

However an ‘unknown’ fraud dataset will not be balanced, thus we see very low precision on the test set.

Section 3: Balancing inside C-V (under-sampling)

Here we plot the precision results of balancing, with under-sampling,

only the train set of each CV fold before fitting the model on it and

making predictions on the CV fold’s test set:

Average Train Precision among C-V folds: 99.21 %

Average Test Precision among C-V folds: 4.2 %

Single Test set precision: 3.38 %

Single Test set Low Class(No-Fraud) Precision: 100.0 %

Single Test set High Class(Fraud) Precision: 3.40 %

We see stable precision results among CV folds, but not efficient enough.

We see the precision on unknown data very close to the average precision of the CV folds.

What we have done here is, that we made each fold’s model recognize the existence of fraud transactions, and try to predict on a realistic, unbalanced version of unknown test (fold) data.

Still the precision is very bad, in contrast to the ~ 76% of the unbalanced version.

Does this mean we give up on balancing?

A next step could be to examine different balancing ratios.

But first let’s examine over-sampling instead of under-sampling, because our fraud transactions are too few.

Section 4: Balancing outside C-V with over-sampling

Here we plot the precision results of balancing, with over-sampling,

only the train subset before applying CV on it:

Average Train Precision among C-V folds: 98.51 %

Average Test Precision among C-V folds: 98.52 %

Single Test set precision: 12.61 %

Single Test set Low Class(No-Fraud) Precision: 100.0 %

Single Test set High Class(Fraud) Precision: 12.6 %

Results are more stable than in Section 2, thanks to over-sampling;

under-sampling left too few instances.

under-sampling left too few instances. Yet again, CV scores do not represent the reality of the unknown test set.

Section 5: Balancing inside C-V (over-sampling)

Here we plot the precision results of balancing, with over-sampling,

only the train set of each CV fold before fitting the model on it and making predictions on the CV fold’s test set:

Average Train Precision among C-V folds: 98.38 %

Average Test Precision among C-V folds: 8.7 %

Single Test set precision: 12.61 %

Single Test set Low Class(No-Fraud) Precision: 100.0 %

Single Test set High Class(Fraud) Precision: 12.6 %

Thanks to oversampling precision score rises from that of Section 3.

Still though, we get in each CV fold prediction score results close enough to those that the model will produce on an unknown test dataset.

It is clear, that balancing so far did not help in getting good test results.

However, this is out of scope for this article ( :-) ) and the goal of this article is achieved:

To make the model produce, on each CV fold’s test set, evaluation metric scores similar to those that it would produce on an unknown one,

for the case that the train data are balanced.