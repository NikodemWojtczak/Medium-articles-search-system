Despite some impressive achievements, and breathless predictions about the future, it feels like something might be wrong in the world of AI. No, this isn’t yet another post about “our new silicone masters”. In fact, what keeps me awake at night is something like the opposite:

If you believe the function of AI research is building intelligent machines, you could be forgiven for thinking that “intelligence” means the same thing as “problem-solving ability”. The soft stuff: Kindness, love, compassion, intuition; things that arguably define real intelligence, and certainly keep us honest, aren’t getting much attention. For the moment, at least, they’re in the “too hard” basket.

I always thought the domain of Language Understanding could offer us a bridge between the two worlds: A small subset of the full emotional landscape but with tangible units of meaning. Just look at the trouble they’re having though.

1. It’s all Getting Way too Big

For a few years now, IBM have been dining out on the success of “Watson”, their Question Answering supercomputer which famously won “Jeopardy!” in 2011. Impressive, yes, but have you ever thought about the size of the thing? Here’s a clue: It won’t fit in your pocket.

Back then, “Watson” was actually a cluster of 90 servers, each with 8 cores running 4 threads in parallel, and 16 terabytes of RAM (yes, RAM). For the uninitiated, that’s about 360 high spec laptops joined together and a LOT of extra memory. It wasn’t running Android either: Specialised hardware has to be programmed specifically for its task — in this case, a quiz show. It was also the size of a bedroom.

True, since then, IBM has made significant improvements. Watson now fits in the bottom of your fridge. But here’s the real problem: after all those years of research and untold millions of dollars, it still only does one thing.

Jameson Toole’s great Medium article on this really nailed it down for me: Where language understanding is concerned, we’re already hitting the flat part of the diminishing return curve. True, as he says, improvements and efficiencies can be made, but as far as you and I are concerned these are still just machines that find a restaurant for you (if you’re lucky) or solve simple comprehension puzzles like “who did what to whom”.

The gulf between that and the way we use language as humans is enormous. Do we really imagine that adding more cores, more inputs, more layers, more data and more power (and to hell with the environment!) will do the job? Even if we could, how can we make that technology useful if it’s confined to the server-rooms of Google or IBM? We must be missing something.

2. No More Moore’s Law

You might think that the answer to the problem of size is just to wait a bit until we have bigger, faster chips. A few years ago you might have been right. Moore’s law states that computing power (measured in number of transistors per chip) will double every two years, but the evidence is that it’s no longer holding. Below a certain size limit, it becomes very difficult to fit enough transistors on the wafer, and just isn’t profitable.

Moore’s Second Law — the cost to produce smaller transistors also increases exponentially Source: IBS

Don’t forget, for AI systems to make money, they need to be widely available. Yes, specialist technologies like GPUs and TPUs, 3D stacking and so on will allow the big players to keep evolving for a while, but a few years from now this problem is going to start bleeding into the portable device market and that’s where the big money is.

In fact, the biggest impact of all might be in the kind of “heartbeat” function that Moore’s law used to have for the industry — regulating innovation across software, hardware, retail and design sectors inside a 2 year cycle. Developers may be less prepared to take risks if they’re not sure that the hardware they need will be on spec.

3. We Don’t Understand the Problems We’re Trying to Solve

Ask yourself a question: “Why are some jokes funny?”. I don’t know either. There are some rules to it: timing and so on, but really, like painting landscapes at night, comedy can be a bit of a dark art.

Say you wanted to build a comedy AI. In the current paradigm, you’d likely approach it by building a couple of neural networks, throwing billions of human-labelled examples of funny and not-funny statements at them for a week or so, then pairing them in a “Generative Adversarial Network”. After another week or so of training on a supercomputer, it might even work (sort of).

The thing is, though, even if it did, it would learn only from the data you gave it. Feed it Monty Python and it’ll talk to you about shrubberies and dead parrots, show it episodes of Seinfeld and you’ll get nihilism and existential angst. Otherwise it’s completely inflexible. If you want to adapt it to a new situation, context or audience, you’ll have to start again.

Let’s not forget, either, that humour is just one facet of the way we use language. If you want a bot that “does compassion”, it’ll face similar limitations. Both at once? Well that, at the moment, is madness.

As humans, though, we do it naturally. We seem to have an instinct about how language is to be used in this or that situation, and no real idea how or why. My guess is that it’s more than just a rehash of things we’ve seen before, that there’s some kind of real spontaneous creation going on, but the architecture of a deep neural network doesn’t allow for that.

The Feels are Important

In its current form, AI is a really powerful tool for all kinds of things, but language use — especially emotional language — isn’t one of them.

Emotions, like compassion and kindness, are hugely important. They’re the glue that holds us together. As powerful thinking machines become more commonplace, and more often charged with decision-making, do we really want them not to feel?

In reality, the way things stand, it’s an academic question. The most advanced technology we have is nowhere close, and it’s already running into problems.

We might be here a while.