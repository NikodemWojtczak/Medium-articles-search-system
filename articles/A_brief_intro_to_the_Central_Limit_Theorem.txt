A brief intro to the Central Limit Theorem

The theory they say you can’t be a data scientist without knowing… GreekDataGuy · Follow Published in Towards Data Science · 3 min read · Nov 9, 2019 -- 1 Listen Share

According to wikipedia.

In probability theory, the central limit theorem (CLT) establishes that, in some situations, when independent random variables are added, their properly normalized sum tends toward a normal distribution (informally a “bell curve”) even if the original variables themselves are not normally distributed.

Translation: If you take enough samples from a population, the means of those samples will approximate a normal distribution.

The cool thing is that this works for a population with (almost) ANY distribution. Let’s generate some examples and prove that.

Generate random data

Generate 300 random numbers between 0 and 50.

import matplotlib.pyplot as plt

import random X = [] for i in range(0,300):

v = random.randint(0,50)

X.append(v)

Plot it so we can see the shape of the data.

plt.rcParams.update({'figure.figsize':(7,5), 'figure.dpi':100})

plt.hist(X, bins = 50)

Definitely not a normal distribution

Take samples and calculate the mean

Now take 10,000 samples with a sample size of 5, calculate the mean of each sample, and plot the frequency of the means.

import numpy as np means = [] for i in range(0,10000):

sample = []



for ii in range(0,5):

v = random.choice(X)

sample.append(v)



mean = np.mean(sample)

means.append(mean)

Plot the distribution

Plot the frequency of the mean of each sample.

Looking a little like a normal distribution. Pretty cool, huh?

Now let’s increase the sample size from 5 to 30.

Even more like a normal distribution.

And this happened despite the original population not following a normal distribution at all.

My generated data was intended to be random. But the exact same affect can be achieved with almost any distribution, continuous or discrete, beta, gamma or uniform.

Why this is important

If the mean of enough samples approximates a normal distribution, we can “pretend” the distribution of the sample itself is also normal. And feel more comfortable making inferences about the population as a whole given a single sample.

The above is my understanding but I’m not a statistician. So I’d like to put it to the crowd. Why is the central limit theorem so important to statistics and machine learning?