“Behold, the people is one, and they have all one language; and this they begin to do: and now nothing will be restrained from them, which they have imagined to do.” – Genesis 11:6 KJV¹

As conveyed in the Tower of Babel origin myth, massive cooperation is so crucial to the self-determination of our species that when a common language enabled the people of the world to unify in a collective enterprise the Almighty saw fit to intervene lest humans achieve omnipotence.² The scientific enterprise represents a modern-day Tower of Babel. It is the most massive, concerted effort in existence with an interminable goal of achieving omniscience. And there is growing recognition that science is the most effective tool we have for reducing uncertainty around the rising tide of complex issues that threaten our species and the future of the earth system.³

In 1945, Vannevar Bush, one of the most influential scientists of the 20th century, aptly observed that science has “provided a record of ideas and has enabled man to manipulate and to make extracts from that record so that knowledge evolves and endures throughout the life of a race rather than of an individual”⁴. Nonetheless, even with the rapid knowledge sharing enabled by our current techno-social infrastructure, science cannot seem to keep pace with the runaway problems we face today — many of which are side effects of our own inventions.

Though inspired by Bush’s writings, Douglas Engelbart, who invented the computer mouse, recognized that rapid knowledge sharing was not enough to address the increasingly complex problems of the world. Engelbart believed the only way to address these problems was through augmented collaboration, and advocated a bootstrapping approach geared to “improving the way we improve”. Indeed, Engelbart’s mouse was an intentional step toward tighter integration between humans and machines⁵ and, consequently, among humans connected to other humans by those machines.

Thus, before we can rapidly improve our understanding of the world, we need a framework that allows us to study and apply human/AI partnerships⁶. Combining such a framework with a commerce system that treats information processing assets and services as commodities could give rise to a thriving new Thinking Economy. Citizen science projects and their enabling technologies could become sustainable enterprises, leading to a rich marketplace of reusable and extensible citizen science platform widgets, turnkey access to communities of online cognitive laborers, and a steady supply of research datasets.

The current situation

Today, we are swimming in unprecedented computing power and connected 24/7 to a ubiquitous grid, and yet we are still drowning in data as we grapple with intractable societal issues. Humans have never been more connected in more places more of the time, but this unstructured connectivity is not optimized for purposeful knowledge transfer. For example, a social network analysis of Twitter reveals the formation of polarized subnetworks that become “echo chambers” for like-minded members⁷, rather than providing a fertile environment for new ideas that might arise from mixing diverse perspectives.

To make matters worse, scientific disciplines are becoming more specialized as new technologies enable researchers to dive deeper into niche areas, creating isolated communities with their own impenetrable folksonomies⁸, and resulting in a proliferation of knowledge siloes. There’s just too much to know⁹ for any one field to get solid traction on issues that require transdisciplinary approaches, which include most wicked¹⁰ problems today. And even if the data and knowledge needed to address these issues is latent among the learned, we lack the ability to integrate them into effective and actionable solutions.

Many have turned to machine learning (a form of Artificial Intelligence) as an avenue of recourse because of its ability to reveal complex cause-and-effect relationships from heterogenous data sets. Thanks primarily to increased computing speeds, machine learning methods that have been around for decades have suddenly become practicable for making predictions that support decision-making in areas as diverse as medical diagnosis, bank loan approval, and criminal sentencing. However, when human biases seep into the training data or contextual information is missing, you can end up with sexist or racist predictions that do not distinguish between correlation and causation.¹¹ More research is needed to address the bias issue and better understand how the respective influences of the AI and human affect outcomes. Until then, people may be adversely and unfairly impacted when these tools are used.

The value of human/AI partnerships

The holy grail of “strong AI” is to be able to perform any human cognitive task — in other words, to achieve humanlike intelligence. Surprisingly, this ability exists today. It’s called… humans! Indeed, AI systems are often trained using data that are the product of human cognition, which has increased demand for them. One method for acquiring such human-generated data is through “citizen science” in which online volunteers help acquire or analyze data in exchange for the opportunity to participate in scientific research. These projects are often developed to address a research need that cannot be met using machine-based methods alone because some aspect of the work (such as classifying an image) can only be accomplished adequately by a human.

One such project recently made waves when, on April 13, 2019, thousands of public volunteers from around the globe jointly set a new world record in biomedical data analysis by accomplishing three months of Alzheimer’s disease research in less than a weekend¹². They did this by playing an online game we created called “Stall Catchers”¹³. Like most citizen science projects, Stall Catchers exemplifies an emerging class of hybrid distributed information processing systems¹⁴. These so-called human computation (HCOMP) systems leverage the respective strengths of humans and machines to achieve futuristic AI capabilities today¹⁵. Thus, these hybrid systems achieve unprecedented capabilities simply by looping in humans to handle the cognitive tasks that elude even the most advanced AI systems¹⁶.

Stall Catchers is not the only success case. HCOMP is advancing cancer¹⁷ and HIV¹⁸ research, diagnosing malaria¹⁹ in sub-Saharan Africa, reducing female genital mutilation in Tanzania²⁰, predicting flood effects in Togo²¹, endowing the blind with real-time scene understanding²², expediting disaster relief despite language barriers and failing infrastructure²³, rewriting our understanding of cosmology²⁴ ²⁵, and improving predictions in conservation science²⁶.

Barriers

Today, applying HCOMP to a major societal problem is a “high risk / high reward” enterprise. It requires a substantial investment of time and money and has a low probability of success. There is no straightforward integration path for linking AI and human-based processing components, there is no mechanism for workflow automation, data pipelines are hobbled together via manual researcher interventions and custom-built adapters, there’s a lack of modularity and reuse at transferrable levels of granularity, and there’s no standard approach for validation methods or data quality assurance.

Although the recent Stall Catchers event demonstrates real-world utility and impact, as with many successful HC platforms, it took several years to fully develop the complex information processing system that made this feat possible. How did we do it? First, we built a data pipeline employing a convolutional neural network to pre-process the raw data and isolate the cognitive task. Next we developed a gamified crowdsourcing platform with a closely coupled human and AI system²⁷. To motivate participation, we created a bias-neutral reward engine but also continuously incorporated user feedback to improve the interaction design. Finally, we developed an optimizing dynamic consensus algorithm for machine-assisted validation of the crowd-generated data. All with the singular goal of maximizing analytic throughput while meeting stringent biomedical data quality requirements.

If each citizen science project tends to be treated as a tabula rasa then it’s no wonder the success rate is low and time to fruition is high. But each component in each successful project is potentially separable. Indeed, there is no principled reason why each piece couldn’t be made into module that is shared, such that it could be reused, repurposed, and extended into new modules with new capabilities. In practice, however, this will happen only if there is a convenient way to do it and a benefit to the originator.

Another key issue is sustainability. For many of these projects, a central objective is to use the human-generated data to train an AI system that can free up the volunteers or at least absorb some of the workload. But this outcome is rare. More often, just as these citizen science projects develop a thriving community, validate their methods, and begin producing high quality research data, their funding dries up and they cannot be easily sustained (e.g., see inline excerpt from Dr. Tsueng’s termination letter to Mark2Cure participants). This often occurs because they are supported by time-limited grants scoped either to proof of concept development or to addressing specific research questions.

“Unfortunately, we do not have the resources needed to fix all the issues that are plaguing the current iteration of Mark2Cure that damper the user experience and hamper participation. While our research has demonstrated that Mark2Curators can contribute high quality and meaningful data, these design issues make it unlikely that we will be able to recruit enough users […] After lengthy discussions, we have decided to end contributions …” – Dr. Ginger Tseung, Scripps Research

Toward a thinking economy

What if we had a way to infuse projects with revenue to help sustain their operations? The barriers to progress described above exist within a larger ecosystem of stakeholders, all of whom are invested in new data science methods and their applications. Each stakeholder brings their²⁸ own set of challenges and related needs, but also brings potential solutions for other stakeholders. This complementarity of needs and offerings suggests a latent economy.

For example, a machine learning graduate student who craves data could provide AI modeling services for a fee or in exchange for access to large training sets generated by citizen science projects. They could then license their best performing models to academic researchers for publication credits and to for-profit entities for a licensing fee.

A citizen science project could, itself, generate revenue by providing cost-effective data analysis to grant-supported domain researchers that budgeted for those analyses. Another revenue stream could involve licensing “wisdom of crowd” algorithms or specialized interface widgets to other citizen science projects.

Meanwhile, a citizen science volunteer could develop a performance-based reputation and, on that basis, be hired as a paid cognitive laborer. Mooqita has already shown the viability of this approach by integrating online tasks from external organizations into MOOCs as learning opportunities²⁹. Some students who demonstrated proficiency in the online task have been hired to continue the same online work beyond the MOOC.

But none of this happening today because 1) an information processing exchange does not exist, and 2) we are stuck in a Knowledge Economy. The Knowledge Economy is a mindset that values the production of first order knowledge — rote recipes for using data to inform our actions in isolated contexts. Such knowledge ignores the complex interdependencies that exist among many interacting systems in the real world.

But we are slow to build higher order knowledge from the things we already know because integrating knowledge is much harder than integrating data. It requires more than data processing — it requires thinking. Consider the challenge of precision medicine, where the goal is to predict the impact of a prospective treatment based on a patient’s genome, microbiome, medical history, and lifestyle choices. This requires discovering patterns in heterogeneous multisource data and then applying interdisciplinary expertise to their interpretation. For this, we would need HCOMP.

Introducing Civium

Toward these ends, the Human Computation Institute is leading the development of Civium, an integration platform and commerce engine for sustainable human computation. The purpose of Civium is to streamline information processing for the advancement of science and to commoditize cognitive labor and its products toward a new thinking economy. For citizen science practitioners, this could reduce development time and operating costs, enable rapid community engagement, and create revenue opportunities that help sustain projects.

Today there exists a platform called brainlife.io, which enables neuroscientists to publish algorithms and datasets directly on cloud computing infrastructure. By enforcing interoperability up front, all datasets and algorithms are interchangeable and combined easily via workflows into more complex pipelines called “open services”³⁰. This approach enables turnkey reproducibility of data modeling results. Consequently, brainlife.io has received broad adoption within the Neuroscience community.

This simple, yet sensible approach seems like a suitable foundation for a broader initiative to develop an open science platform that supports human computation research, including the full life-cycle of crowd-based computing. Various crowd science methods, such as data preparation, participant engagement, mechanism design, aggregation algorithms, and validation approaches, as well as the specific dynamic workflows that coordinate these methods materially influence the successful evolution of human computation systems³¹. Moreover, the ability to quickly string together assemblies and subassemblies of these components for rapid evaluation and validation is critical for effectively calibrating various dimensions of system behavior.

Beyond extending brainlife.io to include human-based information processing that supports citizen science and HCOMP research, Civium will critically incorporate a commerce system to fuel development and sustainability. Indeed, the key assumption underlying Civium is that technical innovation cannot be sustained in an economic vacuum. Success will therefore depend critically on interweaving integration “glue” with a commerce system in ways that model and address real-world constraints and incentives. We believe that properly combining these technical and economic systems within the same platform will unleash latent market-driven forces, giving rise to a thriving ecosystem for advancing the development of thinking systems. In the following two sections, we peer one level deeper into these key aspects of Civium.

Integration

Several citizen science project builders exist today (e.g., Zooniverse, citsci.org, Spotteron, Anecdata, etc.), which typically offer a menu of options within a linear design process. Each of these is tailored to a set of use cases (e.g., image classification). These platforms enable rapid development and participant-access, though they may constrain the scope of potential datasets, applications, and data science methods that could be employed for a new project. Additionally, there are crowdsourcing libraries, like PYBOSSA and TurKit³², which are very flexible, enabling an almost limitless variety of applications, but may require extensive coding.

Civium seeks to increase the accessibility of these existing community assets and others and reduce operating costs by making them available from within a shared information processing environment built on cloud computing services where they can be combined in different ways and applied to new datasets.

Civium also encourages modularity of widgets and services so they can be shared and reused. For example, a new sickle cell disease project could combine an image preprocessing algorithm from EyeWire with a Virtual Microscope interface from stardust@home, a consensus algorithm from Stall Catchers, and then register with the SciStarter.org dashboard service while publishing the project in the Zooniverse portal service. Moreover, a workflow could use the project’s output to automatically train an AI model that provides a continuously improving collaborative input to the human cognitive task.

Sustainability

It is no longer a secret that hybrid human/machine systems and platforms are transforming the world, and demand for these capabilities will only grow. Isn’t it time to come together and address the sustainability issue for everyone in a manner that is aligned with our value systems? In addition to being an integration platform, Civium will be designed from the bottom-up with an integrated commerce engine to help seed a new “thinking economy” (as introduced at the 2019 Microsoft Faculty Summit) by creating a marketplace for information processing and digital assets. Under this model, any digital artifact or service is a potential source of revenue. For example, analytic capabilities provided by citizen science platforms can be provisioned as paid services. Functional modules (e.g., consensus algorithms) used in existing citizen science projects can be registered and made available for use in other projects. Portal services, like Zooniverse, could be purchased for a monthly fee with turnkey access, as could registration with SciStarter’s dashboard service. Even citizen science projects themselves could require a participation fee, such as Project FeederWatch, which already collects $18 annually from its users. But how could such usage policies be specified and implemented?

Civium combines integration and sustainability features within a single platform

In Civium, we want to promote open science by making it easy to share artifacts transparently, but we also want to give creators the opportunity to specify their own licensing terms whether or not financial remuneration is involved. Many researchers today share values of Open Source and Open Science as means for expediting research and impact. At the same time, researchers seem viscerally cognizant of the hardships associated with sustainable funding. This incongruity between value-driven behavior and sustainability may stem from the widely perpetuated myth that open source = free. The “free” part of open source (and Open Science, for that matter) is in the vein of source code (and data) transparency. It has nothing to do with money. For example, if someone created an algorithm and posted the module on Civium, they could say “anyone can use this as long as they credit me” (just like CC-BY), or stipulate “I own this software but I will grant a free license to any academic researchers, and a 6-month renewable license to for-profits for X dollars. Any derivative products must adhere to these licensing terms.” and so on. This could apply not only to widgets but online services (e.g., access to a large community of participants), and could be measured in many ways. Indeed, Civium provisions for a valuation and credit assignment system that keeps track of what is getting utilized and how much.

The key to making this practicable is an a la carte licensing capability, which will enable usage policies to be embedded in registered services and digital artifacts (e.g., algorithms, datasets, etc.). These will be protected by identify verification services and blockchain-based accountability, under the premise that a transparent society is a polite society. Providers will be able to stipulate any terms they choose, and the market is free to accept those terms or look elsewhere. Usage policies could additionally stipulate using published artifacts only on Civium’s underlying compute resources, which potentially addresses collaboration restrictions that might otherwise hinder collaborations among data modelers and data providers from different organizations. Geolocalized processing made possible by globally distributed Azure server farms would enable data scientists on other continents to process data on servers within their countries of origin, thus further enabling collaborative opportunities that were previously prohibited, such as the analysis of GDPR-protected genomic datasets in support of precision medicine research.

Bringing Civium to life

In contemplating this ambitious undertaking, it became apparent that related initiatives undertaken by smart and capable organizations have not come to fruition, and that it would be hubris to think the Human Computation Institute could just come along and solve this problem in isolation. So we socialized these ideas with members of our community, reflected on the reasons why such a platform does not yet exist with broad adoption, and considered what could be done differently now to make that possible. Indeed, thinking about Civium as a human computation system itself, with multiple stakeholders, helped inform this approach.

So why doesn’t something like Civium exist today? One barrier may be related to organizational limitations and mission misalignments. Large corporate solutions are quick to market and may be sustainable but serve profitability goals that introduce restrictive paywalls and bias offerings to a narrow set of industry stakeholders. Community-based initiatives, on the other hand, tend to serve broader objectives but involve slow-moving, monolithic consortia. By the time these groups develop and agree upon a set of requirements, those feature specifications no longer meet the needs of this rapidly changing field. Moreover, academically-rooted projects tend to circumvent big business by design, which blocks a potential revenue stream.

As a non-profit, Human Computation Institute is mission- rather than profit-driven, to advance HCOMP for the benefit of society. This mission aligns with seeking a common basis among academic, corporate, and governmental interests. Under HCI’s stewardship, Civium can serve the interests of many while coming to fruition on a non-geologic timescale.

What’s next?

Today, the Civium initiative is moving forward in partnership with Microsoft Research and Indiana University’s Pestilli Lab, which developed brainlife.io. A groundswell is forming in the broader community of citizen science practitioners, HCOMP researchers, and machine learning enthusiasts, and with expressed interest from key members of global coalitions seeking economies of scale for citizen science, such as the United Nations task group on aligning Citizen Science with Sustainable Development Goals³³(SDG), EU-Citizen.Science, and others. An open, community meeting will be held at the HCOMP 2019 conference in late October to answer questions and gather feedback, followed by a project kickoff meeting in January to converge on a specification and assign roles.

Civium does not intend to reinvent the wheel. In fact, the goal is just to pull relevant pieces together so they can be easily combined and reused. Civium also seeks to sustain the organizations and people who create digital artifacts and provide crowd-powered services, including the cognitive laborers themselves. Civium includes a set of core capabilities, many of which will be derived from existing platforms and services. For example, Civium will build upon brainlife.io, which provides a tried-and-true approach for interoperability and turnkey reproducibility. For its licensing engine, Civium will build upon a mature project developed by one of our other partners, GridRepublic, which applies blockchain traceability to contracts. And these core capabilities will intertwine technical integration and economic sustainability within a seamless platform where participant portals, citizen science projects, AI algorithms, and datasets can interoperate and thrive.

In some sense, Civium is an operating system for a new class of supercomputers powered by computer hardware and cognitive “wetware” that will enable us to build, improve, and deploy transformative human/AI systems in support of open science and innovation. It is also a bazaar, for sharing, trading, and finding the widgets and services we need to create and sustain the capabilities we seek, breathing new life into unsupported projects and platforms. Ultimately, we believe Civium has the potential to seed a new thinking economy that rewards the uniquely human cognitive abilities needed to tackle our most pressing societal issues.

Acknowledgements

Sincere thanks to Eglė Marija Ramanauskaitė and Jennifer Couch for extensive feedback that materially improved the structure and tone of this article. Grateful to numerous colleagues for discussions and feedback about Civium and related ideas, including Vani Mandava, Lucy Fortson, Danna Gurari, Diane Bovenkamp, Franco Pestilli, Soichi Hayashi, Darlene Cavalier, Lea Shanley, Besmira Nushi, Matthew Blumberg, Nancy Kleinrock, and many others.

References

1. Genesis. In: The King James Bible.

2. Tower of Babel. In: Wikipedia. ; 2019. https://en.wikipedia.org/w/index.php?title=Tower_of_Babel&oldid=912466515. Accessed August 27, 2019.

3. Daigle K. Public trust that scientists work for the good of society is growing. Science News. https://www.sciencenews.org/article/public-trust-scientists-work-good-society-growing. Published August 2, 2019. Accessed August 4, 2019.

4. Bush V. As We May Think. The Atlantic. https://www.theatlantic.com/magazine/archive/1945/07/as-we-may-think/303881/. Published July 1, 1945. Accessed August 27, 2019.

5. Finley K. 50 Years Later, We Still Don’t Grasp the Mother of All Demos. Wired. December 2018. https://www.wired.com/story/50-years-later-we-still-dont-grasp-the-mother-of-all-demos/. Accessed August 27, 2019.

6. Michelucci P. Human Computation and Convergence. In: Bainbridge WS, Roco MC, eds. Handbook of Science and Technology Convergence. Cham: Springer International Publishing; 2016:455–474. doi:10.1007/978–3–319–07052–0_35

7. Williams HTP, McMurray JR, Kurz T, Hugo Lambert F. Network analysis reveals open forums and echo chambers in social media discussions of climate change. Glob Environ Change. 2015;32:126–138. doi:10.1016/j.gloenvcha.2015.03.006

8. Casadevall A, Fang FC. Specialized Science. Infect Immun. 2014;82(4):1355–1360. doi:10.1128/IAI.01530–13

9. Smaldino PE, Richerson PJ. Human Cumulative Cultural Evolution as a Form of Distributed Computation. In: Michelucci P, ed. Handbook of Human Computation. New York, NY: Springer New York; 2013:979–992. doi:10.1007/978–1–4614–8806–4_76

10. Rittel HWJ, Webber MM. Dilemmas in a general theory of planning. Policy Sci. 1973;4(2):155–169. doi:10.1007/BF01405730

11. Tan S, Adebayo J, Inkpen K, Kamar E. Investigating Human + Machine Complementarity for Recidivism Predictions. ArXiv180809123 Cs Stat. August 2018. http://arxiv.org/abs/1808.09123. Accessed September 28, 2019.

12. Stevenson L. Citizen scientists set new record for accelerating Alzheimer’s research. Discov Mag Citiz Sci Salon. May 2019. http://blogs.discovermagazine.com/citizen-science-salon/2019/05/06/citizen-scientists-set-new-record-for-accelerating-alzheimers-research/. Accessed May 6, 2019.

13. Alzheimer’s Disease Could Be Cured by Thousands of Amateurs | WIRED. https://www.wired.com/story/searching-for-lost-memories-under-thousands-of-microscopes/. Accessed October 21, 2019.

14. Newman G. Citizen CyberScience ─ New Directions and Opportunities for Human Computation. Hum Comput. 2014;1(2). doi:10.15346/hc.v1i2.2

15. Michelucci P, Dickinson JL. The power of crowds. Science. 2016;351(6268):32–33. doi:10.1126/science.aad6499

16. Bowser A, Sloan M, Michelucci P, Pauwels E. Artificial Intelligence: A Policy-Oriented Introduction. Wilson Center; 2017. https://www.scribd.com/document/364545974/Artificial-Intelligence-A-Policy-Oriented-Introduction. Accessed January 8, 2019.

17. Candido Dos Reis FJ, Lynn S, Ali HR, et al. Crowdsourcing the General Public for Large Scale Molecular Pathology Studies in Cancer. EBioMedicine. 2015;2(7):681–689. doi:10.1016/j.ebiom.2015.05.009

18. Khatib F, DiMaio F, Cooper S, et al. Crystal structure of a monomeric retroviral protease solved by protein folding game players. Nat Struct Mol Biol. 2011;18(10):1175–1177. doi:10.1038/nsmb.2119

19. Luengo-Oroz MA, Arranz A, Frean J. Crowdsourcing Malaria Parasite Quantification: An Online Game for Analyzing Images of Infected Thick Blood Smears. J Med Internet Res. 2012;14(6). doi:10.2196/jmir.2338

20. Leaders MO. Put Rural Tanzania on the Map. Read Write Particip. April 2018. https://medium.com/read-write-participate/put-rural-tanzania-on-the-map-79d0888df210. Accessed May 2, 2019.

21. Suarez P. Rethinking Engagement: Innovations in How Humanitarians Explore Geoinformation. ISPRS Int J Geo-Inf. 2015;4(3):1729–1749. doi:10.3390/ijgi4031729

22. Bigham JP, Jayant C, Ji H, et al. VizWiz: Nearly Real-time Answers to Visual Questions. In: Proceedings of the 23Nd Annual ACM Symposium on User Interface Software and Technology. UIST ’10. New York, NY, USA: ACM; 2010:333–342. doi:10.1145/1866029.1866080

23. Meier P. Human Computation for Disaster Response. In: Michelucci P, ed. Handbook of Human Computation. New York, NY: Springer New York; 2013:95–104. doi:10.1007/978–1–4614–8806–4_11

24. Evidence for interstellar origin of seven dust particles collected by the Stardust spacecraft | Science. https://science.sciencemag.org/content/345/6198/786.full. Accessed May 2, 2019.

25. Cardamone CN, Schawinski K, Sarzi M, et al. Galaxy Zoo Green Peas: Discovery of A Class of Compact Extremely Star-Forming Galaxies. Mon Not R Astron Soc. 2009;399(3):1191–1205. doi:10.1111/j.1365–2966.2009.15383.x

26. Kelling S, Gerbracht J, Fink D, et al. eBird: A Human/Computer Learning Network for Biodiversity Conservation and Research. In: Twenty-Fourth IAAI Conference. ; 2012. https://www.aaai.org/ocs/index.php/IAAI/IAAI-12/paper/view/4880. Accessed May 2, 2019.

27. Michelucci P. A Human/Machine Partnership to Accelerate Biomedical Research. Microsoft Res. https://www.microsoft.com/en-us/research/video/a-human-machine-partnership-to-accelerate-biomedical-research/. Accessed May 3, 2019.

28. Singularthey. In: Wikipedia. ; 2019. https://en.wikipedia.org/w/index.php?title=Singular_they&oldid=918961982. Accessed October 2, 2019.

29. Krause M, Schiöberg D, Smeddinck JD. Mooqita: Empowering Hidden Talents with a Novel Work-Learn Model. In: Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems. CHI EA ’18. New York, NY, USA: ACM; 2018:CS14:1–CS14:10. doi:10.1145/3170427.3174351

30. Avesani P, McPherson B, Hayashi S, et al. The open diffusion data derivatives, brain data upcycling via integrated publishing of derivatives and reproducible open cloud services. Sci Data. 2019;6(1):1–13. doi:10.1038/s41597–019–0073-y

31. Michelucci P, Shanley L, Dickinson J, Hirsh H. A U.S. Research Roadmap for Human Computation. ArXiv150507096 Cs. 2015. doi:10.13140/RG.2.1.4517.2648

32. Little G, Chilton LB, Goldman M, Miller RC. TurKit: Human Computation Algorithms on Mechanical Turk. In: Proceedings of the 23Nd Annual ACM Symposium on User Interface Software and Technology. UIST ’10. New York, NY, USA: ACM; 2010:57–66. doi:10.1145/1866029.1866040

33. Fritz S, See L, Carlson T, et al. Citizen science and the United Nations Sustainable Development Goals. Nat Sustain. 2019;2(10):922–930. doi:10.1038/s41893–019–0390–3