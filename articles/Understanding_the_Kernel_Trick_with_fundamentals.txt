Understanding the Kernel Trick with fundamentals

Here, we learn the fundamentals behind the Kernel Trick. How it works? How the Kernel Trick does the dot product (or similarity) in infinite dimension without increase in computation? Chitta Ranjan · Follow Published in Towards Data Science · 8 min read · May 9, 2019 -- 3 Listen Share

<<Download the free book, Understanding Deep Learning, to learn more>>

What is a Kernel Trick? In spite of its profound impact on the Machine Learning world, little is found that explains the fundamentals behind the Kernel Trick. Here we will take a look at it. By the end of this post, we will realize how simple the underlying concept is. And perhaps, this simplicity makes the Kernel Trick profound.

If you’re reading this, you may already know as a fact that if there’s a dot product in a function we can use the Kernel trick. We typically come across this fact when learning about SVM. An SVM’s objective function is,

In this objective function, we have the dot product 𝐱𝑖ᵀ⋅𝐱𝑗. Due to this dot product, SVM becomes extremely powerful because now we can use the Kernel trick.

What’s this Kernel trick and how does it make SVM powerful?

In the following, we will look at this concept and get to the smallest details to help us understand. This post should clear most of the why Kernel trick works questions, including what does it mean to work in infinite dimension? We’ll start with the most common example and then expand to the general case.

Figure 1: Example of a labeled data inseparable in 2-Dimension is separable in 3-Dimension. Source: [2]

In the above example, the original data is in 2-dimension. Suppose we denote it as, 𝐱={𝑥₁, 𝑥₂}. We can see in Fig.1 (left) that 𝐱 is inseparable in its space. But they are separable in a transformed space (see Fig.1, right) given by,

where, Φ is a transform function from 2-D to 3-D applied on 𝐱. These points can also be separated with Φ(𝐱)→x₁², x₂² transformation, but the one in Eq. 2 above will help explain the use of a higher dimensional space. The √2 is not necessary but will make our further explanations mathematically convenient.

With the Φ in Eq. 2, now we can have a decision boundary in a 3-D space that will look like,

If we were doing a logistic regression, our model would be like Eq. 3. In SVM, a similar decision boundary (a classifier) can be found using the Kernel Trick. For that we need to find the dot products of ⟨Φ(𝐱𝑖),Φ(𝐱𝑗)⟩ (see Eq. 13 in [4]) Let’s do that. I’ll do it like this,

My way:

Instead, my friend Sam, who is smarter, did the following,

What’s different?

Computation operations:

My way: To get to Eq. 4a, I perform 3×2 computations to transform each of 𝐱 i and 𝐱 𝑗 into the 3-D space of Φ. After that we perform a dot product between Φ( 𝐱 𝑖) and Φ( 𝐱 𝑗) which has 3 additional operations (in Eq. 4b). Total: 9 operations.

i and 𝑗 into the 3-D space of Φ. After that we perform a dot product between Φ( 𝑖) and Φ( 𝑗) which has 3 additional operations (in Eq. 4b). operations. Sam’s way: Until Eq. 5b, Sam did 2 computing operations. Finally, in Eq. 5c, he did one more computation. Total: 3 operations. Note that in Eq. 5b we are squaring a scalar, hence just one operation.

Computation space:

My way: I applied the mapping transform function Φ on my data 𝐱 . And then performed my operations in the Φ space (a 3-D space).

. And then performed my operations in the Φ space (a 3-D space). Sam’s way: Sam did not apply the transform function. He stayed in the original 2-D space and arrived at the same result as I had from computations in a 3-D space.

Sam is definitely smarter than me. What he did turns out to be the Kernel trick. But I won’t leave just here. Let’s stew more on it with more examples.

Suppose I wanted a bigger expression for my decision boundary than the one in Eq. 3 (because we expect it to work better), which has both first- and second-order terms as,

Let’s see my way and Sam’s way again,

My way:

Sam’s way:

Comparing my way and Sam’s way again:

I had to explicitly define Φ. While one could argue that Sam had to explicitly know as well to add a 1 to the dot product of 𝐱 𝑖, 𝐱 𝑗, but as we will see soon that Sam’s approach can be easily generalized.

𝑖, 𝑗, but as we will see soon that Sam’s approach can be easily generalized. My way took 16 operations, Sam’s way still took only 3 operations. (Again, note that in Eq. 8a, we are squaring a scalar, hence just one operation.)

operations, Sam’s way still took only operations. (Again, note that in Eq. 8a, we are squaring a scalar, hence just one operation.) Sam again did not leave the original 2-D space to find the same similarity measure (dot product is also a similarity measure) I found in the 5-D space.

Similarly, we can keep going to higher dimensions. If it was to Sam, he can easily find the similarity measure that has third-order terms in a 9-D space by,

Sam’s way:

At this point, I will not even bother to go my way. I hope you got the point why Sam’s way is clearly better than mine. He’s just computing the dot product in the original space and raising the result (a scalar) to a power. And this is exactly same as the dot product in a higher dimensional space.

This is precisely the Kernel trick. Let’s summarize the Kernel trick from Sam’s method.

Kernel: In the above examples, the Kernel functions used by Sam are,

Mapping function: Sam did not need to know a 3-, 5-, or 9- dimensional mapping function, Φ, to get the similarity measure (the dot product) in these high-dimensions.

Magic recap: All Sam needs to do is realize there is some higher dimensional space which can separate the data. Choose a corresponding Kernel and voila! He is now working in the high-dimension while doing computation in the original low dimensional space. And he is yet separating the earlier inseparable data. The Kernel examples Sam used so far are special cases of a Linear Kernel,

How does the Kernel trick work in Infinite dimension?

We know that Kernels can find the similarities in infinite dimensional spaces, and, without doing computation in the infinite space. If you’re still with me so far, now get ready for this crazy one.

Here is the trick behind this magic. I will show the trick with a Gaussian Kernel (also called Radial Basis Function, RBF), and the same logic can be extended to other infinite-dimensional Kernels, such as, Exponential, Laplace, etc.

A Gaussian Kernel is defined as,

For simplicity, suppose 𝜎=1. The Gaussian Kernel in Eq. 10 can then the expanded as,

From Sam’s way of calculations now we know that ⟨𝐱𝑖,𝐱𝑗⟩ⁿ will yield 𝑛-order terms. Since the Gaussian has an infinite series expansion, we get terms of all orders till infinity. And, therefore, a Gaussian Kernel enables us to find similarity in infinite dimension. In this instance too, all the computation we have to do is find the squared Euclidean distance between 𝐱𝑖 and 𝐱𝑗, and find its exponential (the computations happen in the original space).

So Sam has nothing left to worry about while using Kernels?

He does. At the end, he does need

to choose which of the Kernel function to use. [1] has an exhaustive list of Kernels to choose from. And,

tune the hyperparameters of a Kernel. For example, in Gaussian Kernel we need to tune 𝜎. [3] has a list of papers which talk about this.

Do we need to know the appropriate Kernel function first?

Yes, we do need to determine which Kernel function will be appropriate. However, we do not need to know it first. First, we have to realize that a linear decision boundary is not going to work. This is realized when we see a poor model accuracy, and some data visualization can be used (e.g. Figure 1), if possible. Upon realizing that linear boundary is not going to work, we go for a Kernel trick.

Most Kernels will lead to a non-linear, and possibly better, decision boundary. [1] gives an exhaustive list of choices. But there is no direct way to know which Kernel function will be the best choice. Conventional Machine Learning model optimization methods, such as Cross Validation, can be used to find the Kernel function that performs the best.

However, since with Kernel trick there is no additional computation for separating data points in some high-dimension or infinite dimension, people go with the infinite dimension by using the Gaussian (RBF) Kernel. RBF is the most commonly used Kernel.

In short, as a rule of thumb, once you realize linear boundary is not going to work try a non-linear boundary with an RBF Kernel.

Conclusion

In this post, we went through the elementary details of the Kernel Trick. Our objective was to understand the Kernel Trick. We also found answer to how the Kernel Trick finds the dot product (similarity) in infinite dimension without increase in computation.

Please leave a comment if you did not understand a part or any of it.

Disclaimer: This is an edited version of my previous post on Quora for the question, what is the kernel trick and also a Quora blog here.

References

[1] Kernel Functions for Machine Learning Applications

[2] Berkeley CS281B Lecture: The Kernel Trick

[3] Support Vector Machines: Parameters

[4] Support Vector Machines: Lecture (Stanford CS 229)