Cross-validation partitions a dataset, trains and validates models on complementary subsets, and averages prediction errors in such a way that each datapoint is validated once as an out-of-sample prediction. By averaging errors of out-of-sample predictions across the whole dataset, we hope that the cross-validation error acts as a proxy for how a model will perform on unseen test data. If it acts as an accurate proxy, then we can use cross-validation as a guide to adjust a model so as to achieve better performance in the general context.

K-fold cross-validation randomly partitions a dataset into k non-overlapping subsets. Models are trained k times with each subset, in turn, left out and used as out-of-sample test data. For smaller values of k, k-fold can be less expensive to compute, but it has the unattractive property that it introduces randomness into the cross-validation result.

Leave-One-Out Cross-Validation (LOOCV) is a form of k-fold where k is equal to the size of the dataset. In contrast to regular k-fold, there’s no randomness in the LOOCV result since each datapoint is always partitioned into its own subset. While it can be expensive in the general case, requiring a model to be fit for every point in the dataset, for certain models, such as ridge regression, LOOCV can be computed efficiently and optimized to find the best model parameters¹.

If feasible, LOOCV should be preferred to other forms of k-fold; but it still has some undesirable properties. Let X and y represent the dataset’s n x p feature matrix and target values. If Q is an n x n orthogonal matrix, put

Suppose we fit a ridge regression model to the dataset. If b represents the model’s regressors, then b has the same error on the rotated regression problem as it does on the original

And so, there’s an equivalence between the regression problems X′, y′ and X, y, yet the LOOCV is not invariant to rotations².

Furthermore, LOOCV performs especially poorly when variance for a feature is concentrated about only a few data points³. In the most extreme case, suppose

when the ith data vectors is excluded (i ≤ p), there’s no variance in the remaining dataset to fit the ith regressor coefficient so that the LOOCV is

and independent of the regularization matrix.

Generalized Cross-Validation

The idea behind Generalized Cross-Validation (GCV) is to modify LOOCV so that it’s computed on a rotation of the original regression problem where the rotation matrix has been chosen in a very particular way so that 1) GCV is invariant to rotations of the original regression problem and 2) variance of feature vectors is “spread out” evenly across the transformed matrix X′⁴.

Let U, S, and V represent the singular value decomposition of X

where S is a rectangular diagonal matrix and U and V are orthogonal matrices. Let W represent a unitary matrix (GCV uses complex rotations). Put

Let X′, y′ represent the rotated regression problem for this matrix. Then

Now, set

The columns of W are the common eigenvectors of n x n circulant matrices so that for any diagonal matrix D

is a circulant matrix with the form

Thus,

is a circulant matrix with constants down the diagonal. Additionally, for any row of X′, the covariance of it with the other rows of X′ is represented by the same shifted vector⁵.

Computing the GCV

Let Γ denote the regularization matrix of ridge regression. Then the LOOCV of the regression problem X, y can be efficiently computed with the equation⁶

where

The same equation can be used on the rotated regression problem X′, y′ to compute the GCV, only we need to account for the fact that X′, y′ are complex valued. The GCV equation, then, becomes

where

(Note: A and b̂ are invariant to rotations so they remain unchanged)

We can also apply the same modifications to LOOCV’s derivative equations to compute the GCV derivatives, making it possible to apply an optimizer to minimize GCV with respect to Γ.

A Special Case

Suppose we parameterize our regularization matrix Γ with a single variable

Then h′ is the diagonal of

and

Now, because

is a diagonal matrix, it follows that

is a circulant matrix. Therefore, h′ is a vector of constant values equal to

or, since the trace is invariant to rotations,

Substituting this back into the LOOCV equation and making use of the property that rotated regression problems have the same sum of squared errors, the GCV for the single-variable regularization is

This is the most common form that GCV is presented in. But caution: For the general case where Γ is parameterized by multiple variables, this simplified equation won’t evaluate to the proper value⁷.

A Real-World Problem

We’ll compare the performance of different approaches to setting regularization parameters on an example problem. The data is taken from Motor Trend 1974 US magazine and the task is to predict the fuel efficiency of vehicles from other given attributes. We’ll compare the following models on a Leave-One-Out Cross-Validation of the data:

LS: Least squares regression

Least squares regression RR-LOOCV-1: Ridge regression with a single regularization parameter set via a second order optimizer to minimize the LOOCV error on the training data.

Ridge regression with a single regularization parameter set via a second order optimizer to minimize the LOOCV error on the training data. RR-LOOCV-p: The same as RR-LOOCV-1 but with separate regularization parameters for each principal component.

The same as RR-LOOCV-1 but with separate regularization parameters for each principal component. RR-GCV-1: Ridge regression with a single regularization parameter set via a second order optimizer to minimize the GCV error on the training data.

Ridge regression with a single regularization parameter set via a second order optimizer to minimize the GCV error on the training data. RR-GCV-p: The same as RR-GCV-1 but with separate regularization parameters for each principal component.

Note: There are two levels of cross-validation going on in this experiment. At the highest level we form a pair of training data and test data for each data point to train the models on; and at the lower level, the ridge regression models perform another cross-validation on the training data to set their regularization parameters.

The table below shows the Root-Mean-Squared-Error (RMSE) of the LOOCV for each model ordered from best to worse⁸.

Conclusion

Ridge regression is one of the most commonly used models in machine learning. With the right regularization matrix, it frequently outperforms least squares. Using an optimizer, we can set regularization parameters to maximize performance on certain forms of cross-validation; but to get good out-of-sample performance, we need to use cross-validations that act as accurate proxies for out-of-sample prediction error. We saw how LOOCV can be problematic in certain cases and has the undesirable property that it’s sensitive to rotations. We introduced GCV as an alternative that’s invariant to rotations and deals with the problem cases of LOOCV by spreading out the variance of the feature matrix. We compared the performance of different ridge regression models on an example problem and found that GCV gave us the best results.

For finding regularization parameters that optimize either the leave-one-out cross-validation or generalized cross-validation, check out buildingblock.ai.