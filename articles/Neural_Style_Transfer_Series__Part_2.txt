Neural Style Transfer Series : Part 2

This article follows from what we discussed in the first article. While we spoke about the intuition and the theory of how Neural Style Transfer works, we will now move onto implementing the original paper. If this is the first article you’ve read of this series, I would implore you to read the previous article given below. We’ve explained how neural style works in depth, this post is majorly about implementing the same. Understanding the theoretical nuances would help you understand the implementation as well.

Neural Style Transfer Tutorial — Part 1: Theory of Neural Style Transfer

Before explaining the entire code, I’ll go through the frameworks we implement them in first. Then going onto the implementation.

The codes of the implementation can be found here. [Pytorch, Tensorflow]

Note: You must have noticed in that instead of modularizing the code into set of loss, network creation and training file. We just have created two files named train_Pytorch.py and train_TensorFlow.py, we might change this workflow in future codes.

Lets try to now take a holistic view of Deep Learning frameworks before we dive into the code implementation. Since we will be using Python, I will stick to the major frameworks like Pytorch and Tensorflow. This will be a very over the top view of the libraries, We could write another article purely on the different frameworks available and the pros & cons there on. There are several other frameworks you might want to look at which include,but not limited to, Dynet, Chainer, and Julia language’s recently released framework called flux. Another thing to observe is although these are frameworks to build models in, there are even more higher level abstractions available like fastai, spacy and keras.

A computational graph

Pytorch was created by Facebook borrowing from torch in Lua and building on caffe2, while Tensorflow was created by google. Tensorflow remains to be the current choice of framework for most people,but Pytorch is quickly picking up speed and is said to be better in some aspects. Both Pytorch and Tensorflow create something called as Computational Graphs and the major difference in the two is how these two create these computational graphs. Pytorch creates dynamic graphs while Tensorflow creates static graphs. This means, that in Pytorch you can manipulate your graph on the go while in Tensorflow you have to define it entirely beforehand. Tensorflow2.0 is trying to incorporate dynamic graphs into the language in the form of eager execution. Pytorch is more pythonic and hence intuitive to learn while Tensorflow has many good features like Tensorboard ( a visualization platform for your models) and also is easier to launch into production.

Back to our article now, In this code explanation i hope to give some sort of overview about what i told about computational graphs and see some differences and similarities in the frameworks. I would like to implore you to understand the code first, run it and then implement it without seeing while referring back to the code if there are any mistakes. I wont be covering the utilities written for the program since they are just data loading functions and are mostly trivial in nature.

The first thing to pay heed to the “content_layers” and “style_layers”, you can experiment with these in any way you want trying to get different representations. As explained in the original article, these two arrays are for mentioning the layers you will get your feature representations from.

In pyTorch, the way you create a network is overloading nn.Module. there are two parts to it always, first is it __init__ method which initializes all yours layers, and then there is the forward method, the forward method is used to actually pass your data through the network and get the outputs. In this code, I chose to write my own VGG19 network, while there are ways in which you could just import the inbuilt model and load weights. Some things i chose to do in a particular way are keeping the average pooling layers, you can use max pooling layers as well.

If you are wondering what the use of the transform variable is, it fundamentally exists so as to transform your input image, no matter what original resolution to convert it into something that can be fed into the VGG network. We normalize the image so as to subract the imagenet mean, more on that latter.

One thing to pay heed to within frameworks and in general while replicating papers or making your models is, usually we have our data on the cpu, but the training of NN models for efficiency is trained on GPUs, hence as you can see we use the .cuda() function to transfer objects like our style and content image onto the gpu. We load the pretrained VGG weights onto the model, these pretrained weights are from a VGG trained on the Imagenet. Extension of saved weight files is .pth and one thing to pay heed to is when you write your network and save weights and try to run it on mine, it might not work although we both are training a vgg network, details like different layer names can or choice of layers like max instead of average pooling may throw an error while loading weights.

Coming onto the Loss function, First we have the gram matrix class. As explained in the previous article we use the Gram Matrix to extract feature layers, the .bmm() stands for batch matrix multiplication. In batch matrix multiplication, the last 2 dimension are used for the matrix multiplication while the rest of the dimension are treated as batches. I wrote an example in the comments after the variable to give an idea of matrix dimensions. The style loss is computed by first passing the layer to get its gram matrix representation and then taking a mean squared error between the representation and the target. Going through the rest step by step, we detach the layers from the VGG network and add them to the respective target layers. Then we go onto define the losses, targets and the loss layers. Finally we have the style weight and content weight, and you can experiment with this anyway you want, The paper suggests that 1000 and 5 is a good way to start, and the higher your content weight the more you are preserving your original image and the higher your style weight, you will observe that the style becomes more and more prominent while the content takes a back seat.

Finally the training loop, the training loop is a crucial part of all kinds of deep learning model building. To outline a general framework, we do three things in any training loop, We decide on the optimizer, compute the loss and backprop after that. In the case above, we use the LBFGS optimizer. We can see that we call the .zero_grad() function that is because, pytorch by default accumulates gradients, so we need to set the gradients to zero each time before computing them. The total loss is then accumulated by going through the all the style and content layers, after which the .backward() function is used for computing the gradients and .step() is used for updating the weights. The reason we have a the loss function being passed to step() is because, in the case of LBFGS, the loss needs to be computed multiple times.

Now, Instead of going through the Tensorflow code entirely, I shall try to point out the major difference since the rest of the code is very similar to that of pyTorch. One of the fundamental differences is in the run_style_transfer() we can observe that a session variable is being created, this brings us to the point the fact that, since tensorflow creates static graphs we need to first define a session after which the model is trained within this session. The Adam optimizer is used to train the model instead of LBGFS. In the training loop, the session is ran by passing the optimizer to the same. and the loss is only updated if the current loss is better (i.e lesser) than the previous loss. The VGG model with preloaded weights is imported from keras, instead of being written from scratch. While training the model, it might take a while since tensorflow will download the weights if they aren’t available locally.

So there you have it, You have successfully taken a look at the code for neural style transfer. The next thing to do is, to run the code and see the outputs for yourself. We will continue the series with Fast Neural Style transfer, to give a short introduction, when you train the network something you might observe is for each style you need to retrain the entire network repeatedly. A better approach to solving the problem might be learning the style weights saving them and then loading whatever content image you want whenever you want. This might sound confusing, so look forward to the indepth article.

Thanks a lot for waiting for this article.

Thanks to Vamshik Shetty for co-authoring this series with me.

You can find the code at this repository, where we will have codes for future articles in the series as well.