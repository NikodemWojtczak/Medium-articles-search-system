As promised, this is the third and last post on Principal Component Analysis â€” Math and Intuition. We had a brief introduction to PCA in Post 1 with a real world example and grasped the intuition. We learned some of the most important concepts in Linear Algebra relevant to PCA and perhaps various other data science applications, in Post 2. With all the hard work done, it is now time to use our solid mathematical framework and connect the dots to really understanding how and why PCA works.

PCA is a dimensionality reduction technique. It simplifies a complex dataset making it computationally amenable. There is no feature elimination involved; rather PCA works by extracting the significant bits of information from all features in the original dataset and creates lesser numbers of new features. In simple words, if you have a data set with n-dimensions (n number of features), applying PCA reduces it to a k-dimensional feature space where k < n.

Let us use an example here. You have a dataset on hotel rooms listing the room areas and respective prices.

The 2-dimensional feature space can be plot as shown below.

The goal is to reduce the 2 dimensions that are represented as x-axis and y-axis respectively, into one.

Note that I have chosen a 2D dataset because it is easy to visualise. In a real world scenario, you may have 1000s or more features and therefore the need for dimensionality reduction. Also, please do not confuse the above plot with linear regression, because although it looks similar it is an entirely different concept. There is no prediction going on here.

Coming back to PCA, we would like to simplify our original dataset which is described on a 2D basis. Recall the concept of basis change that we encountered in Post 2. The question we need to ask here is,