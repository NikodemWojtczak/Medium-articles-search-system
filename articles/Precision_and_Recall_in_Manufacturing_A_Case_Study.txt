Recall: Conditioned on Groundtruth Labels

Remember that at the very beginning of the experiment, we have selected 1000 DUTs, of which 900 are good (pass) and 100 are bad (fail). We can calculate recall for each label e.g. one recall for pass and one recall for fail.

Recall for pass answers this question: of the 900 good / pass DUTs that I have tested, how many are accurately classified by the algorithm? In our report example, we saw that recall for pass is 0.67 (or 67%). This means that 67% of the 900 pass DUTs are correctly classified as pass too.

Visualizing Recall with a Tree

The same logic could be explained by traversing the above decision tree. At the root we have a box with 1000 DUTs. The box is then split into 2 child nodes (1 box for 100 bad/fail DUTs and another one for 900 pass DUTs). This splitting corresponds to the groundtruth label. We can further split the 900 pass DUTs into two boxes, one for those that the algorithm mark as fails (300 DUTs), and another for those deemed as passes (600 DUTs). We see that the algorithm is only 600 / 900 ~= 0.67 or 67% accurate for the subset of passed DUTs.

Recall for fail looks at the other side of the tree, answering this question: of the 100 fail DUTs, how many did the algorithm manage to capture? In this case only 80 / 100 = 0.8 or 80% of them are correctly classified.

We can use recalls to quantify to what degree have we met the two product requirements listed before.

Minimizing risk of escapees is equivalent to maximizing recall for fail. Maximizing saving is equivalent to maximizing recall for pass.

Precision: Conditioned on Algorithm Labels

Now assume that we have deployed the algorithm in production. Most likely, our customers (OEM / CM) would start to ask if they can trust this new system. Precision could be used to answer such questions.

Visualizing Precision with Recall

Precision for pass answers this question: if the algorithm passes a DUT, how much can we trust that decision? In other words, from a frequentist’ point of view, if the algorithm passes 620 DUTs, how many of them are truly good? In our case, 600/620 ~= 0.97 or 97% of the time the judgement is correct.

Again, we can arrive to the same number by traversing the tree. The only difference is that we split the root node (1000 DUTs) based on the algorithms’ judgements instead of groundtruth label.

By using the same method, we get a precision for fail of 80 / 380 ~= 21%. This means that when the algorithm says that a DUT is bad, only 21% of the time it is correct.

This tells us that this algorithm is useful in identifying good DUTs (passes) but not bad ones (failures).

Conclusion

To conclude, we need to improve the algorithm in the following manner:

improve recall for fail to prevent escapees improve recall for pass to increase cost savings, thereby shortening the investment’s payback period (if we are going to sell this product to a customer, we better make sure that the Return of Investment calculation is sensible). maintain/improve precision for pass so that we can be sure that the model is very confident when it passes a DUT

This ends our discussion on Precision and Recall. Bear in mind that these 4 numbers are not independent i.e. increasing one number will decrease the others (look at the appendix to see how precision is related to recall, by virtue of Bayes’ rule). So simply biasing the decision boundary of an algorithm will not work. One could try to play around with the algorithm’s hyperparameter(s) or try a completely different architecture. This is best left for another discussion topic.

So, I hope that this little case study gives a better insight into what each metric conveys, through the lens of manufacturing analytics.

Appendix: Linking Precision and Recall with Bayes’ Rule

Definition of recall and precision with conditional probabilities

Applying Bayes’ Rule to find relationship between recall and precision for a particular class (fail)

Substitute values from our case study to prove the equality sign.