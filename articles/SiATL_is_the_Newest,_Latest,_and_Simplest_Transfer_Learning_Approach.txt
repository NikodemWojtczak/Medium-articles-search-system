Photo by Allure Graphic Design

This research summary is just one of many that are distributed weekly on the AI scholar newsletter. To start receiving the weekly newsletter, sign up here.

Many conventional transfer learning methods utilize pre-trained Language Models (LMs) which have become very popular, and boast of capability to translate contextual information, high-level modeling syntax and semantics language features generating high-end results in many tasks such as object recognition, machine translation, text classification and more.

However, existing LMs face downsides including high computational costs and the demand for task-specific architectures. Additionally, most require pre-training and fine-tuning of the task at hand. Well, it’s a different story now as researchers have recently released a new single-step transfer learning method that does not require pre-training or fine-tuning. Furthermore, the new method outperforms state-of-the-art transfer learning approaches including ULMFiT in all tasks.

Single-step Auxiliary loss Transfer Learning (SiATL)

SiATL is a simple yet efficient transfer learning method that addresses the problem of catastrophic forgetting. SiATL combines a task-specific function with an auxiliary LM loss that is adjusted during the training process and is based on pre-training a LM and reassigning its weights to a classifier. This enables it to preserve language regularities captured by language models, at the same time facilitating sufficient adaptation for solving tasks.

As mentioned in the introduction, SiATL does not require pre-training or fine-tuning which make it unbelievably simple to use. The new model has already been tested on a variety of challenging text classification tasks and yielded competitive results indicating its superiority over conventional transfer learning methods.

Potential Uses and Effects

As you already know, model training doesn’t have to start from zero and doesn’t have to be wild. SiATL can be used to harness a model that is trained for one task and apply it to another domain in a straightforward fashion. It can also come in handy in situations of data insufficiency.

For data scientists and developers, SiATL offers a simple, cheap and practical approach to accelerate model training with transfer learning capabilities to achieve enhanced performance for a wide variety of applications such as speech recognition, intelligent video analytics, question answering systems, medical imaging, and more.

Thanks for reading. Please comment, share and don’t forget to subscribe! Also, follow me on Twitter and LinkedIn. Cheers!