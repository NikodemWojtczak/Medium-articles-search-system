Reinforcement Learning (RL) can be a daunting space to those new to the field due to terminology and complex mathematics formula. However, the principles underlying it are more intuitive than first imagined. Let’s imagine RL in the form of a new, yet unreleased, the game of Zelda, which takes place far in the future, the year 2119.

Link’s mission in 2119 is to save humanity from Rittenhouse, a secret agency based in 2019 which has a strong AI and a time machine. Because Rittenhouse is evil, they devise a plan to destroy humanity in 2119 and send their agents forward in time to complete the task.

As the game begins, Link drops down onto a mostly desolate island where he must find the first sign of human civilization and warn them of the impending arrival of Rittenhouse’s agents. As he lands on the top of a tower, an old wise man appears and gives him a paraglider as a gift. Link’s mission is now to paraglide and finds a town with human civilization to warn them of Rittenhouse.

Link is a cyber-alien that can install any software in his mind. He does not know how to paraglide. Before Ritten-House’s agents get there, your goal is to write a program to teach Link paragliding and all the tricks he reaches human-civilization as fast as before possible and avoids Ritten-House as much as possible. Your plan is to send this simulation to Link before Rittenhouse’s agent come and stop him, so he would know to protect himself.

Your plan is to send you this program through light-waves in the future.

In order to design this program, you need to know something called Reinforcement Learning. In Reinforcement Learning, there is an agent that is interacting with an environment.

We model the agent’s interactions with an environment based on the mathematical framework called the Markov Decision Process (MDP). Each agent starts in the state “X” and at each time step takes an action “A”, gets the reward “R”, and land in the next state “Xt+1,”, and this cycle repeats until the agent reaches the target state “X`”.

Now, in the simulation that you are making, Link as the paraglider is an agent in the initial state X in the sky. At each time step Link takes an action from sets of possible actions. Here steering his kite to the left or right are considered as his possible actions, and he goes to a new state or new place in sky. His goal is to land in the target state X`, which is a few states in the future and where he can find human civilization.

At each time step, based on each action he takes, he will be in a different position in the space. For example, if he steers his kite to right, he would be in a different place than when he steers his kite to left.

However, not all of these possible actions are identically favorable. His goal is to find a sequence of optimal actions in order to get to his goal.

This means you want Link to take a route that is most efficient to get to humans. In order to do this, it is better if we use the model-based method. In the model-based approach, in order to take optimal actions, Link also needs to predict the ideal future states, so can choose the best route to get there.

When Link is paragliding, he wants to know how he should steer his kite in order to not fall (i.e., find an optimal action). But, also he wants to avoid the enemy and land in an optimal location like where the town is (i.e., predict the next optimal state).

The prediction of future states and where the right place is to land will, in turn, affect how Link steers his kite in the present moment. You want to find optimal actions based on his prediction of future states.

Link might predict the enemy will be at certain spots and to avoid that enemy, he should steer his kite in other directions.

The prediction of the future state accounts for how the environment is changing. This change of the environment with respect to the current state and action is described as a function, and we refer to this function as the model.

Your goal is to teach Link to lean this model.

The input of the model is the current state x and action u , and the goal is to predict future state x t+1. We can write this as: x t+1 =f(xt, ut)

We call the process of selecting the sequence of actions until the episode ends as the policy. Intuitively, policy means how Link chooses the best way to steer his kite at each time step until he finally reaches to the town to save humans. We can describe the policy with the following notation: ut= лθ(xt). This means at every state xt , policy лθ tell Link what the optimal action ut is.

Cost or reward function

We use the cost function or reward function to find an optimal policy or in another word, optimal actions over the trajectory. Depending on the setting, we use either cost or reward function.

Note that reward is just negative of the cost function. We are trying to minimize cost or maximizing reward. In this setting, we use a cost-function c(xt,ut).

How is finding optimal action related to the prediction of the future state? To put it another way, how is optimal policy related to the model?

Before answering this question, I want you to imagine a world in which the only one of these functions works.

What do you think it would happen?

The first scenario, the model is capable of predicting the next state but incapable of taking good actions.

Here green is the optimal future state and the model has the prediction of both green and red.

This means, even if Link is capable of predicting future state, he knows where is the enemy is, but he is not capable of taking good actions based on his knowledge of the future state. He does not know how to steer his paraglide and falls off or wear off towards Rittenhouse because he does not know how to steer his kite.

Now, imagine the opposite scenario, Link is capable of taking good actions and is an expert at paragliding but he is incapable of predicting where his action will take him. He knows how to steer his kite but he does not know where to go. This might also put him in a bad spot.

This means he does not have any prediction about the future. He might spend lots of time taking actions based on his current circumstance and need to encounter with the enemy and not based on the prediction of the future state, also it might search randomly through the whole area until he finds where is the town with a human. This is similar to lots of model-free environments. The prediction makes Link a wise person, he can predict the consequence of his actions and make decisions based on those predictions. This is what rational human would do too.

Again with the paragliding example, Link is capable of steering his kite at a given moment to not fall but could not remember where the enemy was and as a result, he constantly encountering them and as the result not having a clear idea about how the environment is changing, he can not know know exactly which direction would take him to the town.

Link not only needs to know how to steer his kite but also predict where he should go and steer his kite based on his prediction. Now, you might have a better intuitive sense of why these two functions are related.

Let’s elaborate more on this mathematically.

Here is the formula for the total loss, which we are trying to minimize. Note that this loss consists of two functions.

1. cost function: c(xt,ut).

2., our model ~ transition function: f(xt-1, ut-1)

Instead of xt in c(xt,ut), we can write x as the transition function of its previous state and its action. c(f(xt-1, ut-1),ut). We can rewrite the above formula as the following:

Since we have a sequence of steps, we are trying to minimize the cost at each step. Therefore the total loss is the sum of cost at each time step.

This intuitively means that, Link wants to take a good action at any moment but also towards the whole territory until he reaches humans.

The promise of the model is based around the relationship between how we find the optimal policy лθ based on transition function f(xt, ut) for predicting the future state x t+1.

So, now we want to optimize the total loss function for the entire sequence that includes two functions: cost function at each time step, and a transition function. We want to find an optimization technique that minimizes the loss for these two functions. We can use either linear or nonlinear optimization. We can use the Neural Network to minimize the total cost function.

If our environment is differentiable, we can optimize via back-propagation. in order to calculate the total loss, we require the following derivatives:

This part is what we mean by model. We want to understand the model with respect to state and action:

This part is our cost. We want to understand cost with respect to state and action:

World-Models

So far you learned about model-based method, We can use one of the model-based methods, World-Model, as the core mechanism behind software for the simulation for Link.

Here, the MD-RNN is similar to transition function x t+1 =f(xt, ut) for predicting the future state. However, it is only slightly different from what we learned so for. The MD-RNN adds its own hidden state to predict future state and ht+1.

As you might remember, earlier we discussed depending on the setting, we use either cost or reward function. World-Models uses a reward instead of the cost function. Here, the controller network acts as the reward function and it aims to find the policy that maximizing cumulative reward through the entire roll-out. The inputs of the controller are zt and ht and the output is an optimal action at. ht is an additional variable that controller network uses to predict optimal action. I encourage you reading world-model by yourself.

Thanks for saving Link ❤