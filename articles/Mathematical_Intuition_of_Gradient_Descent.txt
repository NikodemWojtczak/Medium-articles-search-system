“Mathematical reasoning may be regarded rather schematically as the exercise of a combination of two facilities, which we may call intuition and ingenuity. ”

This one is for the curious ones. Everyone knows about the gradient descent and the formula of how to apply it to the cost function:

for j=0 to n

But in this article, we will focus on the highlighted part of the above formula to understand the intuition behind gradient descent: Why subtraction of this factor from every component of theta guides us eventually towards the optimum theta? So, basically understand “How gradient descent works?”.

Guiding Steps :

Gradient Vector What gradient vector represents? Applying the gradient vector to cost function

1. Gradient Vector :

Let’s say we have a cost function J which depends on two independent variables θ0 and θ1. Then the gradient vector of J:

∇G=< ∂J ∕ ∂θ0 , ∂J / ∂θ1 >

where,

∂J ∕ ∂θ0= derivative of function J w.r.t θ0 keeping θ1 constant.

∂J ∕ ∂θ1 = derivative of function J w.r.t θ1 keeping θ0 constant.

Let’s break this down. Assume that the plot of function J against θ0 and θ1 looks like this:

If the bowl shaped 3-D graph is sliced by a plane parallel to zθ0 plane at a particular value of θ1, it will result in a 2-D parabola like this:

Partial derivative ∂J ∕ ∂θ0 represents the slope of tangent to the 2-D parabola for a particular θ1 and vice-versa for ∂J ∕ ∂θ1. Gradient vector at a particular (θ0,θ1) is a vector with these slopes as its components.

2. What does gradient vector represent?

Before we get to the answer, we need to understand the concept of contour plots. If the above graph of J is sliced by a plane parallel to θ0θ1 plane at a particular z=z1 level, it will result into a 2-D circle such that for every point on the circle function J will give a constant z1 value. Many of such plots at different z levels gives the contour plot for a multidimensional curve.

slicing at z=25

Contour Plot of J(θ0 ,θ1)

In contour plots, gradient vectors are perpendicular to the level surfaces and always point towards higher level i.e. point to the level surface with higher value of J. There is a logical reason for this behavior but that would require a whole another article of multi-variable calculus concepts.

3. Applying the gradient vector to cost function

Since we need to find such values of θ0 and θ1 which minimizes the value of J, we move in the direction opposite to gradient vector by distance proportional to the magnitude of gradient vector (since it is a perpendicular distance to next level surface, it is the shortest distance to next level surface).

So, let’s say we are at a point A with co-ordinates θ0=a and θ1=b at level surface z=4. We want to move to next lower level surface say z=3. We have the gradient vector with direction and magnitude to guide us to a new point (θ0',θ1') at this lower level surface z=3. Hence by simple vector mathematics,

θ0' = a + (-∂J ∕ ∂θ0) = a - ∂J ∕ ∂θ0

θ1' = b + (-∂J ∕ ∂θ1) = b - ∂J ∕ ∂θ1

This is the gradient descent formula that we started with. This process is repeated until convergence i.e. until we reach the lowest level surface. Alpha is the learning rate multiplied to the gradient vector to decrease the number of steps needed to reach the lowest level surface.

As you can see , the final convergence point depends a lot on the initial point.

I hope you got the intuition. It seems a bit overwhelming when you are bombarded with one formula after another. But, there is a mathematically derived logic behind everything. You just need to be curious.