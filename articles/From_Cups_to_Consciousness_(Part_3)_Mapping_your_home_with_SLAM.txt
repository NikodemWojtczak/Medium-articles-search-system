Introduction

“All you need is a plan, the road map, and the courage to press on to your destination” — Earl Nightingale

In the previous part of this series we talked about how the road to AGI could be divided into perception and control. Within control, navigation and grasping are a crucial part of the roadmap for building a general robot for household tasks. But as the Cheshire Cat said to Alice in Wonderland,

“if you don’t know where you want to go, does it really matter which road you take?”

This is the first of two parts where we talk about how our seemingly kidnapped robots find their bearings. What do we mean? Well, from their point of view, every house is a mysterious place they’ve been thrown into or suddenly awoken in. As one can guess, this process of discovery is almost exclusively in the domain of perception and is where, logically, we should start. One must first perceive before acting accordingly, including understanding your surroundings, mapping your environment, i.e. understanding what is where, where is “occupied” and where one can move to unimpeded.

Consider this step one in building a robot that can perform general tasks. Forget about having robots do singular tasks, e.g. a roomba to vacuum your floor, alexa to tell you the time, and a toast-robot. To perform general tasks, we need our robot to be aware of its environment, i.e. to recognise what is an obstacle, as well as where things are, and use that information to navigate and complete tasks.

Seems simple enough.

Mapping in simulation

In part 1, we introduced our philosophical motivations and desires for cup-picking and covered a set of simulated 3D house environments. In part 2, we covered our wrapper for the ai2thor house environment, some reinforcement learning (RL) experiments and two specific physics simulations called PyBullet and Gazebo, enabling us to put the robot Vector onto the moon.

Knowing that mapping was our next milestone, naturally we began by implementing initial mapping within the PyBullet simulation. Since PyBullet contains a depth and colour RGB image, as well as the pose of the camera (the position and orientation), we can use this depth information from the virtual camera to convert each 2D pixel into its 3D coordinates in space, creating what is known as a point cloud.

Using that point cloud we can create a grid data structure that defines where the robot can move and where the obstacles surrounding our robot are, i.e. an occupancy grid. This depth image, to point cloud, to occupancy grid pipeline is a very common operation within computer vision and robotics.

Extending an official PyBullet code example to create a point cloud from depth information, we were able to take all points above the floor and below the roof in our simulated kitchen. From this, we create an occupancy grid with each grid cell showing the likely traversability of an area of 10cm2. This way, each area in the grid is assigned a probability of being occupied. This is, in fact, the most common format and approach for robot navigation.

Note: Our own simulated PyBullet kitchen and living room with dynamic objects (e.g. beer bottle, bowl, cup, etc) and occupancy grid mapping with exact pose information. Within this GIF, we also present a semantic object occupancy grid map, mapping aggregation and a simulated robot with 2 wheels and a caster wheel moving around the living room with depth and segmented views displayed from the viewpoint of the robot. The semantic map describes which grid cells are occupied by specific objects. The aggregation combines the information from 3 viewpoints (local maps) into one merged map.

So what did we learn?

This taught us a lot about occupancy grids, sensor models and aggregation of multiple viewpoints into one big occupancy grid. But everything mentioned above was found using the exact pose (position and orientation) of the camera provided by PyBullet. We knew that next it was necessary to find a way to localise the agent’s pose in the world, and specifically, in the real world.

As we mentioned in our last blog, we decided to move from simulation towards getting algorithms working on real robots first. One simple reason for this; there are many more problems that occur on real cameras compared to virtual scenes.

In the future, to scale the number of tasks, cups, kettles and environments that our robots can tackle we will most certainly need simulation. But for right now, real-world cup-picking trumps matrix-style every time. As you can see, the road to AGI is long, winding and treacherous. It all begins with first finding yourself.

Simultaneous localisation and mapping (SLAM)

“Not until we are lost do we begin to find ourselves” — Henry David Thoreau

It is relatively easy to create a map using known localisation (the mapping problem), and it’s also relatively easy to localise within a known map (the localisation problem). Many solutions and variations for both exist, but what if we had neither the robot’s pose, nor a map? That is, how do we create a map of the environment when we don’t know where we are?

This, relatively speaking, isn’t so easy.

Return to our kidnapped robot, who has just been placed in an unknown room. Upon opening its eyes how could it understand where it was? Most people will naturally use reference landmarks around them, i.e. a bed, the corner of a TV, an open door, to roughly locate themselves in an environment. Mapping and localisation are inextricably linked, they need each other like toast needs butter and it’s essentially a chicken and egg problem.

In order to pick up cups at breakneck speeds and to operate in homes the robot has never seen before, it should be able to both localise itself within the environment and at the same time map it, i.e. find the 3D positions of parts of the world. This is why in our journey we inevitably arrived at the field of Simultaneous Localisation and Mapping (SLAM).

SLAM research can be categorised by the sensor suite used to approach the problem (more on this later). For example, you could use a camera (or a set of cameras) to find the pose of the robot by landmarks detected in the environment. But you may not trust the weaknesses of these cameras when the lights are off, or the visual scene might be confusing, in which case you could use a radar instead. Or, as with most companies building autonomous cars, you could use an expensive LiDAR if you need a more reliable sensor that can find objects hundreds of meters away from you in high detail.

There is no single sensor to solve all of our problems. We will need to combine many sensors together (i.e. sensor fusion) in our quest to build robots that can understand the world in a way that we can relate to while at the same time, keeping the costs low. To do this we decided to focus our efforts on visual algorithms. That is, camera based SLAM, or as named in the literature, Visual SLAM (V-SLAM).

Over 2 years ago, we mentioned SLAM in our Computer Vision report and now we’re delighted to have the chance to really dive deep into this fascinating technology. If you are looking for a good review, or details on the state of the art, we recommend:

The architecture of a typical SLAM system

A typical Visual SLAM algorithm has two main components that can be easily parallelised, meaning that they can be run independently even though both parts are interconnected. Following the literature, we will refer to these as the “front-end” and “back-end”.

The Front-End

The front-end abstracts sensor data into models that are amenable for estimation. It’s in charge of preprocessing the input, as well as the detection and tracking of relevant landmarks to allow for an estimation of the sequence of poses, from which we observed them.

In the case of VSLAM the algorithm that accomplishes this is visual odometry (VO), which essentially calculates the relative pose (the transformation) between two frames according to salient visual features.

The most common method is to extract keypoints (e.g. using SIFT, ORB) from a frame and then match these same keypoints in the next frame or track them with optical flow. Moreover, while matching/tracking those observed keypoints from different positions, the nature of these algorithms may incur errors of wrong data association, so usually another algorithm is applied afterwards to remove possible outliers that potentially propagate additional error to our estimations, e.g. random sample consensus (RANSAC).

Another important consideration is that if we keep tracking every salient point of every image we would consume an immense amount of memory including lots of redundant information. For efficiency reasons, typically only a subset of all images observed, these “key-frames” are selected by a selection algorithm while matching and tracking the observed features only between keyframes. A simple keyframe selection algorithm would be to take every 5th or 10th frame, but other methods involve only adding a keyframe if the image has changed enough from the previous one (e.g. measuring parallax changed or the distance the keypoints moved).

Source: ORB-SLAM in the KITTI dataset (Sequence 00) YouTube link Note: The top view is from the KITTI dataset with ORB features overlayed from ORB-SLAM2. On the bottom we see top-down trajectory in green but only the blue frustums are keyframes. In red are the features which are tracked.

Note: By adapting a simple VO example in Python (from here) and by using every 10th frame as a keyframe we were able to get reasonably good results on a sequence in one of our houses. The top down trajectory is shown on the right on the black background as we go around a table 3 times with the camera attached to a chair. But this example was carefully cherry-picked; other sequences didn’t show as reliable results on the trajectory estimation. In general, the algorithm suffers from severe drift problems, e.g. by using only every 5th frame as a keyframe this caused caused the drift errors to accumulate much faster. This per keyframe error can be blamed on many factors e.g. calibration, initialisation or monocular scale issues.

Additionally, we could calculate the transformation between two consecutive point clouds (e.g. point cloud registration with the Iterative Closest Point (ICP) algorithm) to localise the agent’s trajectory.

Note: Pointcloud registration is about finding the transformation from one pointcloud to the other so that they overlap and align together in a way that makes most sense. The GIF shows the resulting point cloud (yellow + blue) after the registration step is performed, i.e. one of the point clouds (yellow) is transformed to be aligned to the other one (blue: which is the reference to align to), optimising the transformation so that overlaying this transformed point cloud onto the reference represents a single scene as consistently as possible. This obviously implies some overlap and consistency between both pointclouds. This relative transformation between two frames/pointclouds can be the basis of RGB-D or LiDAR odometry. For reference, a famous algorithm for computing this registration is called Iterative Closest Point (ICP).

Also, we can use a lower dimensional representation of features found in the image instead of their full description, for example by using “Bag of visual words” methods (DBoW), which creates a dictionary of possible features and transforms an image to a vector formed by a combination of the possible features (or “words” in the dictionary) encountered for a more compressed representation. This can then be used for place recognition/relocalisation and loop closing.

Source: Bag of Visual Words in a Nutshell Note: First row are the images, the second row are the image patches from these images and the third row are the histograms “bag of visual words” in a simplified four word dictionary.

The Back-End

The back-end is usually the component that uses all the extracted information from the front-end in order to build, extend and further correct the robot’s trajectory and map. It includes several algorithms like bundle adjustment — where the goal is to correct errors by enforcing reprojective consistency over more than a pair of frames. It also extends to the generation and optimisation of a graph with the different poses estimated, as well as the comparison of the bags of visual words stored by the front-end to accomplish relocalisation and loop closure.

Loop closure consists of applying corrections to the graph when the robot recognises a previously seen place. Using this information, we can alleviate possible cumulative “drift” errors encountered within the whole SLAM process.

To give you a clearer picture of how these different parts interact with each other, here is a high-level architecture of one of our favourite SLAM systems.

Source: RTAB-Map slides Note: Without the back-end, SLAM essentially reduces to odometry. RTAB-Map is an amazing SLAM algorithm that works out of the box and creates a dense pointcloud and an occupancy grid. It can be used with an IMU, stereo camera and an RGB-D camera (more on what these sensors do below)

Sensor suite: Mono, Stereo, RGB-D, IMU

It goes without saying, that different sensor configurations can make the task of SLAM much easier or much harder. Often, the more informative your sensors are the better but with additional sensors you also need to merge and fuse their information in a clever and principled way. This can become quite expensive computationally-speaking. Ideally by using multiple sensors, their pros and cons cancel each other out and together the system is incredibly robust, but often there are trade-offs involved with this choice.

Monocular SLAM involves the use of a single camera as an input to the corresponding algorithms for SLAM. This suffers from scale issues, i.e. from a monocular SLAM system’s perspective we couldn’t tell the difference in size between a regular home and a ‘doll house’ if the camera was scaled accordingly. This scale issue can be solved through various means e.g. a good initialisation procedure or using a known length of an object or real-world distance. Although dealing with these imperfections can be algorithmically more tedious. Having a single sensor is a very simple and elegant solution in terms of robot hardware and architecture. Some famous examples are MonoSLAM, ORB-SLAM and LSD-SLAM.

Note: Example of tracking points using a monocular camera.

From a set of “n” observed points (in this case 4 on the house) in an initial position (from the red image), we move across the scene and capture a second image (highlighted in green). Using matching algorithms we can find the points observed on the first image in the second, and use that information to figure out the motion of the camera (odometry) and the structure of the scene (3D coordinates of the observed points). Structure from Motion (SfM) is another famous field with a lot of similarities to Monocular SLAM. By seeing how much a keypoint moved between two frames we can calculate the extrinsic transformation of the camera in the 2nd frame.

We found that one of our RGB cameras had a very small Field of View (FoV) of horizontal 69.4 degrees that could potentially lead to losing tracking if moving too fast. A larger FoV theoretically allows to keep track of the same keypoints even after longer displacements. In the future we will also experiment with much wider FoV cameras like the fisheye camera which observes a bigger area at a given time and therefore could potentially make our lives easier in keeping the track of the observed scene at higher speeds than a narrower FoV camera would allow.

Note: With a common laptop webcam we can barely capture the center of the living room (right), whereas the 200 degree wide angle camera (left) allows us to look at the entire living room, part of the kitchen on the left and also a small peek into the bedroom after the corridor to the right.

Stereo vision involves the use of two cameras to find the structure of the scene. Having two images from different known positions taken at the same time provides significant advantages. For example, within monocular SLAM the matching happens between two images at different times, which means that between those times any object in the scene could have moved, which would completely ruin the visual odometry calculation, whereas in stereo vision, the matching is done between images taken at the same time, i.e. no movement is needed. However, stereo SLAM and in fact most SLAMs will still have this “dynamic objects problem” since they still have to track the same features taken across multiple frames. Popular stereo SLAMs are VINS-Fusion or SOFT-SLAM.

Moreover, typically matching algorithms require a computationally expensive search for the matches, but since in this case the relative positions of the cameras is known, we can project both images to an imaginary plane in order to make the search easier and prevent some errors on the matching. This process is called stereographic rectification, and it is illustrated in the image below.

Note: The dotted rectangles represent the original unrectified images. The epipolar lines help on matching by constraining the search to the epipolar lines marked in solid red. The solid black rectangles represent the rectified images, in which the epipolar lines are all parallel to the horizon, allowing for a more efficient search of possible matches.

Note: Example of the left and right stereo infrared images (from the RealSense D435i camera we bought in the top and bottom left) with a calculated disparity image on the bottom right and colour image in top right. If you alternate closing your left eye and right eye, you should observe that objects closer to you appear to jump in position more compared to objects further away (see the pen in both infrared images being in different positions). This is essentially how stereo vision can calculate disparity/depth (bottom right). The pen is so close that it gives a blind spot to one camera from calculating disparity in certain parts of the disparity image. Calculating this disparity from two (preferably rectified) stereo images essentially computes the inverse depth map which can be visualised with a colour map going from white to yellow to red, in which white represents close distances and yellow is further away.

Passive vs active. Stereo vision doesn’t only have to use passive RGB cameras to calculate the disparity map, it’s also possible to do active stereo. Active stereo projects light-beams onto the world, and by using the deformation patterns, it can help increase the accuracy of the depth. More info on rectification and active vs passive stereo here.

RGB-D SLAM typically refers to when the input to the SLAM system is both color and depth images. The depth can be achieved by the use of a commercial depth camera which commonly contains a stereo camera but the details of how this is retrieved and optimised is left to internals of the depth camera. RGB-D SLAM is capable of calculating dense maps, i.e. which include all the visible pixels, due to having the depth information of these pixels. Compared to the sparse features mentioned above (i.e. a small number of keypoints), this represents an abundance of information which can be used and displayed in a dense map. In the next blog we’ll cover RTAB-Map (architecture shown earlier) which is an excellent RGB-D SLAM system but other famous examples include KinectFusion and Kintinuous.

Note: We bought the RealSense D435i depth camera from Intel. Above is the RealSense viewer. In the color RGB image (bottom left) we can’t see as much in dark areas compared to the infrared images (top 2) which can see better in the dark. Also enabling and disabling the emitter will show or remove the white projected dots. The emitter improves depth accuracy (you can see as it turns off and on in the depth image in the bottom right) and is good for textured areas. One can disable the IR emitter and use the infrared cameras as a typical stereo pair but it is grayscale only.

Note: The depth accuracy is high and it works out of the box. Active stereo RGB-D cameras works better at low light as well as night time. The camera also contains an internal IMU and we can see the accelerometer and gyroscope display on the right. Half way through, we switch to the 3D point cloud viewer.

Note: An example of the RealSense rs-motion program which demonstrates how a Complementary Filter can be used to estimate orientation but not position on the D435i using the inbuilt IMU only.

Inertial Measurement Units (IMU) usually contain both an accelerometer and a gyroscope (and sometimes a magnetometer). In the IMU contained in the D435i, the first measures acceleration, the second angular velocity and both measure 3 degrees of freedom (DoF) to form 6 DoF altogether in this 6 DoF IMU. Many other variants are possible, e.g. an included magnetometer would make it 9 DoF, and simpler 3 DoF IMUs are often used in autonomous vehicles.

However, to extract position from these involves double integration (since the integral of acceleration is velocity and the integral of velocity is position) and even state of the art solutions involve a lot of localisation deviation which will accumulate dramatically over time. This is called drift and why an IMU localisation system usually needs to be combined with visual feedback for less drift. The main use of the IMU comes from calculating the orientation of the robot at a high frequency from the raw data. For example, in our D435i, the accelerometer can be set to 250Hz (250 measurements per second) and gyro to 200Hz and a filter (e.g. Kalman or Complementary filter) can be used on the raw data to calculate the orientation of the IMU.

When you add an IMU to the VO system, it is called Visual Inertial Odometry (VIO). There are also many approaches (loosely-coupled and tightly-coupled) which can output almost “drift-free” localisation If you choose and carefully tune the correct VIO system. We recommend the “A Benchmark Comparison of Monocular Visual-Inertial Odometry Algorithms for Flying Robots” paper for an overview on the top VIO or SLAMs that use IMUs. In the next blog we’ll talk about some great VIO systems like VINS-Mono and ROVIO.

All SLAM fanatics likely dream of the ultimate SLAM system which can handle any situation (low light, low texture, fast motion, loop closure, occlusions, dynamic objects and scenes) and still be incredibly robust. We call this mystical mythical creature a “Parkour SLAM”.

While this might be pure hogwash and simply a ridiculous dream, algorithms which are suitable for fast drones, like these VIO systems, come close to the speed and flexibility requirements. So maybe one of these could be the Parkour SLAM we’re looking for, or maybe another day in the future the SLAM of our dreams will arrive. Part 4 of this blog series will be our attempt to find this street urchin SLAM algorithm, test it relentlessly with technical questions, and award one SLAMdog Millionaire a million dollars.

Source: ROVIO YouTube Note: Incredibly fast movements, could this be our ultimate champion SLAM, the Parkour SLAM?

Feature-based vs Direct

Source: wavelab waterloo slides. Very good slides that are worth checking out to go deeper into many direct methods and compare them on a high level with feature-based SLAMs.

From the perspective of the Deep Learning (DL) revolution, which has dominated computer vision since 2012, it’s easy to be confused by the fact that most SLAMs use “classic” keypoint or feature detectors (e.g. SIFT, ORB, FAST, Harris corners). This is surprising given all the hype and good results around DL. So why no deep-SLAM?

Firstly, it’s important to mention that in “classic” classification pipelines, keypoint descriptors were used as a feature engineering step before a typical classifier was used to output the class, e.g. a Support Vector Machine (SVM). This paradigm was completely destroyed by DL, but the main takeaway is that keypoints and keypoint descriptors are different: Keypoint descriptors are vectors of values which describe statistical properties of the image patch centred on the keypoint, keypoints refer to the points themselves, i.e. their locations.

Keypoints are very common and intuitively useful here since SLAM finds the geometric relations and the positions of the 3D points which these detected and tracked 2D keypoints represent. And in fact, this is what VSLAM does in a nutshell.

SLAM will likely greatly improve with the added use of DL — for keypoint detection, for semantics or maybe an “end-to-end SLAM”. However, classic features detectors do very well at present. For more on how DL and SLAM could help each other check out Tomasz Malisiewicz’s excellent blog on The Future of Real-Time SLAM and Deep Learning vs SLAM.

Note: This image highlights the difference between keypoints (2D pixel positions) or image descriptors of an image. In general, it’s important for feature detectors to repeatedly find re-identifiable parts of an image which can be found in successive frames e.g. corners, edges, points. Whereas the descriptor of each keypoint is a vector describing an image patch around the keypoint.

Note: Example of keypoints extracted using ORB features in ORB-SLAM2 algorithm running on a sequence from one of our houses. When referring to the word “features” it can mean a combination of the keypoint and its descriptor or just the keypoint. At the end of the GIF we see an example of loop closure fixing the accumulated drift. Keep an eye out for part 4 of this blog which covers the back-end and loop closure for greater depth on this.

There is another rising paradigm within SLAM that avoids the use of sparse keypoint detectors. These so called “direct” methods use pixel intensities — unprocessed RGB values from the image — to directly estimate the motion of the camera. This minimises a photometric loss whereas feature-based methods usually minimise a geometric loss. Photometric essentially means we are calculating how to projectively warp the first image into the second image using the pixel values directly from both images. We enforce consistency on the transformation from both the raw image, and the transformed synthetic generated image, by minimising the intensity differences.

Source: link Note: This highlights the projective transformation (T) between the first image to the second. Finding the correct T is the goal here.

There are many variations within direct methods. For example, we could use the sparse points in Direct Sparse Odometry (DSO). The DSO paper in provides a good comparison between all four combinations of dense + direct, dense + indirect, sparse + direct, sparse + indirect. We encourage you to check it out, especially if you find it confusing like we did.

Another famous paper Semi-dense Visual Odometry (SVO) is a mix of both feature-based and direct methods. Meaning that it only tracks high gradient pixels from edges and corners (using direct photometric error) but relies on feature-based methods for joint optimization of structure and motion. These combinations are why it is deemed to be only ‘semi-direct’.

Direct has the potential to be able to track parts of the image with low texture, whereas feature-based SLAM might find this more difficult. Many famous SLAM fanatics believe direct-based SLAM will eventually prevail over indirect. But the results of feature-based SLAM speak for themselves. Only time will tell.

Conclusion

In summary, we discussed everything from the motivation behind mapping, mapping in simulation, to how localisation and mapping desperately need each other (SLAM); and many related fields (e.g. Visual Odometry, SfM), and the axes of variation between different SLAM algorithms (e.g. their front vs back end, sensor suite, etc.). Not only this, but also the specific methods used, e.g. direct vs feature based, sparse versus dense maps/pointclouds, and whether these systems have global optimisation and loop closure. To illustrate the point of how different SLAM can vary, you can find a comparison between many famous SLAMs in the table below.

Source: Visual SLAM algorithms: a survey from 2010 to 2016 Note: This compares many different SLAMs across their varying dimensions. For example, the visual SLAM algorithms used with the raw image data could be feature-based (ORB-SLAM, MonoSLAM) vs direct (DTAM, LSD-SLAM) vs semi-direct (SVO) vs RGB-D (KinectFusion, SLAM++).

This blog, we hope, should largely introduce and conclude the front-end component of SLAM. In some sense, this component is mostly responsible for calculating the odometry of the camera. In a few cases we saw, it can produce highly accurate localisation without any back-end component needed.

However, the longer the robot’s trajectory, the more drift will accumulate, even if just 1 millimetre in translation or 1/100 of a rotational degree per 10 kilometres, this will compound. Because of this, in our opinion, a back-end system and the ability to recognise previous places is needed. Fortunately, we reserved the details of this component for the next part of this blog series which will include bundle adjustment, pose graph creation and optimization, as well as loop closure.

But the most exciting part is yet to come! In the next instalment of C2C (#cuplife) we will display our selection and evaluation of a number of state of the art systems in SLAM, including some algorithms that we tested and where we see the future of the field potentially heading. Stay tuned!