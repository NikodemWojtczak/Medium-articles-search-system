XLNet — a clever language modeling solution

Language modeling with a deep model — the challenge and a solution

TL;DR

Unsupervised learning of probability distribution of word sequences in a language by predicting each word within its sentence context in a large corpus, has proven to be useful to create models and word representations that can then be fine tuned for downstream NLP tasks. Two factors seem to play a key role to boost performance when fine tuning models for downstream syntactic and semantic tasks:

Is the representation of a word capturing the full context of that word in a sentence? Full context is the entire sequence of words that precede and follow a word in a sentence(bidirectional context)

Full context is the entire sequence of words that precede and follow a word in a sentence(bidirectional context) Is the model multilayered to output multiple representations for a word, one from each layer? Representations of a word from the different layers of a model have been found to be useful for downstream syntactic and semantic tasks

Satisfying both these design factors poses a challenge (described in detail below) that XLNet addresses by a clever solution. This enables XLNet in part to perform better than previous state-of-art model BERT which satisfies the above two requirements but with some deficiencies. However, XLNet requires more compute power and memory (GPU/TPU memory) in comparison to BERT. It also remains to be seen if improvements to reduce compute/memory requirements etc, will make XLNet good enough from a practical sense to replace BERT in all the NLP tasks that BERT currently excels in (Q&A, text classification, sequence tagging tasks like NER etc.).

The challenge to create deep bidirectional context representations

Language modeling, which is essentially learning the probability distribution of word sequences in a language by predicting words within their sentence contexts in a large corpus, can be done in multiple ways as illustrated in figure below