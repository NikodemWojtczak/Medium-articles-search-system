Need for dimensionality reduction

In machine learning projects we often run into curse of dimensionality problem where the number of records of data are not a substantial factor of the number of features. This often leads to a problems since it means training a lot of parameters using a scarce data set, which can easily lead to overfitting and poor generalization. High dimensionality also means very large training times. So, dimensionality reduction techniques are commonly used to address these issues. It is often true that despite residing in high dimensional space, feature space has a low dimensional structure.

Two very common ways of reducing the dimensionality of the feature space are PCA and auto-encoders. I will only provide brief introduction to these, for a more theoretically oriented comparison read this post.

PCA

PCA essentially learns a linear transformation that projects the data into another space, where vectors of projections are defined by variance of the data. By restricting the dimensionality to a certain number of components that account for most of the variance of the data set, we can achieve dimensionality reduction.

Autoencoders

Autoencoders are neural networks that can be used to reduce the data into a low dimensional latent space by stacking multiple non-linear transformations(layers). They have a encoder-decoder architecture. The encoder maps the input to latent space and decoder reconstructs the input. They are trained using back propagation for accurate reconstruction of the input. In the latent space has lower dimensions than the input, autoencoders can be used for dimensionality reduction. By intuition, these low dimensional latent variables should encode most important features of the input since they are capable of reconstructing it.

Comparison

PCA is essentially a linear transformation but Auto-encoders are capable of modelling complex non linear functions. PCA features are totally linearly uncorrelated with each other since features are projections onto the orthogonal basis. But autoencoded features might have correlations since they are just trained for accurate reconstruction. PCA is faster and computationally cheaper than autoencoders. A single layered autoencoder with a linear activation function is very similar to PCA. Autoencoder is prone to overfitting due to high number of parameters. (though regularization and careful design can avoid this)

When to use which?

Apart from the consideration about computational resources, the choice of technique depends on the properties of feature space itself. If the features have non-linear relationship with each other than autoencoder will be able to compress the information better into low dimensional latent space leveraging its capability to model complex non-linear functions. What does it mean for the features to have non-linear relationships? Let us do a couple of simple experiments to answer these questions and shed some light on comparative usefulness of both techniques.

Experiments 2D

Here we construct two dimensional feature spaces (x and y being two features) with linear and non-linear relationship between them (with some added noise). We will compare the capability of autoenocoders and PCA to accurately reconstruct the input after projecting it into latent space. PCA is a linear transformation with a well defined inverse transform and decoder output from autoencoder gives us the reconstructed input. We use 1 dimensional latent space for both PCA and autoencoders.

It is evident if there is a non linear relationship (or curvature) in the feature space, autoencoded latent space can be used for more accurate reconstruction. Where as PCA only retains the projection onto the first principal component and any information perpendicular to it is lost. Let us look at the reconstruction cost as measured by mean squared error (MSE) in the table below.

Experiments 3D

Conducting similar experiments in 3D. We create two three dimensional feature spaces. One is a 2D plane existing in 3D space and the other is a curved surface in 3D space.

We can see that in case of a plane there is a clearly two dimensional structure to the data and PCA with two components can account for 100% of the variance of the data and can thus achieve perfect reconstruction. In case of a curved surface two dimensional PCA is not able to account for all the variance and thus loses information. The projection to the plain that covers the most of variance is retained and other information is lost, thus reconstruction is not that accurate. On the other hand autoencoder is able to reconstruct both plane and surface accurately using two dimensional latent space. So 2D latent space is able to encode more information in case of autoencoder because it is capable of non-linear modelling. Reconstruction cost is provided in the table below.

Random Data Experiment

Here we create a random data without any collinearity. All features are independently sampled from a uniform distribution and have no relationship with each other. We use two dimensional latent space fro both PCA and Autoencoder.

We see that PCA is able to retain the projection onto the plane with maximum variance, and loses a lot of information because the random data did not have a underlying 2 dimensional structure. Autoencoder also does poorly since there was no underlying relationship between features.

Conclusion

For dimensionality reduction to be effective, there needs to be underlying low dimensional structure in the feature space. I.e the features should have some relationship with each other.

If there is non-linearity or curvature in low dim structure than autoencoders can encode more information using less dimensions. So they are a better dimensionality reduction technique in these scenarios.

All code for the experiments can be found here: