Welcome to the data era. Where everything produces data and everyone wants to use it. The big question is HOW?

In a recent article by Cambridge Semantics, called “The Story of the Data Fabric”, they point out three specific issues we have in organizations and our data:

There is more data than ever from a multitude of both structured and unstructured sources.

Data in its raw form is highly variable in data quality. Sometimes it is well formed and clean. Other times it is sparse and uneven.

Data comes in many different (and incompatible) formats.

All of this came from the “Data Lake Era”. Some years ago, when data started to grow exponentially we changed our Data Warehouses that in a few words are systems that pulls together data from many different sources within an organization for reporting and analysis to Data Lakes. The data warehouses were present in almost every organization, and they were created to be used in “Business Intelligence (BI)”, the predecessor of data science (together with data mining). In BI we did reports, analysis and studies from organized and structured data from relational databases (mostly), and raw data, although it was there, was not that used.

When data grew and became more and more weird and unstructured a new paradigm appear. The mighty data lake.

In almost all companies doing big data and data science it’s the standard. The premise of data lakes is: store all of your structured and unstructured data at any scale. So we started doing that. Then a lot of new technologies were created to manage them, like Hadoop and Spark.

This is a brief story of Apache Spark that I created a while ago but shows how big data transformed over the years too (it’s updated btw):