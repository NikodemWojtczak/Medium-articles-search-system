Stage 1: Collecting and pre-processing the data

Scraping the reviews:

For developing a decisive system, a subset of the extensive review data available on the Internet is taken into consideration. The reviews for neurological drugs for the treatment of epilepsy, seizures and bipolar disorders were scraped using scrapy, a Python library for developing custom web crawlers.

The final dataset consisted of an average of 200 reviews for each of the seven drugs, which was then split into training and test dataset in the ratio of 80:20.

Examples of scraped reviews

Cleaning the reviews:

Tokenizing the reviews into sentences using sent_tokenize from Natural Language Toolkit(nltk).

from Natural Language Toolkit(nltk). Standardizing of text which involved lowercase conversion, splitting of conjugate words, and correcting misspelled words.

Lemmatization to get the root word form of the words using nltk.

review = “But since I started alternating, I haven’t had a seizure”

pre_processed_review = preprocess(review)

print(pre_processed_review) ['but','since','I','start','alternate','I','have','not','had','seizure']

The stopwords, negation, and punctuation are retained in this step to preserve the information contained in the reviews as best as possible. At the end of this step, the cleaned sentences are ready to be labeled into appropriate categories.

Labeling the training dataset:

The sentence can be classified into one of the three categories:

Effective: The reviews in which the improvement of patient’s health is implied after use of the drug.

Ineffective: The reviews which imply no change in or worsening of the condition of the patient but contain no mentions of any adverse reactions after use of the drug.

Adverse: The reviews which contain explicit mentions of adverse reactions to the patient after use of the drug.

An auto-labeler was set up which evaluated the sentence on three parameters.

A dictionary consisting of a collection of ‘problem’ words which tend to occur in the case of adverse category sentences.

problems='hallucinations weakness hairloss tired hair loss nausea shakiness tremor tremors stones weight pounds flu flus lbs drowsiness dizziness appetite manic maniac cold vomiting seizures nauseous vision inflammation tingling numb numbness swollen swelling depression attacks blisters skin rash diarrhoea headache headaches head severe fever sleep pain stress numb'

2. The POS(parts-of-speech) tags of individual words of the sentence, generated using the nltk library. A detailed description of the POS tagging process and tags can be found here.

review = 'laying down is excruciating and im in the process of running test' [('laying', 'VBG'), ('down', 'RP'), ('is', 'VBZ'), ('excruciating', 'VBG'), ('and', 'CC'), ('im', 'NN'), ('in', 'IN'), ('the', 'DT'), ('process', 'NN'), ('of', 'IN'), ('running', 'VBG'), ('test', 'NN')]

3. The compound VADER sentiment score of each sentence. VADER is a python module which is used for scoring the sentiment of a review in terms of polarity(positive or negative) and intensity(score). The compound score is an integer value ranging between -1 and 1 to evaluate the sentiment conveyed in the text. A value of 0 is the center point of the scale signifying neutral sentiment.

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

def analyze_sentiment(df):

sentiments = []

sid = SentimentIntensityAnalyzer()

for i in range(df.shape[0]):

line = df[‘Review’].iloc[i]

sentiment = sid.polarity_scores(line)

sentiments.append([sentiment[‘neg’], sentiment[‘pos’],

sentiment[‘neu’], sentiment[‘compound’]])

df[[‘neg’, ‘pos’, ‘neu’, ‘compound’]] = pd.DataFrame(sentiments)

return df

Original reviews with VADER Sentiment Scores and category labels.

Thus, a preliminary labeling scheme for reviews was developed which was further refined by manual labeling of sentences.

Pro-tip by Richard Socher highlighting the effectiveness of manual labeling.

The reviews misclassified by the auto-labeler were manually labeled by two independent annotators, and the conflicts resolved by an unbiased third annotator. The dataset was then verified by a medical health professional.

The training set is now ready for input to the classification algorithm.

Stage 2: Choosing the right approach

Vectorizing:

The vectorizer is used to convert every word into a vector of size equal to the unique count of words in the entire collection of documents(reviews). This approach is known as the ‘bag-of-words’ model. This model converts the text into numerical features form required by the machine learning algorithm.

For example, a review of a certain drug reads ‘This drug has made me worse’ while another review says ‘This drug has made me better’. The count of unique words in the reviews is found to be 7 (‘this’, ‘drug’, ‘has’, ‘made’, ‘me’, ‘worse’, ‘better’).

Thus, the vectors for the reviews are

‘This drug has made me worse’ = [1,1,1,1,1,1,0]

‘This drug has made me better’ = [1,1,1,1,1,0,1].

We can use either the CountVectorizer approach (creation of a sparse matrix of the size of words * reviews) or the Term Frequency-Inverse Document Frequency(TF-IDF) approach (measures the frequency of a word along with the rareness of the word in the collection).

You can learn more about these two approaches and their implementation here.

Creation of bi-grams and tri-grams:

In NLP, each word in the text document is referred to as a ‘gram’. Thus, a combination of co-occurring words is known as an n-gram, where n is the length of the combination considered.

For example, ‘bipolar disorder’ would be an often occurring combination in our corpus of words. Thus, it can be represented with a bi-gram instead of the unigrams for individual words ‘bipolar’ and ‘disorder’, as both of these words may not appear as separate words as frequently.

bigram = gensim.models.Phrases(words, min_count=5, threshold=100)

trigram = gensim.models.Phrases(bigram[words], threshold=100)

bigram_mod = gensim.models.phrases.Phraser(bigram)

trigram_mod = gensim.models.phrases.Phraser(trigram) def make_bigrams(texts):

return [bigram_mod[doc] for doc in texts] def make_trigrams(texts):

return [trigram_mod[bigram_mod[doc]] for doc in texts]

The bi-grams or tri-grams may be obtained as features independently using Gensim (as above) or by using scikit-learn’s feature extraction module to automatically generate them during vectorization.

Choosing the algorithms:

The reviews need to be classified into three categories, that is, effective, ineffective and adverse, therefore we need to use a multi-class classifier instead of a binary classifier.

For comparative analysis, four multi-class algorithms are used for prediction of categories.

OneVsRest SVM classifier:

It involves the fitting of a single SVM classifier per class while considering all other classes as one class, effectively turning the problem into a binary classification problem.

A graphical illustration of OneVsRest classification from Andrew Ng’s course is shown here (via stats.stackexchange here). Logistic Regression multi-class classifier Random Forest classifier Bagging meta-estimator with logistic regressor base:

This ensemble technique uses random subsets of data to fit individual classifiers of the base type and then aggregates their predictions to obtain a single prediction.

The code for training and testing the classifiers mentioned above using scikit-learn is given below.

Creation of feature selections:

The performance of the algorithms was tested against a variety of feature selections in a trial-and-error fashion. Thus, various combinations of features were generated by combining vectorization techniques, the number of words considered as features and sentiment scores of the reviews. An example is shown below

from sklearn.feature_extraction.text import TfidfVectorizer vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=15000)

vector = vectorizer.fit_transform(corpus) df2 = pd.DataFrame(vector.toarray())

df_final = pd.concat([df2, reviews], axis =1)

Top 15000 word vectors + VADER Sentiment Scores.

Here, we convert the top 15,000 occurring words and their bi-grams (as ngram_range is set between 1–2) to feature vectors using TF-IDF. The vectors of each review are combined with the VADER sentiment scores to obtain the features, which are to be fed to the classification algorithm to decide the class of that review.

Similarly, 7 other such feature sets are created as below:

FS-1 : CountVectorizer

FS-2 : CountVectorizer + VADER Sentiment Scores

FS-3 : CountVectorizer top 10000 features + VADER Sentiment Scores + n-gram range 1–3

FS-4 : CountVectorizer all features + VADER Sentiment Scores + n-gram range 1–3

FS-5 : TfidfVectorizer

FS-6 : TfidfVectorizer + VADER Sentiment Scores

FS-7 : Tfidf Vectorizer top 10000 features + VADER Sentiment Scores + n-gram range 1–3

FS-8 : Tfidf Vectorizer top 15000 features + word tokenize analyser + VADER Sentiment Scores + n-gram range 1–3

Stage 3: Visualising the Results

We present the system results in three formats: a review classifier, a summarization of each of the categories using TextRank, and an interactive visual plot of reviews for drug comparison.

f1 score evaluation:

reviews.groupby("Category").count().Category Adverse 1089

Effective 1276

Ineffective 335

We use the weighted f1 score metric from sklearn’s f1_score metric module for evaluating performance as it has the added advantage of accounting for class imbalance in a multi-class classification problem. It calculates the f1 score for each class and averages them by also considering the support (number of instances as shown above) of each class.

Various approaches, feature selections, and their respective weighted f1-scores

An f1 score of ~0.74 is obtained using feature selection 8 with logistic regression approach.

Ranking Reviews by TextRank

The TextRank algorithm uses the similarity graph of TF-IDF vectors to calculate the importance of each node. The node or review which is most similar to most other reviews is considered ‘central’ to the class it belongs.

Top reviews for the effective category with similarity scores

Here, the reviews for the effective category are ranked for a particular drug. The phrases ‘best drug’, ‘helped me lots’, ‘would not be able to live without it’ best reflect the theme of the effective class, and therefore the reviews containing them are ranked at the top using TextRank.

Similarly, the adverse category reviews are compared against a dictionary of adverse reactions and an occurrence-ordered graph is generated for the adverse reactions caused by a drug.

Occurrence-ordered graph for adverse category reviews.

Interactive Visualization using Bokeh

The webapp uses Bokeh, an interactive visualization library in python, to present interactive bar graphs to show a side-by-side comparison of drugs for the user.

A bokeh server is used to initiate a service which responds to changes on the webapp to trigger callbacks for updating the data for the plot. These changes are synced by the browser of the webapp and the plot is updated accordingly.

To run the bokeh server, a method is called in the app.py by running the command

bokeh serve myapp.py

The visualization is rendered at localhost port 5006 which can be integrated into the app.py

Bar graph dynamic update in Bokeh using checkboxes.

The complete code for interactive visualizing using bokeh is given here.

The code contains three major functions make_dataset , make_plot and update which are used for creating the dataframes and their values, static plotting, and updating the data based on checkbox state (checked or unchecked) respectively. Finally, the plot and the control elements are placed next to each other using curdoc() and the output is rendered to the web browser.

Summary

I wanted to create a user-focused webapp that helps the patients to know more about the experiences of others who have used similar drugs in the past and save them the trouble of reading hundreds of reviews on online forums.

In the future, the system can be improved in many ways by providing support for authentication of reviews, considering reviews of other domains, and improving the efficiency by employing neural networks.

Thanks to my amazing teammates, who put in the efforts to make this idea into a reality.

Feel free to comment if you have any suggestions. I would love to hear your feedback!