Data Scientist’s Guide to Summarization

A text summarization tutorial for beginners Richa Bathija · Follow 13 min read · Mar 18, 2019 -- 2 Listen Share

Team Members: Richa Bathija, Abhinaya Ananthakrishnan, Akhilesh Reddy(@akhilesh.narapareddy), Preetika Srivastava (@preetikasrivastava30)

Did you ever face a situation where you had to scroll through a 400 word article only to realize that there are only 4 key points in the article? All of us have been there. In this age of information, with content getting generated every second around the world, it has become quite difficult to extract the most important information in an optimal amount of time. Unfortunately, we have only 24 hours in a day and that is not going to change. In the recent years, advancements in Machine learning and Deep learning techniques paved way to the evolution of text summarization which might solve this problem of summarization for us.

In this article, we will give an overview about text summarization, different techniques of text summarization, various resources that are available, our results and challenges that we faced during the implementation of these techniques.

Types of text summarization

There are broadly two types of summarization — Extractive and Abstractive

Extractive — These approaches select sentences from the corpus that best represent it and arrange them to form a summary.

— These approaches select sentences from the corpus that best represent it and arrange them to form a summary. Abstractive — These approaches use natural language techniques to summarize a text using novel sentences.

We tried out different extractive and abstractive techniques and also a combination approach that uses both of them and evaluated them on some popular summarization datasets.

Datasets used and their descriptions

Data is the most crucial part when it comes to any Natural language processing application. The more data that you can get your hands on to train your model, the more sophisticated and accurate the results would be. This is one of the reasons why Google and Facebook have outsourced a lot of their work in this domain.

We used 3 open source data sets for our analysis.

GIGAWORD dataset

It is popularly known as GIGAWORLD dataset and contains nearly ten million documents (over four billion words) of the original English Gigaword Fifth Edition. It consists of articles and their headlines. We have used this dataset to train our Abstractive summarization model.

CNN Daily mail dataset

CNN daily mail dataset consists of long news articles(an average of ~800 words). It consists of both articles and summaries of those articles. Some of the articles have multi line summaries also. We have used this dataset in our Pointer Generator model.

Opinions dataset

This dataset contains sentences extracted from user reviews on a given topic. Example topics are “performance of Toyota Camry” and “sound quality of ipod nano”, etc. The reviews were obtained from various sources — Tripadvisor (hotels), Edmunds.com (cars) and amazon.com (various electronics).Each article in the dataset has 5 manually written “gold” summaries. This dataset was used to score the results of the abstractive summarization model.

What is ROUGE?

To evaluate the goodness of the generated summary, the common metric in the Text Summarization space is called Rouge score.

ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation

It works by comparing an automatically produced summary or translation against a set of reference summaries (typically human-produced). It works by matching overlap of n-grams of the generated and reference summary.

Extractive Techniques

Extractive summarization techniques select relevant phrases from the input document and concatenate them to form sentences. These are very popular in the industry as they are very easy to implement. They use existing natural language phrases and are reasonably accurate. Additionally, since they are unsupervised techniques, they are ridiculously fast. As these techniques only play around with the order of sentences in the document to summarize they do not do as great a job as humans to provide context.

NLTK Summarizer

We wanted to start our text summarization journey by trying something simple. So we turned to the popular NLP package in python — NLTK. The idea here was to summarize by identifying “top” sentences based on word frequency.

Simple NLTK Summarizer

Although the technique is basic, we found that it did a good job at creating large summaries. As expected, brevity wasn’t one of its strengths and it often failed at creating cohesive short summaries. Following are the summaries of “A Star is Born” Wikipedia page.

NTLK summarizer — 10 sentence summary

NLTK summarizer — 2 sentence summary

You can find the detailed code for this approach here.

Gensim Summarizer

The Gensim summarization module implements TextRank, an unsupervised algorithm based on weighted-graphs from a paper by Mihalcea et al. It is built on top of the popular PageRank algorithm that Google used for ranking.

After pre-processing text this algorithm builds graph with sentences as nodes and chooses the sentences with highest page rank to summarize the document

TextRank

TextRank is based on PageRank algorithm that is used on Google Search Engine. In simple words, it prefers pages which has higher number of pages hitting it. Traditionally, the links between pages are expressed by matrix as shown in the image below. This matrix is then converted to a transition probability matrix by dividing the sum of links in each page which influences the path of the surfer.

TextRank Overview

In the original “TextRank” algorithm the weights of an edge between two sentences is the percentage of words appearing in both of them.

Similarity Calculation between sentences

However, the updated algorithm uses Okapi BM25 function to see how similar the sentences are. BM25 / Okapi-BM25 is a ranking function widely used as the state of the art for Information Retrieval tasks. BM25 is a variation of the TF-IDF model using a probabilistic model.

Improved Similarity with BM25

In a nutshell, this function penalizes words that appears in more than half the documents of the collection by giving them a negative value.

The gensim algorithm does a good job at creating both long and short summaries. Another cool feature of gensim is that we can get a list of top keywords chosen by the algorithm. This feature can come in handy for other NLP tasks, where we want to use “TextRank” to select words from a document instead of “Bag of Words” or “TF-IDF”. Gensim also has a well-maintained repository and has an active community which is an added asset to using this algorithm.

gensim’s summarization of “A Star is Born” Wikipedia page

You can find the detailed code for this approach here.

Summa summarizer

The summa summarizer is another algorithm which is an improvisation of the gensim algorithm. It also uses TextRank but with optimizations on similarity functions. Like gensim, summa also generates keywords.

summa’s summarization of “A Star is Born” Wikipedia page

You can find the detailed code for this approach here.

Sentence Embeddings

We wanted to evaluate how text summarization works on shorter documents like reviews, emails etc. Most publicly available datasets for text summarization are for long documents and articles. Hence we used the Amazon reviews dataset which is available on Kaggle.

The structure of these short documents is very specific. They start off with a context and then talk about the product or specific matter, and they end off with closing remarks. We used K-means clustering to summarize the types of documents following the aforementioned structure.

For clustering the sentences, one has to convert the text to number, which is done using pre-trained word embeddings. We have used GloVe embedding for achieving this. The embeddings created consisted of a dictionary with the most common english words and a 25 dimensional embedding vector for each one of them. The idea is to take each sentence of the document, and calculate it’s embeddings, thus creating a data matrix of (# of documents X 25) dimensions. We kept it simple for obtaining the sentence embeddings by taking the element-wise weighted average of the word embeddings for all the words in the sentence.

Then, all of the sentences in a document are clustered in k = sqrt(length of document) clusters. Each cluster of sentence embeddings can be interpreted as a set of semantically similar sentences whose meaning can be expressed by just one candidate sentence in the summary.

The candidate sentence is chosen to be the sentence whose vector representation is closest to the cluster center.

Candidate sentences corresponding to each cluster are then ordered to form a summary for an email. The order of the candidate sentences in the summary is determined by the positions of the sentences in their corresponding clusters in the original document. We ran the entire exercise for Amazon Food reviews dataset, and the results were pretty good on the mid length reviews. One of them is shown below:

Fig. Example of Food review summarization using K-Means clustering

You can find the detailed code for this approach here.

Here’s a fun tool we created to play around with extractive summarization techniques..

We wanted to make a tangible end to end solution to our project. We wanted a viable UI for our algorithms through which the user could interact with the backend.

For this we integrated widgets into Jupyter Notebook

The Widget enables the user to get a holistic view of the project, by asking for user input and generating the output. The widget takes three parameters as input:

The type of algorithm the user wants to use for text summarization

The URL of the Wikipedia page whose text needs to be summarized

The shrink ratio (Amount of output text to be displayed as a percentage of the original document)

The user enters these three inputs and hits the RUN button. The algorithm gets called from the backend and the output is generated based on the shrink ratio.

You can see the detailed code for this notebook here.

Widget Layout

Check out this cool video to see how it actually works..

Abstraction techniques

Abstractive summarization began with Banko et al. (2000) ‘s research suggesting the use of machine translation model. However, in the past few years RNNs using encoder — decoder models with attention has become popular for summarization.

Sequence2Sequence Attention model

We attempted to train our own encoder — decoder model by using the GIGAWORLD dataset from scratch by trying to imitate the first state-of-the art encoder-decoder model.

If you’re unfamiliar with Recurrent Neural Networks or the attention mechanism, check out the excellent tutorials by WildML, Andrej Karpathy and Distill.

Using Google Cloud Platform

We used Google Cloud platform to train our RNN and test it. To learn more about how to setup the environment on Google Cloud Platform, you can view this tutorial also written by our team here.

We used a 16 — CPU system with 2 Tesla P100 GPUs in order to train a simple encoder decoder Recurrent neural network with the following hyper parameters:

Pre-trained tensor flow embeddings were also used.

Although we ran our code on the cloud, given the sheer size of GIGAWORLD dataset, training for even 5-epochs took 72 hours

When we scored the model on validation data, we observed that even after 5 epochs (and 3 days), the model did a good job at matching the actual summaries written by humans. Following are a few examples: