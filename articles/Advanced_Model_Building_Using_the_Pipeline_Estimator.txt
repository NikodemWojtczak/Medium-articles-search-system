In this article, we show how the Pipeline estimator from sci-kit learn can be used for efficient machine learning model building, testing, and evaluation. We shall illustrate our example using the cruise ship dataset to build a linear regression model for recommending ship’s crew size.

In the previous article (Feature Selection and Dimensionality Reduction Using Covariance Matrix Plot), we’ve shown that a covariance matrix plot can be used for feature selection and dimensionality reduction.

Using the cruise ship dataset cruise_ship_info.csv, we found that out of the 6 predictor features [‘age’, ‘tonnage’, ‘passengers’, ‘length’, ‘cabins’, ‘passenger_density’], if we assume important features have a correlation coefficient of 0.6 or greater with the target variable, then the target variable “crew” correlates strongly with 4 predictor variables: “tonnage”, “passengers”, “length, and “cabins”. We, therefore, were able to reduce the dimension of our feature space from 6 to 4.

Now, suppose we want to build a model on the new feature space for predicting the crew variable. Our model can be expressed in the form:

In this article, we show how we can train, test, and evaluate our model in an efficient manner using scikit-learn’s Pipeline estimator.

What is the Pipeline estimator?

The pipeline estimator is a way to group several estimators and transformers together. This helps us to write efficient code. In the figure shown below, the pipeline estimator performs 3 important tasks in the input dataset: (1) Feature scaling using the StandardScaler() transformer; (2) Feature engineering and dimensionality reduction using the PCA() transformer; and (3) Model building using the LinearRegression() estimator.

Why Use the Pipeline estimator?

As data scientists, it is important that we that we code efficiently. An efficient code containing fewer lines makes it easier to catch any errors that might be present. The Pipeline took makes model building very efficient and…