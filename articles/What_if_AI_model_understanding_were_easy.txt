Irreverent Demystifiers

What if AI model understanding were easy?

Let’s talk about the What-If Tool, as in “What if getting a look at your model performance and data during ML/AI development weren’t such a royal pain in the butt?” (Or ignore my chatter and scroll straight to the walkthrough screenshots below!)

Why bother with analytics for AI?

Being able to get a grip on your progress is the key to speedy iteration towards an awesome ML/AI solution, so good tools designed for analysts working in the machine learning space help them help you meet ambitious targets and catch problems like AI bias before it hurts your users.

The What-If Tool is for speedy machine learning analytics, built to accelerate iterative model development and training.

Analytics is not about proving anything, so this tool won’t help with that. Instead, it’ll help you discover the unknown unknowns in your data faster. Learn more about explainable AI (XAI) and its limitations here.

About the What-If Tool

The What-If Tool is for speedy machine learning analytics, built to accelerate iterative model development and training. The first version (released in late 2018) was pretty, but you couldn’t use it unless you were all-in on TensorFlow. As someone who appreciates the expediency of tools like Scikit Learn, I’m delighted that What-If Tool is now geared at all analysts working with models in Python.

No more TensorFlow exclusivity!

We’ve been incorporating feedback from internal and external users to make the tool awesome for data scientists, researchers, and corporate megateams alike. Learn more about our UX journey here. Without further ado, let me make myself a hexpresso and play with the most current version to give you my take on what’s awesome and what’s awful.

What’s awesome about the What-If Tool?

Easy to use and versatile

In the current version of What-If Tool, we expanded access to the magic beyond TensorFlow afficionados. Yes, that’s right — no more TF-exclusivity! This is model understanding and quick data exploration for feature selection/preprocessing insights even if you’re allergic to TensorFlow. Want to compare models made in Scikit Learn or PyTorch? Step right up! Does it work with standard Jupyter notebooks? You bet! It also works with Colaboratory because we know you prefer to choose your weapon. The tool is designed to reduce the amount of code you need to write to get your eyes on your data, so it’s built for ease of use.

In this screenshot, we’re using the tool to compare two classifiers (deep neural network on the x-axis, linear model on the y-axis) trained on the UCI Census Income Dataset to predict whether someone will earn more than $50,000 a year. Numbers closer to 1 indicate that a model is giving a stronger YES vote. The scatterplot shows the votes of one model versus the other. See the notebook here and play with it yourself if you’re feeling curious — no install required.

As expected, there’s a positive correlation but the models don’t give identical results. (Working as intended! They’re different models, after all.) If I’m curious about how the model votes are related to, say marital status, it’s very easy to find out — simply select that feature from the dropdown menu.

Voilà! Most of our dataset shows civil marriages and we see an interesting preponderance of other statuses where the models disagree with one another or both vote a strong no. Remember, this is analytics, so don’t jump to conclusions beyond our current dataset!

The What-If Tool is not going to give you every slice of every view of every way that you might want to explore your data. But it’s great at what it’s designed for: a first start with low effort. It also works on a subsample, which means you get a quick look quickly without having to pay the memory cost to ingest and process all your data if you don’t want to. Huzzah for speed!

It’s great at what it’s designed for: a first look with low effort.

Fighting AI bias

The What-If Tool is also your secret weapon for fighting AI bias. To understand why, check out my discussion of AI bias here. Its bias-catching features are not an accident — a large fraction of the project’s core team hails from Google Brain’s PAIR initiative aimed at human-centered research and design to make AI partnerships productive, enjoyable, and fair.

In the fairness tab, we can play with all kinds of uncomfortable questions. For example, we can find out where we’d have to set our classification thresholds (the ones you’d naively want to put at 50%) for males vs females in our test set to achieve demographic parity between them. Uh-oh.

Smarter ML/AI iteration

The What-If Tool incorporates the Facets tool, which tackles the data analytics piece without the model understanding component.

In the features tab, I can get a quick look at histograms to show me how my features are distributed. Oh my goodness, capital loss is a super imbalanced feature with only ~4% nonzero values. I’m already itching to try dropping it and rerunning both models. If you’ve been around the block a few times (or studied the math) you’ll know that putting something like that in a linear model is bad news indeed. I see similar trouble with capital gains. (If you insist on using ’em, how about doing some light feature engineering to combine them? Minuses are awesome.) Ah, and here’s a question for the more advanced analysts among you: can you see why optimizing for accuracy should make us very nervous?

What-If puts both together to help you iterate smartly. Think of it like this: to figure out what to do next in the kitchen, you want a handy way to compare the tastiness of several potential recipes (with model understanding) and also getting a handle on what’s in your grocery bags (with data analytics) so you don’t accidentally use rotten tomatoes. Facets gave you eyes on your ingredients, while the What-If Tool goes a step further to deliver that plus the recipe comparison. If you’ve been cooking blindly, you’ll love this tool for iterative model development and training.

Exploring counterfactuals

Never underestimate the power of being able to ask your own what-if questions, like “What if we raise this person’s work hours and change their gender? How does the model react?” The What-If Tool is purpose-built to give you more of a grip on guided what-if/counterfactual questions. The tool makes it easy to see how the prediction changes if you vary a variable (finally!) over its domain and shows you whether there’s some value where the prediction behaves in a suspicious way and letting you see exactly where the classification flips from, say, NO to YES. Try playing with the counterfactual options to find a datapoint’s most similar counterpart in a different predicted class. It’s a great way to see the effects of subtle differences on your model’s output.

Back to our first tab. That red point I’ve selected is one where the models are having an argument: neural network says nah, but linear model says a gentle yes to high income. What-If… I want to do a quick deep dive into that point off the diagonal? I simply click on it and there’s the info. Turns out the linear model is right, this is a high income earner. Moreover, it’s a married woman who works 10 hours per week. I love how quickly I could see that.

What’s this “visualize” thing on the left? Let’s see what happens if we try toggling the “counterfactual” setting.

Aha! Here’s the nearest buddy where neural network changes its mind and correctly predicts a large salary. And it is a buddy indeed: this is a male executive who works 45 hours a week. What-If… we do a deep dive and see which of these differences the models are most sensitive to?

Looking at the partial dependence plots, we can see that the neural network (blue) seems to expect pay to go up with hours worked, while the linear model (orange) slopes down. Curious. The statistician in me is shouting at all of us not to get excited — they’re probably both wrong in their own way, so we shouldn’t learn anything grand about the universe, but seeing how models react to inputs is very valuable for picking approaches to try next. Our mystery candidate’s lower hours worked look more compelling to the linear model (yeah, quiet down friends, obviously the economist in me is just as suspicious as the statistician). I bet we also want to take a quick look at other features here — how about gender…?

Interestingly, the linear model (orange) is not getting itself too excited about gender, but the neural network (blue) seems more reactive to it. How about our mystery woman’s question-mark of an occupation? Could that be contributing to her lower score by the neural network?

Whoa, while the linear model (orange) is stoic again, the neural network (blue) gives execs a pretty big prediction boost relative to those with missing occupation information. Now isn’t the time to say that snarky thing about linear models versus neural networks, is it? Well, maybe I’ll restrain myself… the whole point of the tool is to give you eyes on your data so you can iterate wisely, not let biases take you by surprise, and create a more awesome model faster. We’re not done yet! (But I sure have a few ideas I’m inspired to try next.)

Learn more about our two model types here.

What’s annoying about the What-If Tool?

Work in progress

The tool isn’t perfect yet. For example, you’ll occasionally stumble onto something guaranteed to earn a scowl from Tufte fans — for example, the screenshot below had me ranting in a meeting recently. (If you can’t see why, it’s a good opportunity for a little data viz lesson: Why are the text labels “Young” and “Not Young” the only visual cues? Why not shape? Because we’re working on making it better in this way and in others too, but perfection takes time. As part of the collaboration, I rant on your behalf to help these issues should dissipate rapidly.)

Also… how about them axis labels?

Unguided exploration

The tool will go where your curiosity takes it, but what do you do if you’re not feeling creative? Perhaps you wish the tool were more prescriptive, guiding your eye towards what’s important? Your feedback is on our radar and we’re working on it, but for those who think something beautiful might be lost if your exploration gets hemmed in, never fear! We believe in options and understand that not everyone wants the prescriptive side of things, just as not everyone wants to play video games with a fixed storyline as opposed to an open world.

Limited customization

You want every customization under the sun, which is such a data-sciency thing to say. I’ve said things like that too — I remember the first question I asked in a mandatory SAS training for stats PhD students: “How do I write these functions myself so they do exactly what I want?”.

So when you ask the same thing about the What-If Tool, I’ll tell you what my profs told me that day: that’s what raw Python and R are for! (Or, heaven help us, C/C++ if you’re going that far down into the weeds.) Visualization tools like the What-If Tool aren’t replacements, they’re accelerators. They give you a first look with minimal effort so you know where to dig, but once you’ve picked your spot, you’re probably going to want to write your own code to dig exactly the way you like to. If you’re an expert analyst with your own awesome way of doing things, our goal is to help you narrow your search so there’s less code to write later, not to replace your entire approach.

TensorFlow-ish terminology

Another thing that irritates me (and the rest of statistician-kind, I’m sure) is the terminology compromises we had to make for the sake of our TensorFlow user group, keeping some of the TensorFlow legacy lingo that makes traditional data scientists want to punch something. Yeah, that “inference” isn’t inference. TensorFlow is a hilarious bucket of words appropriated and promptly misused — fellow nerds, don’t even get me started on its use of “experiment”, “validation”, “estimator”, or the batch vs minibatch thing… Just let this be a lesson about thinking carefully about what you’re calling things when it’s just you and your buddies bouncing some ideas around in a garage. What if the project is a success and everyone will have to live with your choices? Sigh.

Verdict

All in all, these grumbles are on the petty side. Overall, I really like the What-If Tool and I hope you will too.

See it in action!

While the What-If Tool is not designed for novices (you need to know your way around the basics and it’s best if this isn’t your first rodeo with Python or notebooks), it’s an awesome accelerant for the practicing analyst and ML engineer.

If you’re eager to see the What-If Tool in action, you don’t have to install anything — just go here. We’ve got dazzling demos and docs aplenty. If you want to start using it for realsies, you don’t even need to install TensorFlow. Simply pip install witwidget.

If you’re a fan of Google Cloud Platform, you might be excited by a new integration that just got announced. Now you can connect your AI Platform model to the What-If Tool with just one method call! Check out how here.

Thanks for reading! How about an AI course?

If you had fun here and you’re looking for an applied AI course designed to be fun for beginners and experts alike, here’s one I made for your amusement:

Enjoy the entire course playlist here: bit.ly/machinefriend

Liked the author? Connect with Cassie Kozyrkov

Let’s be friends! You can find me on Twitter, YouTube, Substack, and LinkedIn. Interested in having me speak at your event? Use this form to get in touch.