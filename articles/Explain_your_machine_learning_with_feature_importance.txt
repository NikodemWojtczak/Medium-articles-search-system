Although it is important to be proficient in understanding the inner workings of the algorithm, it is far more essential to be able to communicate the findings to an audience who may not have any theoretical / practical knowledge of machine learning. Just showing that the algorithm predicts well is not enough. You have to attribute the predictions to the elements of the input data that contribute to your accuracy. Thankfully, the random forest implementation of sklearn does give an output called “feature importances” which helps us explain the predictive power of the features in the dataset. But, there are certain drawbacks to this method that we will explore in this post, and an alternative technique to assess the feature importances that overcomes these drawbacks.

The dataset

As we saw in the introduction, we will be using the credit card default dataset from UCI Machine Learning repository, hosted on Kaggle. The dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005, and comprises a good mix of continuous and categorical variables, as shown below:

Columns available in the dataset and their descriptions

The default.payment.next.month is our target variable. A value of 1 indicates that the customer will default the payment in the next month. Let us look at the distribution of this variable:

credit.groupby([‘default.payment.next.month’]).size()

>default.payment.next.month

>0 23364

>1 6636

>dtype: int64

This is an imbalanced dataset with ~22% of the records with default value of 1. Since our focus is on evaluating the feature importances, I will not go into examining the dataset and its features in this post. Rather, I will quickly set up a model and start looking at the scores. Let us first one-hot encode the categorical variables before setting up a model.

Let us see how a plain random forest model performs on this dataset:

Clearly we have an algorithm that is overfitting. The accuracy on the test set is lower than the training set, but not too bad. Since we are dealing with an imbalanced dataset though, the precision and recall numbers are more important. And there is a huge difference in the precision and recall between the training and test sets.

Let us examine the top feature importances behind this model: