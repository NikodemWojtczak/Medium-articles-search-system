Introduction

Convolution neural networks have been a great success in visual recognition tasks. This article particularly focuses on semantic segmentation. The logic behind using CNN is that images have a sense of locality that is, the pixels which are near to each other are more related. CNNs are able to capture this by convolution operations and the local region that comes into consideration (formally known as receptive field) depends on the kernel size. There are also long-range dependencies in an image which can help in visual recognition tasks. For that the concept is to stack may convolution layers which will theoretically increase the receptive field. So now both long range and short range dependencies are taken into account, put the network on the training and you get the results easy right!

But I am sure you are well aware that not many times things match in theory and practical. Recently a paper ( by Luo, Wenjie, et al.) was published which showed that receptive fields do not grow linearly with the number of convolution layers moreover they are severely limited. Furthermore, the receptive field depends on various other factors such as initialization schemes. So what is the solution?

Graphs come to rescue

What if we are able to group the image into regions and use those regions to extract features and further process the image. Along with that, there will also be dependencies between regions which will help in getting long-range dependencies. All this can be achieved using graphical representation of images.

Broadly speaking all the vertices in a graph will represent different regions in the image. The edges will represent the relation or similarity between regions.

The circle represents the regions which have a corresponding feature vector representing that region. The edges are similarities between the regions.

Semantic Segmentation

Semantic segmentation is the task of assigning each pixel a class to which it belongs. A basic technique is that there will be an encoder followed by a decoder whose output will be an assignment at the pixel level.

How does segmentation work? Let us take a very basic example, suppose there is an image which only consists of colors red, blue, green as shown in

Fig1

Fig1 and assume these colors represent some objects. Now suppose who want to segment the image into the three objects represented by the colors. What feature vectors will you need to do this? Right for this only colors are sufficient. If we say in terms of feature vector then each feature vector is of depth 3; for red [0,0,1], for blue [1,0,0], for green [0, 1, 0]. As you can see these features are enough to segment the image into 3 regions. Now consider a real-life image such the one given in Fig2. Will you be able to accomplish the task by using only colors? No, right. We need a feature representation of each object present in the image that separates it from other objects and brings similar objects closer, this is where encoder comes into the picture. It converts the color image representation to some latent space representation in which the features of the same object are closer and that of distinct objects are far away from each other. Obviously, the encoder needs to be trained. Once we have features in some latent space which separates distinct objects we need to propagate the information to pixel level and this where decoder comes into play.

Let us understand how graphs can be useful in semantic segmentation. Graphs come into play once the feature vectors are extracted using an encoder. Consider the images in Fig2. Images are in order actual image, ground truth, segmentation using FCN, segmentation using GCU respectively from top to bottom. In this image, the probability of pillows and bed class comes out to be very close when FCN is used. Hence you can see in Fig2 pillow and bed are merged.

Now suppose a graphical representation follows an encoder. The feature vector representation of the image that results from the encoder is forced to divide into as many as 8 regions (i.e. graph of 8 vertices) ] if I say in terms of loss the loss is higher if a vertex is empty i.e. no pixels are assigned to it. In the initial stages of training in this case also pillow and bed will be assigned to the same class. But as we train the network further, each feature vector is assigned to a vertex i.e. no vertex is empty. The probable regions in which the image will be segmented in graph representation are the regions which are in the ground truth. Due to which the pillow and bed will be segmented into two different regions by the end of the training. The features assigned to a vertex are processed further by multiplying with some weights and the resulting feature vector along with the feature vector from the encoder are further used for segmentation. Thus the graphical representation further improved the feature representation of the image.

One obvious thing that can happen is over-segmentation of the image when dividing it into regions. But the oversegmented regions will recombine after further operations like convolutions.

Now the question is how to convert the grid-based representation of the image to this graphical representation and learn the parameters of graphical representation and the answer is graph convolution units(GCU).

Graph Convolution Units (GCU)

Just like convolution operate on grid-like structures GCU operates on a graph-like structure. There are 3 major steps in GCU

Graph Projection: In this step, the grid-like representation of an image is converted to the graphical representation. The graph is parameterized by the following parameters:

V: Number of the vertex in the graph which implies the number of regions in which the image will be segmented.

W: Feature vectors which represent the region. The shape is (d, V), where d is the dimension of feature vectors

Variance: It is the variance along each dimension across all the pixels assigned to a particular vertex. The shape is (d, V), where d is the dimension of the feature vector.

V is a fixed and W and variance are learned during training. Suppose there is a 2-D feature map of the image with height H and width W and each element has a dimension d. Probability of each feature vector belonging to each vertex is calculated, which results in a probability matrix Q. The equation below is used to calculate probabilities:

where xᵢⱼ is the feature vector of the iᵗʰ row and jᵗʰ column of the 2-D feature map, wₖ is feature representing kᵗʰ region (vertex) and σₖ is variance along all dimensions of vertex k. Now, feature encoding of all the vertices is calculated by taking the weighted average of residuals. More is the residual less is its contribution in calculating the encoded feature. Equations given below are used:

where zₖ is the encoded feature. The adjacency matrix is calculated by ZᵀZ which gives cosine similarity between different vertex. In total 3 things are calculated in this step

Probability matrix Q, shape ( HW, d)

Encoded feature Z, shape ( d, V)

Adjacency matrix A, shape ( V, V)- A representation of similarity between the regions thus it captures the long-range dependencies in an image.

2. Graph Convolution: This step is analogous to forward step of convolution i.e. convolution is done on the generated graph. The equation given below is used:

where W g is the weight matrix of shape (d, dₒᵤₜ). As you can see there is an Adjacency matrix, A, in the equation, long-range dependencies are considered when calculating new encoded features. So the new encoded features depend on all the regions(A)and the current encoded features (Z). For reading about graph convolution refer to this article and this. A very easy to understand and just enough information is given in these articles.

3. Graph Re-projection: Finally, the graph is converted back to the grid-like structure to visualize or do further operations. The equation is given below

Architecture and Implementation

The architecture used is pretrained ResNet 50/101, dilations are added to the last two layers thus output is downsampled by 8. It is followed by GCU. In the original implementation, outputs of 4 GCUs are concatenated to the output of ResNet as shown in the diagram below.

d is 1024 and dₒᵤₜ is 256 in this case. The depth of the output after concatenating will be 1024( from ResNet50) + 256x4 = 2048. The concatenated output is upsampled using bilinear interpolation. Next, convolution layers are used to assign pixels to different classes. The loss function used to minimize the error is Negative log-likelihood loss function.

Fig3 Training loss curve

Pytorch implementation by me is available here. I have given the implementation details below. Dataset used is ADE20K.

ResNet50 dilated is used, pretrained on ADE20K available here. The depth of output of ResNet50 is 2048.

GCUs follow the ResNet50. In the paper, 4 GCU units were concatenated but I have used only 1 GCU with 16 vertices due to limited computing power. I have written a generalized code so you can easily modify the code for 4 GCUs. For knowing in more detail about GCU implementation refer to my next post.

d is 2048 and dₒᵤₜ is 256 in this case. The depth of the output after concatenating will be 2048( from ResNet50) + 256= 2304

It is followed by bilinear upsampling operation and then 1 convolution layer.

Images are resized to the size of 512×512 before feeding to the network.

Due to limited computation, I have used the batch size of 1 and trained for 120 epochs each having 1000 iterations.

SGD was used with a momentum of 0.9. The learning rate starts with 0.01 and it decays as the training proceeds

Currently, the model is using 2 GPUs. One GPU is dedicated to ResNet and other is for all the other computations like GCU, upsampling and convolutions.

Training loss plot is shown in Fig3. After training with given hyperparameters, training accuracy was about 78%.

This post is based on the paper by Yin Li and Abhinav Gupta.

Any doubts or suggestions feel free to ping me 😃. Also, find me on Twitter and Linkedin. Adios!!