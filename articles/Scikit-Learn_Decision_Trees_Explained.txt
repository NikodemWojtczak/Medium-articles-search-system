Decision trees are the most important elements of a Random Forest. They are capable of fitting complex data sets while allowing the user to see how a decision was taken. While searching the web I was unable to find one clear article that could easily describe them, so here I am writing about what I have learned so far. It’s important to note, a single decision tree is not a very good predictor; however, by creating an ensemble of them (a forest) and collecting their predictions, one of the most powerful machine learning tools can be obtained — the so called Random Forest.

Make sure you have installed pandas and scikit-learn on your machine. If you haven't, you can learn how to do so here.

A Scikit-Learn Decision Tree

Let’s start by creating decision tree using the iris flower data set. The iris data set contains four features, three classes of flowers, and 150 samples.

Features: sepal length (cm), sepal width (cm), petal length (cm), petal width (cm)

Classes: setosa, versicolor, virginica

Numerically, setosa flowers are identified by zero, versicolor by one, and virginica by two.

For simplicity, we will train our decision tree using all features and setting the depth to two.