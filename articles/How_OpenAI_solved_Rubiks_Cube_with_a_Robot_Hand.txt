If you haven’t seen the video. Check it out.

Reinforcement learning has seen a lot of success over the past years. We have seen AIs beat pro players in Go, Dota, and Starcraft all by using reinforcement learning. The former go champion Lee Se-dol even quit the game altogether after his loss against the superior AI.

The Problem with Reinforcement Learning

But Reinforcement Learning has a huge Problem — its success is limited to virtual environments. No AI manages to navigate the real world as a human does. Even a 2 year old is better at this than our most sophisticated AI’s. Of course, it is a really complicated problem — it took evolution 4 Billion years to create humans and I still manage to look for my keys for ten minutes just to realize they were in my pockets all along so it's not like intelligent life has reached its peaks.

Nonetheless, humans somehow manage to learn complicated tasks without dying. If humans would learn exactly as our algorithms do, they would have to drive off a cliff thousands of times before realizing that staying on the road may not be such a bad idea, in the first place.

And it gets worse, these algorithms are so data-hungry that a human lifespan would not be enough time to learn even a handful of somewhat difficult tasks. The Dota AI, for example, played 40.000 years of the game before it was able to beat a Pro. Still impressive of course, but speeding up time just works better in virtual environments.

So how did OpenAI manage to control a physical arm using Reinforcement Learning without using the benefits of a virtual environment?

Well, they didn’t. They used a virtual simulation. But can a Simulation be so accurate that its results can just be transferred to the real world? OpenAI concluded no, the real world is too complicated and things like friction and elasticity are too hard to accurately measure and simulate.