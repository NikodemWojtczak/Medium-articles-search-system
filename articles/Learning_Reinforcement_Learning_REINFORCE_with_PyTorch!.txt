Learning Reinforcement Learning: REINFORCE with PyTorch!

Photo by Nikita Vantorin on Unsplash

The REINFORCE algorithm is one of the first policy gradient algorithms in reinforcement learning and a great jumping off point to get into more advanced approaches. Policy gradients are different than Q-value algorithms because PG’s try to learn a parameterized policy instead of estimating Q-values of state-action pairs. So the policy output is represented as a probability distribution over actions rather than a set of Q-value estimates. If any of this is confusing or unclear, don’t worry, we’ll break it down step-by-step!

TL;DR

In this post, we’ll look at the REINFORCE algorithm and test it using OpenAI’s CartPole environment with PyTorch. We assume a basic understanding of reinforcement learning, so if you don’t know what states, actions, environments and the like mean, check out some of the links to other articles here or the simple primer on the topic here.

Policies vs. Action Values

We can distinguish policy gradient algorithms from Q-value approaches (e.g. Deep Q-Networks) in that policy gradients make action selection without reference to the action values. Some policy gradients learn an estimate of values to help find a better policy, but this value estimate isn’t required to select an action. The output of a DQN is going to be a vector of value estimates while the output of the policy gradient is going to be a probability distribution over actions.

For example, consider we have two networks, a policy network and a DQN network that have learned the CartPole task with two actions (left and right). If we pass a state s to each, we might get the following from the DQN:

And this from the policy gradient:

The DQN gives us estimates of the discounted future rewards of the state and we make our selection based on these values (typically taking the maximum value according to some ϵ-greedy rule). The policy gradient, on the other hand, gives us probabilities of our actions. The way we make our selection, in this case, is by choosing action 0 28% of the time and action 1 72% of the time. These probabilities will change as the network gains more experience.