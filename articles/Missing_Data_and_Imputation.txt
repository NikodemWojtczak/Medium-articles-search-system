Missing data can skew findings, increase computational expense, and frustrate researchers. In recent years, dealing with missing data has become more prevalent in fields like biological and life sciences, as we are seeing very direct consequences of mismanaged null values¹. In response, there are more diverse methods for handling missing data emerging.

This is great for increasing the effectiveness of studies, and a bit tricky for aspiring and active data scientists keep up with. This blog post will introduce you to a few helpful concepts in dealing with missing data, and get you started with some tangible ways to clean up your data in Python that you can try out today.

Photo by Carlos Muza on Unsplash

Why do anything at all?

You may be asking yourself — why do I need to deal with missing data at all? Why not let sleeping dogs lie? Well, first of all, missing values (termed NaN, Null or NA) cause computational challenges because. Think about it — if you’re trying to sum up a column of values and find a missing one, what is 5 + NA? If we don’t know the second term in the equation, our outcome is itself NA. So we really can’t derive anything meaningful from missing values, plus it confuses most programs that expect to be handling non-empty cases.

Aside from this, there are three main problems that missing data causes:

Bias More laborious processing Reduced efficiency in outcomes

These are all pretty serious (if not just irritating) side effects of missing data, so we’ll want to find something to do with our empty cells. That’s where “imputation” comes in.

Imputation

Webster’s Dictionary shares a “financial” definition of the term imputation, which is “the assignment of a value to something by inference from the value of the products or processes to which it contributes.” This is definitely what we want to think of here — how can we infer the value that is closest to the true value that is missing?

As an aside— it is interesting to reflect on and consider that this term is likely derived from its theological context. Here, it means “the action or process of ascribing righteousness, guilt, etc. to someone by virtue of a similar quality in another,” as in “the writings of the apostles tell us that imputation of the righteousness of Christ is given to us if we receive Christ.” Just some food for thought as we move along.

Missing Data Mechanisms

When researching imputation, you will likely find that there are different reasons for data to be missing. These reasons are given terms based on their relationship between the missing data mechanism and the missing and observed values. They help us unlock the appropriate data handling method, so they’re really helpful to have a basic understanding of. Below are 3 of the 4 most typical, and you can read more about them on “The Analysis Factor” .

Missing Completely at Random (MCAR)

This one may be the easiest to think about — in this instance, data goes missing at a completely consistent rate. For example, a dataset that lacks 5% of responses from a youth survey. This is because 5% of all students were out sick the day that the survey was administered, so the values are missing at a consistent rate across the entire data set.

2. Missing at Random (MAR)

Despite the name similarities, MAR values are a bit more complex — and more likely to find than MCAR. These are instances that data the rate of missing data can be perfectly explained if we know another variable. For example, imagine the above dataset lacks 10% of responses from girls and 5% of responses from boys. This is because the illness spread at the school was 2x more likely to affect young women than young men. This gets more complex, and more realistic, as multiple variables influence the rate of missing values in a dataset.

3. Missing Not at Random (MNAR)

In this case, the missing-ness of a certain value depends on the true value itself. This one is pretty cyclic, but I like the example given in this video of rates of missing values in a survey of library-goes that collects their names and number of un-returned library books. As the number of hoarded books increases, so does the percentage of missing values from this survey question. The problem with this one is that because the value missing is dependent on the value itself, we have a very difficult time deriving the rate it is missing.

Practical Exploration and Visualization in Python

When dealing with data in Python, Pandas is a powerful data management library to organize and manipulate datasets. It derives some of its terminology from R, and it is built on the numpy package. As such, it has some confusing aspects that are worth pointing out in relation to missing data management.

The two built-in functions, pandas.DataFrame.isna() and pandas.DataFrame.isnull() actually do exactly the same thing! Even their docs are identical. You can even confirm this in pandas’ code.

This is because pandas’ DataFrames are based on R’s DataFrames. In R na and null are two separate things. Read this post for more information. However, in python, pandas is built on top of numpy, which has neither na nor null values. Instead, numpy has NaN values (which stands for “Not a Number”). Consequently, pandas also uses NaN values².

Additionally, the Python package named missingno is a very flexible, missing data visualization tool built with matplotlib, and it works with any pandas DataFrame. Just pip install missingno to get started, and check out this Github repo to learn more.

A “missingno” visualization of cyclist dataset — with Sparkline on the side

Adequately visualizing your missing data is a great first step in understanding which missing data mechanism you are handling, along with the scale of missing data and hot spots to work with.

Methods for Single Imputation:

Starting from the simplest and moving toward more complex, below are descriptions of some of the most common ways to handle missing values and their associated pros and cons.

(Note that one item or row in a dataset is referred to as an “observation.”)

Row (Listwise) Deletion: Get rid of the entire observation.

Simple, but can introduce a lot of bias.

An example of listwise deletion

2. Mean/Median/Mode Imputation: For all observations that are non-missing, calculate the mean, median or mode of the observed values for that variable, and fill in the missing values with it. Context & spread of data are necessary pieces of information to determine which descriptor to use.

Ok to use if missing data is less than 3%, otherwise introduces too much bias and artificially lowers variability of data

3. Hot or Cold Deck Imputation

“Hot Deck Imputation” : Find all the sample subjects who are similar on other variables, then randomly choose one of their values to fill in.

Good because constrained by pre-existing values, but the randomness introduces hidden variability and is computationally expensive

“Cold Deck Imputation” : Systematically choose the value from an individual who has similar values on other variables (e.g. the third item of each collection). This option removes randomness of hot deck imputation.

Positively constrained by pre-existing values, but the randomness introduces hidden variability and is computationally expensive

Example of basic hot deck imputation using mean values

4. Regression imputations

“Regression Imputation” : Fill in with the predicted value obtained by regressing the missing variable on other variables; instead of just taking the mean, you’re taking the predicted value, based on other variables.

Preserves relationships among variables involved in the imputation model, but not variability around predicted values.

“Stochastic regression imputation” : The predicted value from a regression, plus a random residual value.

This has all the advantages of regression imputation but adds in the advantages of the random component.

Challenges of Single Imputation

These are all great methods for handling missing values, but they do include unaccounted-for changes in standard error. Again, “The Analysis Factor” explains this trade-off perfectly below:

“Since the imputed observations are themselves estimates, their values have corresponding random error. But when you put in that estimate as a data point, your software doesn’t know that. So it overlooks the extra source of error, resulting in too-small standard errors and too-small p-values.”

Additionally, values found in single imputation might be biased by the specific values in the current data set, and not represent the total values of the full population. So how do we reduce the impact of these two challenges?

Multiple Imputation

Multiple imputation was a huge breakthrough in statistics about 20 years ago because it solved a lot of these problems with missing data (though, unfortunately not all). If done well, it leads to unbiased parameter estimates and accurate standard errors.

While single imputation gives us a single value for the missing observation’s variable, multiple imputation gives us (you guessed it) multiple values for the missing observation’s variable and then averages them for the final value.

To get each of these averages, a multiple imputation method would run analyses with 5–10 unique samples of the dataset and run the same predictive analysis on each**. The predicted value at that point would serve as the value for that run; the data signature of these samples change each time, which causes the prediction to be a bit different. The more times you do this, the less biased the outcome will be.

Once you take the mean of these values, it is important to analyze their spread. If they’re clustering, they have a low standard deviation. If they’re not, variability is high and may be a sign that the value prediction may be less reliable.

While this method is much more unbiased, it is also more complicated and requires more computational time and energy.

Conclusion

In closing, we looked at:

The importance of handing missing values in a data set The meaning (and root) of “imputation” Different reasons that data could be missing (missing data mechanisms) Ways to explore and visualize your missing data in Python Methods of single imputation An explanation of multiple imputation

But this is just a beginning! Please look into the linked resources on this post, and beyond, for further information on this topic.