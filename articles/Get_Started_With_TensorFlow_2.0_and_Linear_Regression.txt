TensorFlow 2.0 has been a major breakthrough in the TensorFlow family. It’s completely new and refurbished and also less creepy! We’ll create a simple Linear Regression model in TensorFlow 2.0 to explore some new changes. So, open up your code editors and let’s get started!

Also, open up this notebook for an interactive learning experience.

Attention! TensorFlow 2.0 is now available in the stable channel!

For viewing some basic concepts and their simpler explanations see,

Let’s Fight with the Math First.

First impressions for Calculus

We’ll get some information on what Linear Regression at the first instance. In Linear Regression, we tend to find a best-fit line for your data.

The line could be defined in its slope-intercept form as:

The hypothesis function

m and c are the slope and y-intercept respectively. Where W and b are the weights and biases respectively for the 2ⁿᵈ equation.

To optimize our parameters w and b, we require a loss function. That’s where Mean Squared Error ( L1 / MSE ) comes in the picture.

Mean Squared Error

Where N is the number of samples in the batch/dataset, y is the predicted outcome whereas y⁻ ( y-hat ) is the target outcome.

Also, we will need the derivative of the Mean Squared Error function which is as follows:

The derivative of the Mean Squared Error function

Where N is the number of samples in the batch/dataset, y is the predicted outcome whereas y⁻ ( y-hat ) is the target outcome.

Now, enters Gradient Descent through which we will update our parameters θ.

Gradient Descent update rule

Where θ is our parameter, α is the learning rate or step size and loss is our loss function.

We optimize both w and b by obtaining their partial derivatives with respect to the loss function ( MSE ).

Partial derivatives w.r.t loss function

Where w and b are the parameters of optimization, h is the hypothesis function, loss is the loss function and MSE’ is the derivative of the Mean Squared Error loss function.

Dive into code. Get some Data!

TensorFlow 2.0!

We are going to use the data from Graduate Admissions on Kaggle.com. It contains 6 continuous and 1 binary feature making a total of 7 features. The label or the expected outcome is the chance of admission for a student. This is our target variable.

We’ll download and parse the dataset into something which we really love train and validations datasets!

Creating the model in TF 2.0

We define 3 methods with TensorFlow’s low-level APIs for :

Mean Squared Error function The derivative of Mean Squared Error function Hypothesis function/ Regression function

which we have discussed earlier in raw Math.

Then, we initialize some hyperparameters for training and also create tf.data.Dataset object to store and batch our data efficiently.

Noticed a TF 2.0 change? Earlier for TF 1.x versions, we used tf.data.Dataset.make_one_shot_iterator() method for creating an iterator. That’s changed and now we use tf.data.Dataset.__iter__()

Finally, we train the model for num_epochs epochs with a batch size of batch_size which makes a step size of num_samples/batch_size .

Changes: We do not need to run the ops and tensors via a tf.Session() object. TensorFlow 2.0 has Eager Execution enabled by default. To get the value of a tf.Tensor we only use the tf.Tensor.numpy() method.

Also, we can get a plot of epoch-loss using matplotlib.pyplt using,

import matplotlib.pyplot as plt plt.plot( epochs_plot , loss_plot )

plt.show()

And now for evaluating our model’s accuracy, we measure the Mean Absolute Error.

Changes: The tf.metrics now return an op ( operation ) instead of a Tensor. That’s good though!

Wait, there’s more!

That’s All.

ope that was a good introduction to TensorFlow 2.0 and linear regression. Thank You and happy Machine Learning!