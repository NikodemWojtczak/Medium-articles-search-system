Expertise and Inference

Inference is defined as ‘a conclusion on the basis of evidence and reasoning’. Many, or most things we do in life are based on inference in some form or the other. Some, we have a lot of experience with, such as knowing it’s quite safe to carefully cross the street, whereas others, we do not, such as whether to take a certain job offer, or move to a different country. In both cases, though, we eventually have to make some decision, largely based on conclusions drawn from the (often limited) information we have.

Case Study: Guyanese Drinking

If you are here to decide which country to move to, you have come to the wrong place, as the purpose of this post is not to study inference through philosophy, but from the restricted and predictable world of mathematics. Let us say you are trying to estimate the probability that a randomly selected Guyanese person is drinking on a Friday night. For the purposes of this thought experiment, I am assuming that you know nothing about Guyana or the drinking habits of its population. If you do have some knowledge in this area, please bear with me, and imagine a world where you do not.

In the absence of any information, what should we assume the probability should be? Let us further ignore our perceptions of the likelihood of drinking or on Fridays. Without information, our best ‘guess’ for the probability would be 50%. But what if we asked one random Guyanese person, and they were drinking? What if we found two more Guyanese people who weren’t? The way I would suggest to explore this question, is to use Bayes’ theorem. It is a powerful way of updating your ‘views’ of the world based on new information.

Coding a solution

I imported the math and numpy Python modules to help me out:

import numpy as np

import math as math

Since we have no information about these habits, we can assume for now that each probability between 0 and 100% is equally likely, therefore, we should give them all equal weights:

probs= np.linspace(0,1,101)

weights= np.ones(len(probs))

Plausibility

But how do we process information when it starts coming in? When we see the first person, who is drinking, we know that the probability can not be 0%. It is also a result that we see 9 times as often with a probability of 90% as opposed to a probability of 10%. Generalising this intuition, in line with Bayes’ theorem, each probability is as ‘plausibility score’ as its prior weight multiplied by how likely ‘reality’ is to occur with that probability. It is impossible for the actual probability to be 0%, so it is given a ‘plausibility score’ of 0.

How do we see how likely a certain outcome is under a certain assumption? In our simple model, we use what’s called the binomial distribution. I used a helper function called ‘f’ to make the binomial function look prettier:

def f(num):

return math.factorial(num) def binomial_prob(n,x,p):

return (f(n)/ (f(n-x)*f(x) ) * p**(x) * (1-p)**(n-x))

We combine all of this to create our Bayesian probability estimating function. We average out the probabilities using the weights of the plausibilities. We set default values of the probabilities and prior likelihood, to make the user’s job easier:

def bayes_prob_est(n,x, probs=np.linspace(0,1,101), priors='q')):

if type(probs)==list:

probs=np.array(probs)







if len(probs)!= len(priors): priors= np.ones(len(probs)) if type(priors)== list:

priors== np.array(priors)

plausability = priors* binomial_prob(n,x,probs)



return sum(plausability*probs)/sum(plausability)

Conclusions from Guyanese experiment

Now that we have our Bayesian function ready, we are finally ready to make some estimates. Below is a table showing the number of people surveyed, number who were drinking, and likelihood a random person will be drinking, using our inference method:

The interesting pattern that emerges from this is that our best guess for probability is (x+1)/(n+2), which is the logical equivalent of adding one drinking person and one non drinking person to our sample. This is a special form of Laplace smoothing

Case Study: Poker

Small Sample

Does this mean inference is as simple as (x+1)/(n+2)? Does this mean all the expertise in the world can be replaced with (x+1)/(n+2)? You’ll be unsurprised to hear that the answer is no. That sounds like common sense, but mathematics does not work purely through common sense, it requires logical evidence. Let us go over a different example to highlight the value of expertise.

Let us say we are playing a poker game in a casino, at a table of 9 people. A new player arrives at the table, and plays his first 3 hands. What % of hands is he likely playing? Using our Bayesian function, or the above table, we get the following:

bayes_prob_est(3,3) #0.80

So should we assume that he plays roughly 80% of hands? With absolutely no knowledge of poker, yes, this is a fair assumption. However, earlier we defined a prior function and decided that each probability is equally likely. Any poker player will tell you that 80% of hands is an obscene number to play. Let us alternatively assume that a poker expert thinks that 10% of players are reckless and play 60% of hands, 80% of players are reasonable and play 20%, and 10% of players are scared and play 10%. If we now put this information into our Bayesian function, we get the following result:

bayes_prob_est(3,3, [.6, .2, .1], [10,80,10]) #0.51

Therefore, the expert can adjust his probability from 80% to roughly 51% based on his expertise. That is a massive difference of opinion, which makes sense, as complicated games like poker are extremely difficult to understand without much experience. Let us see what happens if we observe the same player for 200 hands, but now he has played 45 hands out of 200.

Big Sample

Non expert:

bayes_prob_est(200,45) #0.23

Expert:

bayes_prob_est(200,45, [.6, .2, .1], [10,80,10]) #0.20

In this case, the non-expert infers a probability of around 23%, whereas the expert infers one of around 20%. The gap between the two has closed significantly, and is almost identical. With bigger samples these values keep moving closer together.

Conclusion: Value of Expertise

We have seen above that prior expertise can give you a vastly different perception of the world in the face of limited information, however, this converges to the perception of a naive but perceptive observer after ‘enough’ time. One must wonder, then, why expertise is required at all in this era where we can get thousands or even millions of data points on various topics, when opinions seem to converge after just a few hundred observations.

Partly, this is true, in the face of overwhelming data, prior opinions do become less important. However, many important problems are not as simple as estimating the likelihood of one simple type of outcome. Real life has a bunch of inputs, for example, ascertaining the likely frequency of different types of crime in a neighbourhood given 50 different socio-economic statistics.

Finally, even if we are able to pull probabilities out of the data, we must also know how to use them. These probabilities at best give us correlations, but to change outcomes in the world (the purpose of studying data), we need to understand causes, and this, especially, requires expertise. Therefore, while data has reduced the value of certain forms of expertise, other forms are still valuable in this data-driven world.