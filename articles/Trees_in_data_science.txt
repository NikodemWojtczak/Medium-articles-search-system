Trees in data science

One of the most easy to interpret models in machine learning are CART’s(Classification and Regression Trees) known popularly as decision trees. In this post I wish to give an overview of Decision Trees, some primary concepts surrounding them and finally Random Forests. The contents are as follows

Understanding Decision Trees

Purity

Bootstrapping and Bagging

Random Forests

Lets Go!

Decision Trees

Basic structure of a Decision Tree (Source: cway-quan)

In the machine learning universe trees are actually upside down versions of real trees. Suppose we have a dataset consisting of our features ‘X’ and a target ‘Y’. What a decision tree does is that it finds patterns within X and splits the dataset into smaller subsets based on these patterns.

Visualize these splits in the slightly simplified image above. ‘Y’ here is whether or not a job offer is to be accepted. The ‘X’ contains features like “commute_time”, “salary”, “free_coffee”.

Based on patterns in ‘X’ the tree is split into branches until it reaches a point where it arrives at a pure answer to ‘Y’. In our scenario, job offers which are accepted have to provide a salary > 50k, commute time < 1hr and free coffee. In this manner the tree reaches the last leaf which is a pure decision about ‘Y’.

Purity in decision Trees

Decision trees conduct splitting based on the purity of the node. This purity is measured based on the distribution of ‘Y’. If our ‘Y’ is continuous our problem is a regression problem and the nodes are split based on MSE(Mean Squared Error). If ‘Y’ is discrete, our model is dealing with a classification problem and a different measure of purity is required.

A widely used metric to measure in classification cases is Gini Impurity. The formula for Gini impurity is given as follows:

Source: General Assembly DSI curriculum (Authors:David Yerrington, Matt Brems)

While deciding which split to make at a given node, it picks the split that maximizes the drop in Gini impurity from the parent node to the child node.

Bootstrapping and Bagging

To understand bootstrapping and bagging, the first step would be to understand why they are needed in the first place. It is basically trying to emulate the “wisdom of the crowd” principle where in the aggregated result of multiple models is better than the results of a single model. The following image by Lorna Yen gives a great idea about boot strapping.

(Author: Lorna yen, Source)

Bootstrapping as shown above is just the random sampling of data with replacement. Bagging is just a process of building decision trees on each of these samples and getting an aggregate prediction. So to summarize Bagging involves the following steps:

From the original data of size n, bootstrap k samples of size n with replacement Build a decision tree on each bootstrapped sample. Pass test data through all trees and develop one aggregate prediction

Bagging is therefore also called Bootstrapped Aggregating.

Random Forest Model

A closer look at the below image gives a basic intuition on random forests.

A basic hurdle in bagging is that the individual decision trees are highly correlated as the same features are used in all trees. So the predictions of our models suffer from the issue of variance. To know more on variance or bias you can read this link. Decorrelating our models is a solution and is exactly what Random Forests do.

Random forests follow similar steps in bagging except that they use at each split in the learning process, a random subset of the features. This mitigates the variance problem in bagging and generally produces much better results. This efficient and simple methodology has made Random forests a widely implemented Machine learning Model.

Bonus

Code for importing the explained three classification models in sklearn.