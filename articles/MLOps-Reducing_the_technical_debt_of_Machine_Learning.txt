MLOps-Reducing the technical debt of Machine Learning

Building an MLOps system using Opensource Tools Saurav Chakravorty · Follow Published in MLOps.community · 5 min read · Dec 20, 2019 -- 1 Listen Share

Photo by John Barkiple on Unsplash

MLOps is growing in popularity as teams try to move Machine Learning (ML)Code to production. I have been influenced by two articles on this topic. This article by Kyle Gallatin provides an overview of the challenges that lie in the interface of Data Science and Production Engineering. The second article is a fairly nuanced review of the processes and tools required for an MLOps system. I would recommend reading these articles in case this is your first introduction to this topic.

We are trying to solve for 5 high-level challenges that any ML project faces —

Reproducible Model Training Reliable model serving Data (training data) Discoverability and Accessibility (I am not addressing this point in this post) Orchestration (note that this is a need for any DevOps process) Testing and quality control for new and live models

In this post, I will set up a system that implements the main features of an MLOps system on a cloud platform. In the next post, I will develop the typical use cases and demonstrate how all these components work together to streamline the MLOps process.

We will be using only Open Source components for developing this MLOps system and all the components below are cloud-agnostic. The below implementation is on Azure but should be fairly easy to port to AWS or GCP.

GoCD for CI/CD (https://www.gocd.org/) MLFlow for ML project tracking (https://mlflow.org/) Git/GitHub for code tracking (note: GitHub is not Opensource) DVC for data tracking (https://dvc.org/) Docker and Dockerhub for creating and managing Docker images Finally, a Kubernetes service to orchestrate the containers

Typical problems and tools that are relevant to MLOps

Setup ML Flow Server

Tech stack for MLOps

As the above diagram shows, we have to set up 4 Virtual Machines for GoCD, 1 for ML Flow and configure Azure Blob, DVC, Kubernetes and GitHub.

ML Server can be installed locally by ‘pip install’. However, as we need to create a central repository for all ML projects in the organization we will create a central ML Flow server and we will use the server URI for tracking all ML experiments. The instruction is available at this link. The command to start the server is shown below. But before we execute this command we have to set up a container on Azure to store out model artifacts and setup AZURE_STORAGE_ACCESS_KEY as an environment variable on the ML Flow server.

Setup ML Flow server

If everything is set up correctly the ML tracking service should be available on port 5000. Henceforth this URI will be used for tracking all ML Projects in the organization.

Setup GoCD server and Agents

The second setup is that of GoCD and its agents. The instructions for the server install is here, and the agent install is here. I installed both agents as directly on Ubuntu VMs but one may choose to use GoCD dockers as well. Note that the user ‘go’ will need some additional privileges to install software during the build. If everything is done correctly, The GoCD server should run on the VM and the configuration tool webpage should be available on the ports 8153/8154. As shown in the screenshot below, the available agents should automatically show up in the agent section of the web page.

Setup Dockerhub, GitHub and DVC

Fortunately, there is no setup required for Dockerhub, GitHub and DVC as they are all available as service. Having said that these tools are not free, so any image pushed to Dockerhub will automatically get published if we don’t use the paid version. As an alternative, we can push our images to Azure Container Repository. The same goes for GitHub, as the projects on the free version of the repo are always public.

We will demonstrate the use and configuration of all these tools in the next post where we demonstrate various use-cases.

Setup Kubernetes (Azure Kubernetes Service)

Though Kubernetes is opensource, managing the cluster is a complex process. I am sure that many readers of this blog know how to self manage Kubernetes. I chose to use Azure Kubernetes Service as the managed platform for Kubernetes. Azure makes it pretty easy to set up the service. Please refer to the instructions here.

Conclusion

This concludes the basic setup of the software, tools, and services required to run out MLOps system. In the next blog, I am going to demonstrate the following use cases for this system.

Data Scientist starts an experiment checking and iterating until she is happy with the results. Lead Data Scientist reviews the experiments and staged one of the models for production deployment. The CI pipeline picks up this new model file, creating and registering a docker image of the model file on DockerHub. The nightly CI/CD pipeline pulls the docker image and runs the container pods with the NEW model.

Notes