An article published earlier this year entitled 4 Ways AI Education and Ethics Will Disrupt Society in 2019 has caught our eye. There is an intense debate regarding AI education and its role in society. Funding for AI-related training and education is on the rise. But is this the right way to do things?

Sure, you can also increase funding for something, and AI education is no different. However, is this the right path? Are we actually doing AI education correctly?

We’re not so sure.

Where Does It Start?

How do we approach AI education in a way that is both clear and accessible for people from all walks of society? It starts with de-stigmatizing AI. That can be difficult, given the portrayal of artificial intelligence in the media and entertainment landscape. Therein lies our first problem.

“Three billion human lives ended on August 29th, 1997. The survivors of the nuclear fire called the war Judgment Day. They lived only to face a new nightmare: the war against the machines.” -Terminator 2: Judgement Day

The above quote from one of the most popular sci-fi films of the 90’s is just one of hundreds, if not thousands of examples of “scary AI.” This dystopian view of AI didn’t happen overnight. It’s a long, continuous theme in AI-centric media, including video games, movies, music, and more.

Rarely, if ever, do movies with AI (or robots) as a central theme not, in some way, focus on the “war against the machines.” From Metropolis (1927) to 2001: A Space Odyssey (1968). From Terminator (1984) to Terminator 2 (1991); the vision of a future overrun, disrupted by, or at odds with man-made machines is common.

Any why shouldn’t it be? A movie about happy robots who love their creators and don’t have a care in the world would hardly make for an entertaining film. A more realistic AI film would portray complex machines charting weather patterns or improving eCommerce shopping cart efficiency. That hardly has “Summer Blockbuster” written all over it either.

This negative portrayal of AI has contributed to the overall confusion, uncertainty, and fear over AI, robotics, and by extension; machine learning and truthfully; big tech in general.

Dispelling the Magic Factor

Is AI education the problem? Sure. However, there is an inherent problem with educating the general public on the “what is” and “what is not” of AI. As a result, large swaths of society largely view AI as a form of technological magic. And magic is hard to trust.

It’s no surprise then that public opinion of AI is at best, mixed; and at worst, suspicious. A survey conducted by the Center for the Governance of AI, Future of Humanity Institute, University of Oxford found that:

41% of respondents somewhat or strongly support the development of AI.

22% somewhat or strongly oppose AI development

82% believe that robots and/or AI should be carefully managed. Of course they do. They’ve seen on television what happens when they aren’t.

Given that 22% of Americans oppose the development of AI, we’d say there is far more work to be done.

It’s The Math

The reality of course is that AI is a complex system of math, statistics, probability, algorithmic design, and more. Hardly magic. Hardly the stuff of nightmares.

To those outside this space, the yawning began with the word “math.” The general public tunes out. It’s too complicated, too complex, too high-level to understand.

Hence, their interaction with this space comes from pop culture and click-bait articles about the end of the world, the coming AI apocalypse, and 1984-levels of government surveillance. We’re making it a mystery that is hard to understand.

Starting From The Basics

What’s the right starting point? It starts at statistical and mathematical literacy. Take algebra and calculus, for example. It’s difficult, if not impossible to make sense of one without understanding the other.

Therefore, we follow a linear progression of mathematical education that starts at the beginning and follows a logical progression.

AI education should be no different. If we want people to truly comprehend this space, we’ve got to find ways to socialize it. This may include increasing funding, but also entails a holistic and comprehensive approach to AI education.

By creating an environment in which AI is viewed with honest eyes and healthy, but not alarmist levels of trepidation, we can demystify this space. This will make it more common, less unknown, and as a result; less scary.

A lot of this starts with understanding how we ourselves arrive at decisions. Truly dispassionate decisions are hard to come by. In many ways we live in a very consensus drive society. This is what drives the desire for “likes” on social media, why politicians earn “points” for bold statements, or why we rely heavily on “gut feelings” when making important decisions.

In the world of advertising, this is why social proof in the form of real testimonials or personal recommendations can be more powerful than 100 studies which support your product’s effectiveness.

When machines are making the calls, however, it can become scary, confusing, or just foreign. We have to change that mentality. Allow people to understand how the machines we build arrive at value-based decisions, and demystify the “magic” that goes into arriving at their conclusions.

A Pragmatic Perspective

Another important part of AI education is to take an honest, real-world approach to discussing what AI is and is not. The early days of consumer “AI” are closely mirroring the early days of “virtual reality.” Which is to say, a scarce resemblance to the real thing.

Arcade goers from the 90’s will no doubt remember “virtual reality” video games which were basically anything but actual “virtual reality.” Rather, they were more like playing a regular video game, but with bad graphics, and a helmet which kind of mimicked sitting really close to the screen.

The Star Trek “holodeck” this was not.

Today, we’re a little more advanced with our early AI tools. Alexa, Google Home, and similar products are often billed or positioned as home AI companions. Are they intelligent? Well, not really. More like advanced search engines with voice recognition. You can’t have a conversation with Alexa. Not really anyhow. And she doesn’t really understand what you’re asking.

Further down the artificial intelligence and even machine learning ladder, we have object detection, sentiment analysis, language processing, etc. Are these tools smart? Do they comprehend context? Do they know what we’re teaching them? Yes and no. But in the context of how we globally think of “artificial intelligence,” no. Not really.

As a result, even current year AI is still decades away from being Skynet or The Matrix. In essence, we’re nowhere near creating the robots or comptuer programs who will overthrow us. Not yet.

What About The Ethics of AI?

As we’ve previously mentioned, AI & Ethics is a young, growing field. Because of this, we believe firmly that everyone who operates in this space has a responsibility to behave ethically, to set high standards, and to continuously evaluate our approach with regard to ethics and morality. Hence the need for sober-minded (not alarmist levels) of trepidation.

It is precisely a fear of the lack of these things (ethics and responsibility) which has contributed to the outlandish, dystopian view of a future where AI is, let’s say, not exactly nice to humans.

Therefore, we see two separate questions at play with regard to the ethical debate.

1. Ethics and AI

This portion deals with how we as humans behave. I.e. how do we cultivate this thing we’re calling AI and how are we applying it? The dystopian view of this aspect of AI is what drives the fear of Big Brother style surveillance. Of tech companies or massive government databases violating civil liberties, etc. Or at worst, a rogue state or actor creating James Bond villain levels of nefarious activity.

2. Ethical AI

This deals with how machines make value-based decisions. This would entail elements of both what we have taught machines as well as what they learn on their own (with our guidance and tutelage of course). The dystopian view of this aspect of AI is what gives us the long-held sci-fi trope of machines becoming self-aware and deciding to exterminate humanity.

Adam, Eve, Apples, And Serpents

From a theological perspective, elements of both of these ethical questions are nothing new. Take the creation myth found in Abrahamic religions of Adam & Eve.

One of the fundamental foundations of humanity is our ability to exercise free will and choice. However, such was not the case of Adam & Eve. At least not in the beginning.

As the mythology goes, humans created without free will (ostensibly machines at that point) obtain it through acquiring knowledge (i.e. the fruit in the Garden of Eden). With it, came free will. And thus, humanity acquired the ability to choose their own paths.

Spoiler alert: they don’t go on to always make the right ones.

A similar path for AI can be imagined which is partially why the doomsday scenarios don’t seem altogether far-fetched. Part of AI education needs to be dispelling this notion that these paths are a foregone conclusion.

Everything Is Fire

AI is by no means the first technological concept which has been met with skepticism or fear. Without hyperbole, nearly every product, process, or technology has had its detractors and doomsday theorists. From automobiles to smartphones; there’s always been a decent amount of wonder and worry at the possibilities, positive and negative.

Again without hyperbole, you can say this about nearly anything. Each product, process, and technology has the potential to be fire. That is to say it can burn as much as it can provide warmth.

This is literally true in the case of nuclear power which has seen its proponents and detractors throughout the years. We have all see the awesome power of nuclear energy; at it’s best and worst.

But AI isn’t Evil

As with nuclear energy, there is of course the potential for bad. However, we would argue that the benefits outweigh the risks. The fear of those risks, however, is why 100% of the world’s energy isn’t generated by nuclear power plants.

It’s also why 22% of Americans oppose AI development.

Can a world exist, sometime in the future, where AI is used for evil? Certainly. We could also witness another Chernobyl or Fukushima disaster. But none of these outcomes are desirable, to be aspired to, or inevitable.

This is why there is as much a need for vigilance in the AI space as there is for safety and regulatory oversight in the nuclear energy sector. To prevent and safeguard against the bad.

This is why we have always maintained that those of us who work in this space must be constantly striving to maintain an ethical standard of its application. To hedge against the bad while striving to attain the good.

The potential for AI to “do harm” is therefore no more dangerous than the potential of an automobile or smartphone in the hands of someone determined to cause harm. In fact, we would argue that the barrier to entry for utilizing AI nefariously is far greater than the barrier to entry for cars and phones. The latter of which are sometimes used simultaneously to our detriment.

The Future Is Not To Be Feared

A world where AI is firmly ingrained into our lives can no doubt be imagined. However, it is far more likely to be intertwined in ways virtually unnoticeable such as eCommerce algorithms, engineer, advertising, or agriculture; than in the form of sentient robots overthrowing us as their creators.

AI education should focus less on the Hollywood mythology of AI as a battle between machine vs creator; and more on socializing and normalizing the statistical foundations which gave it life. The latter is as amazing as it is realistic.

Statistical and mathematical literacy would help dispel the fear surrounding AI and its nefarious potential. Once we do this, we can focus on educating the public on just what AI is, how it will impact their lives, and why it is not something to be feared.

At least, not as widely as Hollywood would have us believe.