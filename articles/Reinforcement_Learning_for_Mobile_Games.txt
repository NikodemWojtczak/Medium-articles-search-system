Reinforcement Learning for Mobile Games

Introduction

Deep Reinforcement Learning has made a lot of buzz since it was introduced over 5 years ago with the original DQN paper, which showed how Reinforcement Learning combined with a neural network for function approximation can be used to learn how to play Atari games from visual inputs.

Since then there have been numerous improvements to the algorithms, continuously beating the previous benchmarks. Research and testing new algorithms typically use the fast and stable Atari Benchmark, as well as custom-tailored environments such as those found in OpenAI Gym and DMLab. In addition, we can run hundreds of classic console games using the Gym Retro library using emulators.

For modern games, the main focus has recently been the hardest competitive games, in particular, Dota2 and Starcraft2. Both have achieved very impressive results from OpenAI and DeepMind, training across a huge distributed cluster, reaching total experience of thousands of years of game-play. In both these cases, the observations/inputs were numeric features and not visual frames, bypassing the need for the AI to learn how to extract these features itself as typically done for Atari games. The computing resources required are also not something most researchers or small companies have available.

Here I attempt to apply RL to some modern mobile android games, using only visual inputs as observations, and a reasonable budget amount of computational resources, on par with what’s typically considered ‘sample efficient’ when learning to play Atari games.

Android Environments

I use a configurable amount of Android Emulators as the environment for the RL agent to learn to play. Each emulator is controlled by an asynchronous driver which gathers experiences for training by grabbing a visual frame, selecting an action using the RL policy, sending that action back to the emulator, and sending the transition data back to be used for training.

Not Really Emulation

At its core, the android emulator is a ‘true emulator’, able to emulate the ARM instruction set and potentially able to be used as a fully emulated RL environment similar to the ALE or gym-retro environments, where the emulation can be paused/resumed as needed when the next action/observation step is needed. In reality though, emulation on the Android Emulator is extremely slow, especially for games, and not feasible for our task.

The only option is to run the emulator with hardware acceleration for the CPU (Using HAXM/KVM on windows/linux). In this case, an x86 android image is used and the android instructions run directly on the host CPU in real-time using virtualization. This lets us reach the stable android performance we need, but has a few drawbacks:

It means the emulator runs in real-time. This means we need to consistently grab frames, select actions and send those actions with minimal latency. The agent cannot for example pause and wait for a training step to complete, so each environment instance must run asynchronously

This also means we are confined to the ‘actual play-time’, for example, 60FPS, compared to emulation environments like the ALE which can speed up emulated play-time by orders of magnitude for much faster training

Because the android emulator is now running with CPU virtualization, it makes it problematic to run the emulators on VM instances from the popular cloud providers. Some of these providers support ‘Nested Virtualization’ for this purpose, but in practice, I could not get this to work well. Either the emulator failed to boot, or it succeeded but ran much slower than on non-VM servers. This limits us to running these environments on custom server builds or ‘bare metal’ cloud servers which are much less common.

Frame Grabbing

The environment driver grabs visual frames from the emulator at a fixed configurable rate and also tracks how many ‘dropped frames’ it had due to latencies in the agent. The main goal was to ensure minimal such frame drops, which could be detrimental to gameplay. This required to limit the number of emulators running on a single machine, in particular when training was also done on the same machine, to ensure enough CPU bandwidth for acting and training. 2 dedicated CPU cores (4 vCPUs) were needed per emulator to get stable gameplay.

I configured the frame grabbing to 30FPS with a frame-skip of either 2 or 4 (Compared to 60FPS with a frame-skip of 4 typically used in Atari training).

The android emulators are run at a 320x480 (Portrait) resolution to minimize the rendering overhead. This was the minimal resolution at which the games still worked normally. This gives us raw observations of size 320x480x3, which were then scaled down to 80x120x3 for the final observation used as the policy input (Compared to 84x84x1 grayscale observations used in most Atari research)

Action Delays

Another issue I encountered was an inherent delay in the Android Emulator from the time I sent an action until it was actually received/executed by the game. The delay was about 60–90ms, which should seemingly be negligible and something the learning should be able to account for (Even recent Starcraft 2 work from DeepMind mentions delays of around 250ms), but in reality, I found these can hurt the ability to optimally learn how to play.

To confirm this point I created a custom wrapper for Atari environments from OpenAI gym which inserted an artificial configurable delay from the time an action was sent by the agent until it was sent to the actual environment. Even with a delay of 60–90ms I saw a non-negligible reduction in final performance when training a tuned RL algorithm with the delay, compared to without the delay.

This isn’t to say we shouldn’t be able to learn how to play with such delays, as these delays can be the common case when applying RL to real-time tasks such as robots or drones. But it might be worthwhile to consider inserting such delays into the commonly used benchmark tasks when testing and comparing RL algorithms, to ensure the algorithms are robust to them.

Detecting and Skipping Screens

In order to provide an environment which can be used for RL we need to handle detecting and tapping through irrelevant screens:

Tapping in the right place to start a new game

Detecting screens which signify the game is over, and tapping the appropriate places to start a new game

Detecting various popup screens and closing them (Messages, new character unlocks, new item, etc..)

The above can probably be automated somehow, maybe even using some sort of AI/learning, but that’s a totally different topic which I did not deal with. For now, I just manually configured the relevant screens per game and where to tap when encountering such a screen.

Rewards

I used a ‘Score Change’ based reward signal. This gives a +1 reward every time the score changes. Instead of dealing with OCR of the actual score value, I found it was enough and easier to just detect the score value changed and give +1 in this case. For most games, this translates to the actual score (Except for some occasional misdetections). Moreover, since many RL algorithms clip the rewards to -1/+1 it is less important to detect the actual score in this case.

Sample Efficient Scalable RL

Because of the slowness and high resource cost of training on the Android emulators, I went looking for a sample efficient RL algorithm which can scale well to multiple environments.

I tried various families of RL algorithms but eventually focused on q-learning variants, in particular due to the fact that the games I was trying to learn had discrete action spaces with near-zero tolerance for errors (i.e. choosing a single wrong action will very oftentimes lead to a game-over). This makes value-based methods like q-learning more suitable than stochastic algorithms such as actor-critic variants (Though it is still possible for stochastic algorithms to converge to a good policy, I initially got fairly good results using PPO, but overall stabler and better with DQN variants).

The current go-to sample-efficient RL algorithm for discrete actions is Rainbow. Rainbow takes the original DQN algorithm, and combines 6 independent improvements into a single agent, reaching state-of-the-art results on the Atari benchmark at the 200M total frames threshold (~38 days of ‘play-time’).

IQN is an improved distributional version of DQN, surpassing the previous C51 and QR-DQN, and is able to almost match the performance of Rainbow, without any of the other improvements used by Rainbow.

Both Rainbow and IQN are ‘single agent’ algorithms though, running on a single environment instance, and take 7–10 days to train. Running a single android emulator agent in real-time at 60fps for 200M frames would take 38 days.

For multi-actor/distributed q-learning, the state-of-the-art is R2D2. R2D2 is a distributed multi-actor algorithm, improving over the previously published APEX with additional improvements, in particular using a ‘recurrent’ version of the DQN model with an added LSTM layer in the middle, to help the agent maintain a ‘memory’ of what happened until now and perform better long-term planning of its actions.

The LSTM is shown to be a big help even on Atari where most games are fully observable (i.e. the full state of the game can be seen on the screen) which should seemingly not benefit from LSTM memory. There is not much research on why it helps, but one possible explanation is it allows the policy to ‘follow a plan’ which allows it to learn and play more efficiently.

R2D2 reaches extremely high scores on almost all Atari games, leaving little room for improvement, however, it does so at the cost of sample-efficiency, requiring to see over 50x more environment frames than sample efficient algorithms. Still though it is able to achieve this in less time (5 Days) than Rainbow/IQN thanks to its highly distributed architecture running 256 asynchronous actors on the high-speed Atari emulations using 256 CPUs. This would not be feasible though for my android setup, where the environment runs ~10x slower and needs 2 CPUs per instance.

In order to maximize my resource utilization and minimize training time, I worked on combining features from IQN/Rainbow/R2D2 into a ‘Recurrent IQN’ training algorithm (full details here), which achieves improved sample-efficient results on the Atari benchmark with the option of running multiple actors in parallel, allowing me to train on the slow android environments efficiently in a reasonable amount of time (Up to a few days).

Training Setup

For training, I used a custom (non-VM) server with 32 CPUs and an NVIDIA 1080ti GPU. I ran 12 Android Instances per training session, each gathering experiences asynchronously in separate processes using a shared-memory copy of the main policy, to avoid latencies when one of the actors had to pause (for example to restart after a game-over). Experiences were retrieved from the processes between training steps to fill the training replay buffer.

I used the same hyperparameters and ‘Recurrent IQN’ algorithm as the one tuned for the Atari Benchmark, with a few modifications:

Reduced from 32 to 12 ENVs

ENVs run asynchronously instead of vectorized, with a shared-memory policy updated every 1000 steps with weights from the training policy

A fixed decaying exploration rate, from 1.0 to 0.001 per actor (The extra low exploration rate is needed as the games I am using are extremely sensitive to ‘wrong actions’, so the common 0.01 rate would make it hard for the training actors to reach longer episodes)

Visual game frames are scaled from 320x480x3 resolution to 80x120x3 RGB observations (Compared to 84x84x1 grayscale observations commonly used in Atari)

Frames are grabbed from the emulator at 30FPS, with a frame-skip of either 2 or 4 depending on the game, without ‘max merging’ (i.e. agent transitions are either 15 or 7.5 steps-per-second, compared to 15 typically done in Atari learning). The lower rate is needed in some games where the actions would otherwise take longer than the step-time (For example swipe actions)

Games and Results

I use the following 3 games to focus testing on:

Flappy Bird: A very hard and frustrating game for human players, requiring very fast and precise reaction time, but has a very simple mechanic and constant visuals, so the overall policy to learn should be a relatively simple one

Subway Surfers: The popular endless arcade game, dodging trains by swiping between 3 lanes, jumping or crouching above and under obstacles, and trying to collect coins along the way

Crossy Road: The infamous chicken hopper game requires the agent to plan ahead and get moving at the right time while considering the oncoming traffic and car trajectories, floating water logs, and fast passing trains

All games are evaluated using the final training policy, with no random-action epsilon.

Flappy Bird

A high score for Flappy Bird. Reached the 30-minute time limit without dying

Flappy Bird was trained at 30FPS with a frame-skip of 2 (15 Steps-Per-Second) for a total of 25M steps (Equivalent to about half the total ‘gameplay time’ used in sample-efficient Atari training). This takes around 40 hours to train using 12 emulators.

The action space has a single action (Tap) and an additional ‘noop action’. The reward is +1 each time the score changes (i.e. each time the bird passes a pipe)

Training chart for Flappy Bird, average reward of last 100 episodes

This game is notorious for taking time to start learning because the initial reward is received only after passing the first pipe, which can be extremely hard and random for an untrained agent using random exploration. There are hundreds of thousands of initial steps with 0 reward, which means the agent has no feedback at all and no way to learn. Only after we get some experiences of passing the first pipe (Which is purely random), the agent finally starts to figure it out, from that point it fairly steadily improves, but it takes over 2M steps to reach that point.

This game is extremely sensitive to wrong actions, for example performing evaluation with a random-action epsilon of 0.001 results in a mean reward of 143 instead of 420, as seen also in the training chart final results (Training also uses 0.001 for the final exploration rate)

Evaluation Score: Average 420, Max 1363 (10 Eval Episodes)

Subway Surfers

2 high scoring Subway Surfer games

For this game, I used the coin collection count as the reward, to encourage the agent to collect coins, which in itself is not necessary in order to progress, but is an important ability to learn. I also added a negative (-1) reward upon game over to encourage the agent to learn not to lose, regardless of coin collection.

The game is trained at 30FPS with a frame-skip of 4 (7.5 steps-per-second compared to 15 commonly used in Atari, due to the swipe gesture taking longer than 66ms), for 25M steps (Equivalent in total play-time to common sample-efficient Atari training using 200M frames at 60FPS). This takes around 90 hours to train using 12 emulators.

The action space here has 4 actions (Swipe up/down/left/right) and the ‘noop action’.

Interestingly the agent almost never chooses the noop action and prefers to always either jump or slide down even when it doesn’t really need to. This sort of makes sense as there is no downside to doing that and it can only help in some cases (Not all training runs are like this, some had many more noop actions). We can maybe try to mitigate this by adding a small negative reward on each action taken (e.g. -0.1).

Training chart for Subway Surfers, average reward of last 100 episodes

Evaluation Score: Average 142, max 434 (30 Eval Episodes)

Crossy Road

3 high scoring Crossy Road games

Crossy Road is trained at the same frame/step rate as Subway Surfers. The reward is +1 on every score change (Which is every time the chicken moves a step forward).

The agent clearly has a preference to try to only move forward, but it knows to move left/right and even back when needed.

Training chart for Crossy Road, average reward of last 100 episodes

Evaluation Score: Average 41, Max 139 (30 Eval Episodes)

Conclusion

The results look promising so far, although still not what we would consider ‘superhuman’ (Except maybe Flappy Bird). Still, it’s promising to see RL able to learn modern casual games in real-time from visual inputs, using the same hyperparameters as Atari, within a reasonable amount of time, frames and resources.

It’s also pretty clear from the charts that learning hasn’t plateaued, and it would be interesting to see what results we can get going to 50M or even 100M steps and beyond, and with some hyperparameter tuning.