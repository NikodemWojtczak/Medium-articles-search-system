Linear regression is arguably one of the most important and most used models in data science. In this blog post, I will walk you through the process of creating a linear regression model and show you some cool data visualization tricks.

We will be using the Ames Housing dataset, which is an expanded version of the often cited Boston Housing dataset. The dataset has approximately 1,700 rows and 81 columns, and the goal is to predict the selling price in the market.

Let’s Begin!

Just like any other project, the first step is to import all the libraries. You can import more libraries as you figure out all the tools you will need to build your model.

Now, before we process or change the data, let’s just get an idea of what we are dealing with.

The short description of the data set shows us that it contains various data types and null values. It’s important to remember this as we process the data and build our model.

Data Preprocessing

Datasets in the real world are never perfect. There will always be missing values and outliers that skew the dataset, affecting the accuracy of our predictive model. That’s why it is always a good idea to clean up the data before you start building your model.

Earlier, I had mentioned that this particular dataset had 1,700 rows and 81 columns. If a value is missing in a particular column, it would be unwise to delete that whole row because we would be losing a datapoint in every other column as well. There are two ways to solve this issue:

Replace every null value in a given column with the median value of that particular column. (This is only applicable to columns with numerical values) Calculate your statistics while ignoring all null values. (I’ll show you what methods to use later in the blog).

I opted for the second method, so I left the null values as they were in my dataset.

Calculating Outliers

There are multiple ways to calculate outliers— z-score, inter-quartile range (IQR), and the Tukey method are just a few methods out there. I chose to use the IQR. For all intents and purposes, I am assuming that you are familiar with the concepts of IQR, so I will only go over how to code it in Python. If you feel like you could use a short overview, this blog post does a pretty solid job of explaining the key ideas behind inter-quartile range.

In order to calculate the first and third quartile, I used the describe() function on the dataset.

summary = data_set.describe()

The describe() function yields a neat and concise data frame with important statistics from each numerical column of the original dataset. As you can see in the image above, I now have access to the mean, standard deviation, and percentile values with just one line of code!

Now, I store each statistic for every column in a data series. This allows me to access the percentile values in all the columns iteratively.

Now let’s dive into the deep end. I’ll first explain at a high level how I calculated the IQR before I dump my code onto here.