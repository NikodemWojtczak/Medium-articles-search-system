Key Blocks

Based on some of the above examples we can say that below are the key components required to make a deep learning inference accelerator. Also, we will only focus on 8-bit inference engine which has been shown to be good enough for many applications.

Matrix multiplication Unit — This is referred by different names like TPC (Tensor processing core), PE, etc. GEMM is the core computation involved in DNN’s, to learn more about GEMM read this great post.

SRAM — This is the local memory used to store the weights or intermediate outputs/activations.

Data movement Energy Vs Compute — Source — Efficient Processing of Deep Neural Networks: A Tutorial and Survey

To reduce energy consumption the memory should be located as close as possible to the processing unit and should be accessed as little as possible.

Interconnect/Fabric — This is the logic which connects all the different processing units and memory so that output from one layer or block can be transferred to the next block. Also referred to as Network on Chip (NoC).

Interfaces (DDR, PCIE) — These blocks are needed to connect to external memory (DRAM) and an external processor.

Controller — This can be a RISC-V or ARM processor or custom logic which is used to control and communicate with all the other blocks and the external processor.

Architecture and Instruction Set

If we look at all the architectures we will see memory is always placed as close as possible to the compute. The reason is that moving data consumes more energy than compute. Let’s look at the computation and memory involved in AlexNet architecture, which broke the ImageNet record in 2012 —

AlexNet Layers and Parameter — Source

AlexNet consists of 5 Constitutional layers and 3 fully connected layers. The total number of parameters/weights for AlexNet is around 62 million. Let's say after weight quantization each weight is stored as an 8-bit value so if we want to keep all the weights in on-chip memory it would require at least 62 MB of SRAM or 62*8 Mega-bits = 496 Million SRAM cells. If we use the 6T (six transistor) SRAM cell just the memory would require — 496M*6~2.9 Billion transistors. So while deciding architecture we have to keep in mind which DNN architectures we can support without keeping weights off-chip (which increases power consumption). For this reason lot of startups demonstrate using newer architectures like MobileNetV2 which use much fewer parameters and less compute, for example, one checkpoint of MobileNetV2 with Top-5 accuracy of 92.5% on ImageNet has only 6.06M parameters and performs 582M MACs (multiply and accumulate) operations during single image inference.

Accuracy Vs Model Size — Source

Weight pruning is another technique which can be used to reduce the model size (hence memory footprint). See results for model compression.

MobileNetV2 uses depthwise separable convolutions which are different from traditional convolution so the accelerator architecture has to be flexible enough so that if researchers come up with different operations they can still be represented in terms of the instruction set available on the accelerator.

We can come up with a very simple set of instructions for our simple accelerator like —