Echo State Networks

So we have made the case that there is no method out there that can handle chaotic time series, which, unfortunately, just so happens to be how we model the stock market. An approach to avoid this difficulty is to fix the recurrent and input weights and learn only the output weights: the Echo State Network (ESN). The hidden units form a ‘reservoir’ of temporal features that capture different aspects from the history inputs.

The mathematical justification behind the ESN is rather involved, so I will try to avoid it for the sake of this article. Instead, we will discuss the concept behind the ESN and look at how it can be implemented relatively simply using Python.

The description in the original paper outlining why it is called an ‘echo’ network is

“The unifying theme throughout all these variations is to use a fixed RNN as a random nonlinear excitable medium, whose high-dimensional dynamical “echo” response to a driving input (and/or output feedback) is used as a non-orthogonal signal basis to reconstruct the desired output by a linear combination, minimizing some error criteria.”

An ESN takes an arbitrary length sequence input vector (u) and (1) maps it into a high-dimensional feature space (i.e. the recurrent reservoir state h), and applies a linear predictor (linear regression) to find ŷ.

Schematic diagram of an echo state network.

We essentially train only the output weights, which drastically speeds up the training. This is the great advantage of reservoir computing. By setting and fixing the input and recurrent weights to represent a rich history, we obtain:

Recurrent states as dynamical systems near to the stability — stability means Jacobians are all close to one (no vanishing or exploding gradients)

Leaky hidden units that partially remember the previous state — They avoid exploding/vanishing gradients, whilst at the same time have no need of training.

Echo state property

In order for the ESN principle to work, the reservoir must have the echo state property (ESP), which relates asymptotic properties of the excited reservoir dynamics to the driving signal. Intuitively, the ESP states that the reservoir will asymptotically wash out any information from initial conditions. The ESP is guaranteed for additive-sigmoid neuron reservoirs, if the reservoir weight matrix (and the leaking rates) satisfy certain algebraic conditions in terms of singular values.

Training

So you might be wondering, how do we pick the values for the hidden state in the first place? The input and recurrent weights are initialized randomly and then are fixed. So, we are not training them. How should we fix them to optimize the prediction?

The training is very easy and fast but there are hyperparameters such as hyperparameters that govern the random generation of the weights, the degree of reservoir nodes, the sparsity of the reservoir nodes, the spectral radius. Unfortunately, no systematic method exists to optimize the hyperparameters, and so this is typically done using a validation set. Cross-validation is not feasible with time series data due to the inherent autocorrelation present in the feature space.

To recap:

The network nodes each have distinct dynamical behavior

Time delays of signal may occur along the network links

The network hidden part has recurrent connections

The input and internal weights are fixed and randomly chosen

Only the output weights are adjusted during the training.

Coding an Echo State Network

Now for the moment you’ve all been waiting for, how do you actually code these mysterious networks? We use a Python library which is available from this GitHub repository. The library is called PyESN. In order to install this library, you must clone the repository and put the pyESN.py file in your current Jupyter Notebook folder. Then when you are in the Python 3 notebook you can simply called import pyESN .

You call the RC as:

For a brief explanation of the parameters:

n_inputs: number of input dimensions

n_outputs: number of output dimensions

n_reservoir: number of reservoir neurons

ranodom_state: seed for the random generator

sparsity: the proportion of recurrent weights set to zero

spectral_radius: spectral radius of the recurrent weight matrix

noise: noise added to each neuron (regularization)

Predicting Amazon Stock Prices

I will now go over an example of using echo state networks to predict future Amazon stock prices. First, we import all of the necessary libraries and also import out data (which, in this case, was scraped from the internet).

We then use the ESN from the pyESN library to employ an RC network. The task here is to predict two days ahead by using the previous 1500 points and do that for 100 future points (check the figure below). So, in the end you will have a 100 time step prediction with prediction-window = 2. We will use this as the validation set.

First, we create our echo state network implementation using some reasonable values and specify our training and validation length. We then create functions to calculate the mean squared error as well as the run an echo state network for specific input arguments of the spectral radius, noise, and the window length.

Now we can simply run one function and obtain our prediction, and then we can plot this to see how well we did.

The above code produces the following plot.

And if we zoom in on this plot, we can see just how impressive this prediction actually is.

Not bad right? The only caveat is that it seems to work well for short time periods (on the order of 1 or two days) with reasonable accuracy, but the errors become increasingly large as that estimate is extrapolated further. The above model was made with a prediction window of two days, meaning that we are only ever predicting 2 days into the future at any given time.

We can illustrate this by increasing the window length. For a window length of 10 days, the prediction is still surprisingly accurate, although it is obviously worse than the two-day prediction window.

In order to obtain a result this good, we had to do a significant amount of hyperparameter optimization; here is the procedure that was done to obtain the hyperparameters used in the above results.

The above code takes a while to optimize, and this is only for two parameters. When plotting the MSE on a heatmap we get the below result.

However, it gives us a pretty good result on our validation set when the window function is small. This makes ESN’s good for short term predictions of stock prices, but things get a bit more uncertain and risky if you try to extrapolate the results.

In the future predictions, the error propagates in time and thus it increases in time. This is the reason that as the window length is increased the accuracy decreases. We can see this behavior in the plot below, where the MSE is an increasing monotonically function of the prediction-window, hence longer predictions mean larger MSE.

Final Word

The ability of the echo state network to analyze chaotic time series makes it an interesting tool for financial forecasting where the data is highly nonlinear and chaotic. But, we can do more with these networks than predict the stock market. We can also:

Forecast the weather

Control complex dynamical systems

Perform pattern recognition

Expect to hear a lot more about these kinds of networks in the future, especially as people move into the development of DeepESN models which are able to work in a much higher dimensional latent space with temporal features that are able to tackle some of the most difficult time series problems.

If you are interested in these networks, there are more articles and research papers discussing these this that are freely accessible.

All of the above work is outlined in my GitHub repository that can be found here.

Happy machine learning!