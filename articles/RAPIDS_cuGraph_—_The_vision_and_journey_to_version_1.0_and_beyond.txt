The vision of RAPIDS cuGraph is to make graph analysis ubiquitous to the point that users just think in terms of analysis and not technologies or frameworks. This is a goal that many of us on the cuGraph team have been working on for almost twenty years. Many of the early attempts focused on solving one problem or using one technique. Those early attempts worked for the initial goal but tended to break as the scope changed (e.g., shifting to solving a dynamic graph problem with a static graph solution). The limiting factors usually came down to compute power, ease-of-use, or choosing a data structure that was not suited for all problems. NVIDIA GPUs, CUDA, and RAPIDS have totally changed the paradigm and the goal of an accelerated unified graph analytic library is now possible.

The compute power of the latest NVIDIA GPUs (RAPIDS supports Pascal and later GPU architectures) make graph analytics 20x faster on average over NetworkX . Moreover, the internal memory speed within a GPU allows cuGraph to rapidly switch the data structure to best suit the needs of the analytic rather than being restricted to a single data structure. cuGraph is working with several frameworks for both static and dynamic graph data structures so that we always have a solution to any graph problem. Since Python has emerged as the de facto language for data science, allowing interactivity and the ability to run graph analytics in Python makes cuGraph familiar and approachable. RAPIDS wraps all the graph analytic goodness mentioned above with the ability to perform high-speed ETL, statistics, and machine learning. To make things even better, RAPIDS and DASK allows cuGraph to scale to multiple GPUs to support multi-billion edge graphs.

Every release of RAPIDS is accompanied with one or more wonderful BLOG(s) about the features of that release (for example, see the release 0.8 blog [1] or Life after Hadoop). This article is slightly different. While current features will be discussed, the main focus is on presenting the vision of cuGraph and how we envision users will interact with the libraries. The cuGraph team has been working hard to provide a rich set of initial features. Over the past three releases (RAPIDS release 0.6 was the first to include cuGraph), the team has provided over a dozen algorithms. The initial goal was to simply get graph analytics released and available with a familiar NetworkX-like API. That is great in the short term since it allows an easy path to switch to RAPIDS. However, cuGraph development will slowly shift towards improving ease-of-use, interoperability, and integration with the rest of RAPIDS. That is not an easy task since there is still so much to be added to the cuGraph ecosystem. Don’t worry, the goal of getting new analytics out will continue since there is a very long list of algorithms to release.

Property Graph and Advanced Workflows: the tie between cuDF, cuML, and cuGraph

The term “Property Graph” has been discussed in the literature since 2010 [2,3,4] and is most commonly used in the context of defining a graph within a relational database — I actually built a database-backed graph system prior to 2010, so the concept is older than the literature. More formally, a Property Graph is a directed multigraph where each vertex and edge can have multiple attributes. A multigraph means that there can be multiple edges between any pair of vertices. Consider a cyber network where vertices represent servers, each server can have multiple IP addresses (attributes). Between any pair or servers there can be multiple edges where each edge represents a different connection with time, port, and protocol information. See the following illustration.

A Multigraph with its DataFrame Representation

Within RAPIDS, a Property Graph can be represented as a collection of Data Frames. In the simple case, one Data Frame is used for vertex information and another for edges, as illustrated in the previous figure. However, more than two Data Frames is also possible (and common). Once a Property Graph is populated, various subgraphs can be extracted. For example, we can extract a subgraph where the edges are only via port 80: edge_df = Edges.query(‘port == 80’). Additionally, we can roll-up all of the edges into a single edge using the group_by function. The benefit of being able to capture all attributes and produce different views into the data is the power of the Property Graph model.

Data Frame to rule them all

The concept of a Data Frame is central to RAPIDS — more than just the fact that RAPIDS leverages the Apache Arrow [5] spec to ensure interoperability between processes by allowing zero-copy data movement. The concept of a Data Frame allows all the components of RAPIDS to speak, understand, and share data. It is the central concept that binds them all together. Consider something like the above Property Graph, except assume that there are more attributes on the nodes — this can be cyber, social, or product information — and assume that we want to further enrich the Node data with graph metrics. We can (1) build a graph from the Edge Data Frame, then (2) compute and add the degree information about each vertex back to the Node Data Frame. Likewise, (3) the PageRank scores can be computed and added back. Finally, we can (4) then take the newly enriched Nodes Data Frame and run K-Means, from cuML, on the data and look at how the data clusters. This highlights the ease of data moving between cuDF, cuML, and cuGraph — and it’s all thanks to the power of the Data Frame.

At the Python API layer, RAPIDS cuGraph fully supports Data Frames, and all functions accept and return a Data Frame. CuGraph also supports Property Graphs and is working to better integrate the concept within the algorithms.

Technologies and Frameworks — Static, Dynamic, and everything in between

There has been an on-going debate within the graph research community for years regarding which technology is best: vertex-centric [6] or graph-centric [7]. Even within the cuGraph team, we debate which approach is better. But cuGraph is about analytics. Rather than picking one technology, cuGraph is opting for providing a collection of technologies. For any analytic, whatever technology provides the best performance and scalability is used. Additionally, as analytics are periodically updated and improved, the underlying technology could also be swapped out. Since analytics are exposed through well-defined APIs, any change (other than performance) will be transparent to the user.

Over the next few releases, the following frameworks [8] will be rolled into cuGraph and analytics exposed via the cuGraph Python API:

GraphBLAS from Tim Davis’ lab at Texas A&M [9]

Gunrock from John Owens’ lab at UC Davis [10]

Hornet from Georgia Tech [11]

nvGRAPH from NVIDIA [12]

Several of those technologies offer dynamic data structures, specifically a dynamic Compressed Sparse Row (CSR) structure. There are numerous places where a dynamic structure is needed. The most obvious is in analyzing graph changes over time. As data is streamed in, how the structure of a network changes can be monitored and reported on. Of equal importance is the use of a dynamic structure within analytics. In many cases, the size of the result set is unknown a priori — for example a sparse matrix multiple (SpGEMM). Being able to collapse, expand, add to, and reduce either the graph or the results on-the-fly is a powerful technique.

Custom Analytics: Prototypes, Primitives, and Bulk Synchronous Parallel

The cuGraph developers are working on providing a full rich set of analytics (see Roadmap section). And each analytic will be periodically revisited and updated to ensure peak performance. However, there are cases where we would like to try out analytics before fully optimizing it. Over the next few releases, we will be rolling out a “proto” module that contains sample code and experimental analytics. It should be stressed that it is experimental. If there is interest in any analytic in that module, then it can be moved into cuGraph proper and optimized. The plan is that we will try out analytics and, if viable, then spend the time to improve the analytics. Every analytic will be periodically reviewed and updated. The update can be through better data structure and data movement, or through replacing the algorithm. The team constantly monitors research and the latest techniques. And as before, all the user sees is better performance and/or scalability.

Beyond just having a play area for cuGraph developers, users are also able to create custom analytics. To help facilitate algorithm development, cuGraph will be providing a collection of primitives. Those primitives can be existing graph analytics or low-level functions. The goal is to eventually expose all the graphBLAS semiring functions as graph primitives. One of the initial prototype analytics, in release 0.9, is an example of computing Strongly Connected Components using the Forward-Backward-Trim method. FW-BW-TRIM performs a forwards BFS, then a backwards BFS, and then looks for the common set (intersection) between the two to form a component. Since the BFS algorithm is already part of cuGraph, the prototype analytics to find a component is highlighted in the following code snippet.

Prototype of Forward-Backward-Trim Strongly Connected Components

Using primitives is one method of building custom analytics, and the other is through using the Data Frames and the Bulk Synchronous Parallel (BSP) programming model. BSP [13,14] is not new, having been developed by Leslie Valiant and first published in 1990. This is the same basic model that has been used for graph processing within a relational database and is used by Pregel. The process involves setting a value, one each vertex for example, and then propagating that value in what is known as a “superstep.” The propogagted values are then aggregated, in a user defined fashion, and assigned to the releated vertices. The benefit of BSP is that it is easy to create a custom analytics, it allows for complex and multi-column data to propagated, and it allows for custom aggregation functions to be defined. The downside is that it is slow.

Simplified BSP Example Looking just at Vertex 4

Getting to version 1.0

RAPIDS cuGraph is moving fast and running down multiple paths. And yes we have broken things. Getting to version 1.0 means that we have a stable foundation and an unchanging APIs. We are still not there yet, but we’re getting close. As additional technologies are rolled into cuGraph, how those features and primitives are exposed is not fully fleshed out. Additionally, the goal is to not have multiple independent technology-based subpaths, but one unified view. Below the API layer, algorithms can bounce between technologies. To help get cuGraph closer, the team is going to take one full release, 0.11, to focus solely on getting the code refactored, organized, and APIs updated. The hope is that a single release cycle will be enough to get the code refactored so that we have a solid foundation for getting to version 1.0.

The roadmap is always in flux, so treat this as a set of goals and not hard deadlines:

Release 0.9

Multi-GPU PageRank

Single-GPU Strongly Connected Components

Release 0.10

Hornet integrated

— Katz Centrality

— K-Cores

— Katz Centrality — K-Cores Gunrock integrated

— Subgraph Matching [15]

— Subgraph Matching [15] GraphBLAS integrated

— K-Truss

— K-Truss nvGRAPH

Additional Primitives

Foundation work

Release 0.11

We are taking a release cycle to focus on a major code refactoring so that all APIs are consistent and updated to C++. This will set us ready for version 1.0.

Release 0.12

OpenCypher — simple queries

GraphBLAS Semiring primitives

Future Releases…

Which release will be version 1.0, is unknown at this time, but cuGraph will be ready.

Conclusion

RAPIDS cuGraph is focused on creating a unified graph analytic capability with the ability to scale to different size problems and leverage different data structures based upon the analytic needs, all while providing an easy to use API that is familiar to data scientists. Our current road map to version 1.0 is intended to lay a solid foundation for that capability, and we intend to further that vision even beyond version 1.0.

If you like the work cuGraph is doing, please give us a star on GitHub: https://github.com/rapidsai/cugraph.

If there is anything that you think cuGraph should be working on, then add an issue: https://github.com/rapidsai/cugraph/issues or start a discussion on the google discussion page: https://groups.google.com/forum/#!forum/rapidsai.

Since RAPIDS is all open-source, please jump in and write some code.

References