Professor Stephen Hawking said, “the rise of powerful AI will be either the best or the worst thing ever to happen to humanity. We do not know which”.

AI (Artificial Intelligence) and Machine Learning are pushing new frontiers, with some of the hype now becoming reality across many industries. Breakthroughs in AI by the likes of Google, IBM, Tesla and other tech giants are being achieved faster than many predicted. While some argue that it is only a matter of time before deep learning, where machines can learn better than humans, takes over completely, I believe that AI needs a healthy dose of HI (Human Intelligence) to support it.

The term AI has become so ubiquitous that it is now used to describe a wide range of algorithmic-based decision-making activities. In fact, many “new” innovations being touted as AI have in fact been in use for a while. Investment management firms have used algorithms to improve portfolio decision-making for years, whilst the use of programmatic buying to optimise media spend and retailers’ use of predictive analytics to manage demand and supply is now commonplace. Furthermore, chatbots have emerged as an established capability across customer services functions, and there has been a sharp rise in the number of intelligent bots operating in areas such as law research, customer call centres and risk analysis.

Many would argue, however, that these examples aren’t real AI. From a purist’s perspective, they’re probably right. They are various flavours of coded logic, operating within different sets of rules.

Some argue that true AI and machine learning capabilities should be focused on “Artificial Neural Networks (ANN)” — essentially mimicking the way the brain works by decoding complex patterns and determining an appropriate course of action.

Whilst it’s widely acknowledged that we are decades away from a true general AI, self-learning algorithms and natural language processing engines have all begun to establish themselves as real value-adding technologies in this space.

What is clear is that there are plenty of use cases for scenarios where technology can now outperform humans and booming investment in this area is testament to that. In 2014 Google paid $400m for DeepMind. Facebook has invested billions in its own AI platforms. IDC predicts that the AI market will be exceed $79bn by 2022 and Gartner has said that by 2020 smart machines will be one of the top CIO investment priorities. Furthermore, PwC and CB Insights calculated that Venture Capital firms invested more than $9bn in AI-related start-ups in 2018. The race to establish smarter machines within our everyday lives seems unstoppable.

Some (Elon Musk perhaps being the most well-known protagonist) argue that it’s only a matter of time before deep learning takes over completely. Whilst we believe that we’re a still far away from a Skynet-type super intelligence seizing control of the world’s computer systems and ushering in Armageddon, we may be closer than ever before to aping Iron-Man’s J.A.R.V.I.S assistant.

However, I still believe there are certain areas where one of the most advanced computers — the human brain — is irreplaceable. While we continue to strive for more efficiencies in numerous aspects of our personal and professional lives, there is still a huge amount of decision making that relies on human intuition, creativity, empathy and our ability to read body language. Those decisions are not black and white and cannot be ‘taught’ or programmed as easily. And this is where one of the most advanced decision-making engines ever to exist, the human brain, comes into its own.

One of the biggest challenges levelled at AI and machine learning is that it still relies on rules and rational thought. On a brute force comparison, there is no question that computers have already eclipsed humans’ capacity for processing data — think of both sides of the hacking equation, where hackers run continuous scripts to force open password protected sites, and where security specialists, such as Cyberint, perform massive searches on the Dark Web for signs that might identify the perpetrators. But it may upset people to hear that for now there are some things that computers can’t do…

Take creativity as one example — where do real creative innovations come from? It isn’t difficult to take a constructive look at human capabilities and see that our abilities in the generation and creation of art is unmatched by machine. Creative breakthroughs have driven us forward through each human age, from the first man to use rock as a weapon through to what is now manifesting itself as the “everything as a service” XaaS Age. Whilst machines are enabling the journey more and more, would the equivalent machines have made the same leaps in science without the creative thought process that spontaneously and sporadically delivered inventions such as flight, steam power and the internet?

Manifestations of machine-generated “creativity” do exist, such as that generated by projects such as Google Dreams, but imagine asking an equivalent machine to create a best-selling novel, or produce a Turner prize winner, or develop a new business model. AI, as a programme, at least for now, whilst able to learn is restricted to the logical parameters that we programme it to learn within.

There is no doubt that the impact of AI and machine learning will be more profound than many could have predicted. The takeaway is simply that it is not a silver bullet to all problem-solving. However, if used alongside the most sophisticated computer that has ever existed, the human brain, it will dramatically change the world we live in.