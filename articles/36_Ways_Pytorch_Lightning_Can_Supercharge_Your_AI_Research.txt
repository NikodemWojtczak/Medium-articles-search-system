Come at me AGI

AGI is not going to solve itself ( deep down you know weâ€™re the AGI of another AI ðŸ¤¯).

But letâ€™s say it didâ€¦

Imagine flipping open the lid to your laptop to find an algorithm like this written for you.

def AGI(data):

data = clean(data)

agi = magic(data)

return agi

Ummm Ok ðŸ¤”. Letâ€™s see where this goes. You convince your research group you need to experiment with this for a bit.

But obvs this wonâ€™t run as written. First, we need a training loop:

for epoch in range(10):

for batch in data:

agi = AGI(batch)

agi.backward()

...

Ok, now weâ€™re kind of training. But we still need to add a validation loopâ€¦

def validate(dataset):

# more magic

Dope. But LOL AGI on a CPU?

You wish.

Letâ€™s run this on multiple GPUsâ€¦ But wait, youâ€™ve also read that 16-bit can speed up your training. OMG, but there are like 3 ways of doing GPU distributed training.

So you spend the next week coding this up. But itâ€™s still slow, so you decide to use a compute cluster. Aaaand now things are getting a bit more complicated.

Sad times

Meanwhile, your AGI has a bug, but youâ€™re unsure whether itâ€™s your GPU distribution code, or how you load your data, or any of the other million things you could have coded wrong.

You decide you donâ€™t quite want to deal with all the training details and you try Keras, but it doesnâ€™t let you implement the AGI function well because you need more control over the training. Fast.ai is also out of the question because this isnâ€™t an off-the-shelf algorithm.

Well, that sucks, now you have code this all up yourselfâ€¦

Nope.