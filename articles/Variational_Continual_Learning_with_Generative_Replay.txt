Theory of Variational Continual Learning

Generally speaking, in machine learning, a discriminative model is defined in terms of probability as:

With f belonging to a class of functions parameterized by θ. For instance, a feed-forward neural network performing classification (softmax as last layer, θ as weights and biases).

The classical bayesian approach models the weights by a distribution over the values the weights can take. It also uses Bayes rule to take advantage of a prior belief of the distribution of θ.

A natural idea, in the frame of continual learning, is to consider the last posterior as the best prior distribution for θ for the new task (new batch of data from a new distribution). Indeed, we notice that:

This is a recursive formula for the posterior distributions over time. In the previous formula, we do not care about the computation of the normalization factor that is often intractable, as we are going to perform approximation of the posterior distribution. (hence the proportional sign)

In most of cases, the posterior distribution is intractable. To overcome this issue, we may approximate the posterior distributions with simpler variational distributions

To perform such an approximation, we consider:

We are going to recursively approximate the posterior by a simpler distribution: given the approximation at time-step t-1, we can compute an approximation for time-step t following the previous formula. This method is called Variational Inference. We are searching for an approximate distribution (here a gaussian mean-field) defined by a set of parameters (μVI, σVI) that fits the target distribution. By fitting, we mean minimizing the difference between the 2 distributions. Typically, the Kullback-Leibler divergence is used to define the difference between two distributions.

Gaussian mean-field approximation

Therefore, to find the approximate variational posterior, we minimize the following Kullback-Leibler divergence.

Therefore, we define the loss as:

The KL term can be analytically computed as the Kullback-Leibler divergence between two gaussians. We can easily estimate the expected log likelihood with Monte Carlo method and the reparameterization trick:

Assume it is hard to evaluate an expected value E = E x∼p [f(x)], One can use Monte Carlo by sampling x1, x2, .. xN ∼ p and get an approximation of E:

This estimator is unbiased but to reduce variance, it is better to use high N

The reparameterization trick for θ drawn from a gaussian distribution is simply the way to rewrite θ independently from the distributions parameters (μVI, σVI)

Therefore, a gradient of the loss function can be computed for the learnable parameters (μVI, σVI) and stochastic gradient descent can be performed.

To refine the learning in the continual setting, we may consider a multi-head architecture. Indeed, single-head architecture are well suited for i.i.d instances or instances where only the input distribution changes overtime. The multihead architecture has the ability to take advantage of shared parameters between tasks and specific heads trained for each task.

The recursive approximation of variational distributions may affect the model performance, and trigger catastrophic forgetting. To avoid such catastrophic forgetting, coresets are introduced: A sample of the training data is stored in a coreset (and removed from the training data), and is used to improve the posterior approximation before prediction. Different techniques may be chosen to select subsets of the training batches. Random sampling is the simplest choice. K-means clustering may be performed to keep a meaningful subset of the training data : centroids are selected to be put in the coreset. Theoritically, we notice that:

We now focus on this new variational distribution that is only trained on non-coreset data. We can compute in a similar way a recursive formula for these variational distributions:

In fact, in the implementation, they never remove any point from the coreset. Therefore, the formula becomes easier:

It is interesting to notice that contrary to the theory developed here, the original implementation linked to the paper is not training on the whole coreset before doing predictions, but only on the subset of the coreset corresponding to the task tested. This is therefore a kind of last-minute training, which is not really satisfactory in the frame of Continual Learning.

We sum up the Variational Continual Learning algorithm: