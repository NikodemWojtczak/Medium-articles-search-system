Modeling Training

There are a few steps involved in model training.

Image collection and labeling (20–30 min) Model selection Transfer learning/model training (3–4 hours) Save model output in Edge TPU format (5 min) Run model inferences on Raspberry Pi

Image Collection and Labeling

We have 6 object types, namely, Red Light, Green Light, Stop Sign, 40 Mph Speed Limit, 25 Mph Speed Limit, and a few Lego figurines as pedestrians. So I took about 50 photos similar to the above and placed the objects randomly in each image.

Then I labeled each image with the bounding box for each object on the image. It seemed like it would take hours, but there is a free tool, called labelImg (for Window/Mac/Linux), which made this daunting task felt like a breeze. All I had to do was to point labelImg to the folder where you stored the training images, for each image, dragged a box around each object on the image and chose an object type (if it was a new type, I could quickly create a new type). So if you are using the keyboard shortcuts, you would only spend about 20–30 sec per picture, and so it took me only 20 min to label about 50 photos (with about 200–300 object instances). Afterward, I just randomly split the images (along with its label xml files) into training and test folders. You can find my train/test dataset in DeepPiCar’s GitHub repo, under models/object_detection/data .

Labeling Images with LabelImg Tool

Model selection

On a Raspberry Pi, since we have limited computing power, we have to choose a model that both runs relatively fast and accurately. After experimenting with a few models, I have settled on the MobileNet v2 SSD COCO model as the optimal balance between speed and accuracy. Furthermore, for our model to work on the Edge TPU accelerator, we have to choose the MobileNet v2 SSD COCO Quantized model. Quantization is a way to make model inferences run faster by storing the model parameters not as double values, but as integral value, with very little degradation in prediction accuracy. Edge TPU hardware is optimized and can only run quantized models. This article that does a deep dive on the hardware and performance benchmark of Edge TPU, for those interested.

Transfer Learning/Model Training/Testing

For this step, we will use Google Colab again. This section is based on Chengwei’s excellent tutorial on “How to train an object detection model easy for free”. Our twist is that we need to run on Raspberry Pi, with the Edge TPU accelerator. As the entire notebook along with its output is quite long, I will present the key parts of my Jupyter Notebook below. The full Notebook code can be found on my GitHub, which contains a very detailed explanation for each step.

Set up Training Environment

The above code chooses MobileNet v2 SSD COCO Quantized model, and downloads the trained models from TensorFlow GitHub. This section is designed to be flexible in case we want to choose a different detection model.

Prepare Training Data

The above code converts the xml label files generated by LabelImg tool to binary format (.record) so that TensorFlow can process quickly.

Download Pre-trained Model

'/content/models/research/pretrained_model/model.ckpt'

The above code will download the pre-trained model files for ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03 model and we will only use the model.ckpt file from which we will apply transfer learning.

Train the Model

This step takes 3–4 hours, depending on the number of steps you train (aka epochs or num_steps ). After the training is done, you will see a bunch of files in the model_dir . We are looking for the latest model.ckpt-xxxx.meta file. In this case, since we ran 2000 steps, we will use the model.ckpt-2000.meta file from model_dir .

!ls -ltra '{model_dir}/*.meta' -rw------- 1 root root 17817892 Apr 16 23:10 model.ckpt-1815.meta

-rw------- 1 root root 17817892 Apr 16 23:20 model.ckpt-1874.meta

-rw------- 1 root root 17817892 Apr 16 23:30 model.ckpt-1934.meta

-rw------- 1 root root 17817892 Apr 16 23:40 model.ckpt-1991.meta

-rw------- 1 root root 17817892 Apr 16 23:42 model.ckpt-2000.meta

During training, we can monitor the progression of loss and precision via TensorBoard (source code in my GitHub link above). We can see for the test dataset that loss was dropping and precision was increasing throughout the training, which is a great sign that our training is working as expected.

Total Loss (lower right) keeps on dropping

mAP (top left), a measure of precision, keeps on increasing

Test the Trained Model

After the training, we ran a few images from the test dataset through our new model. As expected, almost all the objects in the image were identified with relatively high confidence. There were a few images that objects in them were further away, and were not detected. That’s fine for our purpose because we only wanted to detect nearby objects so we could respond to them. The further away objects would become larger and easier to detect as our car approached them.

Save Model Output in Edge TPU Format

Once the model is trained, we have to export the model meta file to the inference graph in Google ProtoBuf format, then to a format that Edge TPU accelerator can understand and process. This seems like a simple step, but at the time of writing, the online resources (including Edge TPU’s own site) were so scarce that I spend hours researching the right parameters and commands to convert to a format that the Edge TPU can use.

Luckily, after many hours of googling and experimenting, I have figure out the below commands to do this.

In the end, we get a road_signs_quantized.tflite file, which is suitable for mobile devices and Raspberry Pi CPU to perform model inference, but not yet for the Edge TPU accelerator.

!ls -ltra '/content/gdrive/My Drive/Colab Notebooks/TransferLearning/Training/fine_tuned_model/*.tflite' root root 4793504 Apr 16 23:43 road_signs_quantized.tflite

For the Edge TPU accelerator, we need to perform one more step. That is to run road_signs_quantized.tflite through the Edge TPU Model Web Compiler.

After you upload road_signs_quantized.tflite to the web compiler, you can download another tflite file, save it as road_signs_quantized_edgetpu.tflite . Now you are done! There will be some warnings, but that can be safely ignored. Note that 99% of the model will on the Edge TPU, which is great!

The difference between the _edgetpu.tflite file and regular .tflite file is that, with the _edgetpu.tffile file, all (99%) model inferences will run on the Edge TPU, instead of Pi’s CPU. For practical purposes, this means you can process about 20 320x240 resolution images per sec (aka. FPS, Frames Per Sec) with the Edge TPU, but only about 1 FPS with the Pi CPU alone. 20 FPS is (near) real-time for DeepPiCar, and this is worth the $75. (Well, I think this $75 cost should be lower considering the entire Raspberry Pi — CPU + circuit board is only $30!)

Planning and Motion Control

Now that DeepPiCar can detect and identify what objects are in front of it, we still need to tell it what to do with them, i.e. motion control. There are two approaches for motion control, i.e. rule-based and end-to-end. The rule-based approach means we need to tell the car exactly what to do when it encounters each object. For example, tell the car to stop if it sees a red light or pedestrian, or drive slower if it sees a lower speed limit sign, etc. This is akin to what we did in Part 4, where we told the car how to navigate within a lane via a set of code/rules. The end-to-end approach simply feeds the car a lot of video footage of good drivers, and the car, via deep-learning, figures out on its own that it should stop in front of red lights and pedestrians, or slow down when the speed limit drops. This is similar to what we did in Part 5, end-to-end lane navigation.

For this article, I chose the rule-based approach, because 1) this is how we, humans, learn how to drive, by learning the Rules of the Road, and 2) it is simpler to implement.

Since we have six types of objects (Red lights, Green lights, 40 Mph Limit, 25 Mph Limit, Stop Sign, Pedestrians), I will illustrate how to handle a few object types, and you can read my full implementation in these two files on GitHub, traffic_objects.py and object_on_road_processor.py .

The rules are pretty simple: if no object is detected, then drive at the last known speed limit. If some object is detected, that object will modify car’s speed or speed limit. For example, stop when you detect a red light that is sufficiently close, and go when you don’t detect a red light.

First, we will define a base class, TrafficObject , which represents any traffic signs or pedestrians that can be detected on the road. It contains a method, set_car_state(car_state) . The car_state dictionary contains two variables, namely speed , and speed_limit , which will be changed by the method. It also has a helper method, is_close_by() , which checks if the object detected is close enough. (Well, since our single camera can’t determine distance, I am approximating distance with the height of the object. To determine distance accurately, we would need a Lidar, or its little cousin, an ultrasonic sensor, or a stereo vision camera system as in a Tesla.)

The implementations for Red Light and Pedestrian are then trivial, which simply set the car speed to 0.

Both 25 Mph and 40 Mph speed limits can use just one SpeedLimit class, which takes speed_limit as its initialization parameter. When the sign is detected, just set the car speed limit to the appropriate limit.

The green light implementation is even simpler, as it does nothing but printing a green light is detected (code not shown). The stop sign implementation is a bit more involved, as it needs to keep track of states, meaning it needs to remember that it has stopped at the stop sign for a few seconds already, and then move forward even if the subsequent video images contain a very large stop sign as the car moves past the sign. For details, please refer to my implementation in traffic_objects.py .

Once we have defined the behavior for each traffic sign, we need a class to tie them together, which is the ObjectsOnRoadProcessor class. This class first loads the trained model for Edge TPU, then detects objects in the live video with the model, and lastly calls each traffic object to change the car’s speed and speed limit. Here are the key parts of objects_on_road_processor.py .

Note that each TrafficObject just changes the speed and speed_limit in the car_state object, but doesn’t actually change the speed of the car. It is ObjectsOnRoadProcessor that changes the actual speed of the car, after detecting and processing all traffic signs and pedestrians.

Putting it together

This is the final outcome of this project. Notice that our DeepPiCar stopped at stop signs and red lights, waited for no pedestrians and moved forward. Also, it slowed down at 25 miles/hour sign and sped up at 40 miles/hour sign.

The full source code of interest is on DeepPiCar’s GitHub:

What’s Next

In this article, we taught our DeepPiCar to recognize traffic signs and pedestrians, and respond to them accordingly. This is not a small feat, as most cars on the road can’t do this yet. We took a shortcut and used a pre-trained object detection model and applied transfer learning on it. Indeed, transfer learning is prevalent in the AI industry when one can’t gather enough training data to create a deep learning model from scratch or don’t have enough GPU power to train models for weeks or months. So why not stand on the shoulders of giants?

In future articles, I have quite a lot of ideas. Here are some fun projects to try in the future.

Hey, Jarvis, Start Driving!

I want to put a microphone on the car, and train it to recognize my voice and my wake word, so that I can say, “Hey, Jarvis, start driving!” or “Hey Jarvis, turn left!” We did a wake-word project in Andrew Ng’s Deep Learning Course, so wouldn’t it be SUPER COOL to be able to summon DeepPiCar, like Ironman or Batman?

JARVIS, Tony Stark’s (Ironman) AI Assistant

Adaptive Cruise Control

Another idea is to install an ultrasound sensor on DeepPiCar. This is similar to a lidar on a real-life vehicle, whereas we can sense the distance between DeepPiCar and other objects. For example, with an ultrasound sensor, we would be able to implement ACC (Adaptive Cruise Control), an essential feature of an autonomous vehicle, or we can enhance our object detection algorithm by only detect for objects when they are within a certain range.

End-to-End Deep Driving

I also want to experiment with full end-to-end driving, where I will remote control the car for a while, save down the video footage, its speed, and steering angle, and just apply deep learning. Hopefully, deep learning will teach it how to drive, i.e. both follow lanes as well as respond to traffic signs and pedestrians, by mimicking my driving behavior. That would be a super cool project if it actually works. This means we don’t have to tell the car ANY rules, it just figures EVERYTHING out by watching!! This would be the Holy Grail of autonomous cars, which is similar to how AlphaZero learns to play Go or Chess, without any human inputted heuristics.

Thank you!

If you have read this far (or built/code along with me), BIG KUDOS to you! If everything worked so far, you should have a deep-learning, self-driving robotic car running in your living room, and be able to stop when it observes a stop sign or a red light! If not, please post a message down below, and I will try my best to help. I really enjoyed sharing this wonderful journey with you. Indeed, it took me a lot longer writing these articles than writing code, training models, and testing DeepPiCar in my living room combined. This also happened to be my first blogging experience, which was both frightening and exciting at the same time. I now realize that I love building things as much as showing others how to build things.

As usual, here are the links to the whole guide, in case you need to refer to a past article. Now I need to find myself a job as a Machine Learning Engineer in an Autonomous Car company! See you in a while!

Part 1: Overview

Part 2: Raspberry Pi Setup and PiCar Assembly

Part 3: Make PiCar See and Think

Part 4: Autonomous Lane Navigation via OpenCV

Part 5: Autonomous Lane Navigation via Deep Learning

Part 6: Traffic Sign and Pedestrian Detection and Handling (This article)