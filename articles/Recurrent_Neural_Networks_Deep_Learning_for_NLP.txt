Recurrent Neural Networks: Deep Learning for NLP

Every time you ask Alexa about the recipe of a dish or a new song by an artist a complex code runs in the background to provide you with relevant answers. This has become possible only in the last few years. Until now, understanding and extracting information from unstructured text data was possible only through manual effort let alone automating acknowledging user requests.

Image by Author

The underlying concept behind the revolutionizing idea of exposing textual data to various mathematical and statistical techniques is Natural Language Processing (NLP). As the name suggests, the objective is to understand natural language spoken by humans and respond and/or take actions on the basis of it, just like humans do. Before long, life-changing decisions will be made merely by talking to a bot.

Here in this article, I will try to provide some basic understanding of neural networks, particularly useful for the purpose of NLP. We will not delve into the mathematics of each algorithm, however, will try to understand the intuition behind it which will place us in a comfortable position to start applying each algorithm on real world data.

Let’s start.

Recurrent Neural Network (RNN)

RNN is widely used neural network architecture for NLP. It has proven to be comparatively accurate and efficient for building language models and in tasks of speech recognition.

RNNs are particularly useful if the prediction has to be at word-level, for instance, Named-entity recognition (NER) or Part of Speech (POS) tagging. As it stores the information for current feature as well neighboring features for prediction. A RNN maintains a memory based on history information, which enables the model to predict the current output conditioned on long distance features. Below is an example of NER using RNN.

Recurrent Neural Network (RNN) — Image by Author

Long Short-Term Memory Cell (LSTM)