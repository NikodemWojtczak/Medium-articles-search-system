This paper by Quanshi Zhang, Ying Nian Wu, and Song-Chun Zhu from University of California, Los Angeles proposes a method to modify traditional convolutional neural networks (CNNs) into interpretable CNNs, in order to clarify knowledge representations in high conv-layers of CNNs.

Figure 1: Comparison of a filter’s feature maps in an interpretable CNN and those in a traditional CNN

Problem: without any additional human supervision, can we modify a CNN to obtain interpretable knowledge representations in its conv-layers?

Bau et al. [1] defined six kinds of semantics in CNNs, i.e. objects, parts, scenes, textures, materials, and colors.

In fact, we can roughly consider the first two semantics as object-part patterns with specific shapes, and summarize the last four semantics as texture patterns without clear contours. Filters in low conv-layers usually describe simple textures, whereas filters in high conv-layers are more likely to represent object parts.

Their approach is to train each filter in a high conv-layer to represent an object part. In a traditional CNN, a high-layer filter may describe a mixture of patterns, i.e. the filter may be activated by both the head part and the leg part of a cat (Figure 1). Such complex representations in high conv-layers significantly decrease the network interpretability. Therefore, their approach forces the filter in an interpretable CNN is activated by a certain part.

Learning a better representation

This paper invented a generic loss to regularize the representation of a filter to improve its interpretability.

The loss encourages a low entropy of inter-category activations and a low entropy of spatial distributions of neural activations which means forcing feature map of a layer in a CNN not to be randomly activated by different region of an object and to have consistent distribution of activations.

The filter must be activated by a single part of the object, rather than repetitively…