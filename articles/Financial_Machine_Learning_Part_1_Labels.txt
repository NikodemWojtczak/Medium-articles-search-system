Financial Machine Learning Part 1: Labels

Introduction

In the previous post, we’ve explored several approaches for aggregating raw data for a financial instrument to create observations called bars. In this post, we will focus on the next crucial stage of the machine learning pipeline — labeling observations. As a reminder, labels in machine learning denote the outcomes of the random variable that we would like to predict.

Just like in the rest of this series, the techniques shown in this post are based on Advances in Financial Machine Learning by Marcos Lopez de Prado. I recommend checking out the book for a much more detailed treatment of the subject. With that said, it’s time to jump in and swim around.

Labeling Observations

In the financial context, a simple approach for a supervised learning problem is to try to predict the price of an instrument at some fixed horizon in the future. Note that this is a regression task, i.e. we’d attempt to predict a continuous random variable. This is a hard problem to solve because prices are notoriously noisy and serially correlated, and the set of all possible price values is technically infinite. On the other hand, we can approach this as a classification problem — instead of predicting the exact price, we can predict discretized returns.

Most financial literature uses fixed-horizon labeling methods, i.e. the observations are labeled according to returns some fixed number of steps in the future. The labels are discretized by profit and loss thresholds:

This labeling method is a good start, but it has two addressable problems.

The thresholds are fixed, but volatility isn’t — meaning that sometimes our thresholds are too far apart and sometimes they are too close together. When the volatility is low (e.g. during the night trading session) we will get mostly y=0 labels even though the low returns are predictable and statistically significant. The label is path-independent, meaning that it only depends on the return at the horizon and not the intermediate returns. This is a problem because the label doesn’t accurately reflect the reality of trading — every strategy has a stop-loss threshold and a take-profit threshold which can close the position early. If an intermediate return hits the stop-loss threshold, we will realize a loss, and holding the position or profiting from it is unrealistic. Conversely, if an intermediate return hits a take-profit threshold we will close it to lock in the gain, even if the return at the horizon is zero or negative.

Computing Dynamic Thresholds

To address the first problem, we can set dynamic thresholds as a function of rolling volatility. We’re assuming that we already have OHLC bars at this point. I use the dollar bars of BitMex:XBT, the bitcoin perpetual swap contract from the previous post — this code snippet will help you catch up if you are starting from scratch.

Here we will estimate hourly volatility of returns to compute profit and loss thresholds. Below you’ll find a slightly modified function directly from Lopez De Prado, with comments added for clarity:

def get_vol(prices, span=100, delta=pd.Timedelta(hours=1)):

# 1. compute returns of the form p[t]/p[t-1] - 1 # 1.1 find the timestamps of p[t-1] values

df0 = prices.index.searchsorted(prices.index - delta)

df0 = df0[df0 > 0] # 1.2 align timestamps of p[t-1] to timestamps of p[t]

df0 = pd.Series(prices.index[df0-1],

index=prices.index[prices.shape[0]-df0.shape[0] : ]) # 1.3 get values by timestamps, then compute returns

df0 = prices.loc[df0.index] / prices.loc[df0.values].values - 1 # 2. estimate rolling standard deviation

df0 = df0.ewm(span=span).std()

return df0

Adding Path Dependency: Triple-Barrier Method

To better incorporate the stop-loss and take-profit scenarios of a hypothetical trading strategy, we will modify the fixed-horizon labeling method so that it reflects which barrier has been touched first — upper, lower, or horizon. Hence the name: the triple-barrier method.

Triple-Barrier Label y=0 | Source: quantresearch.org

The labeling schema is defined as follows:

y=1 : top barrier is hit first

y=0 : right barrier is hit first

y=-1 : bottom barrier is hit first

What about the side of the bet?

The schema above works fine for long-only strategies, however things get more complicated when we allow for both long and short bets. If we are betting short, our profit/loss is inverted relative to the price action — we profit if the price goes down and we lose when the price goes up.

“Because I was inverted” — Maverick | Top Gun

In order to account for this, we can simply represent side as 1 for long and -1 for short. Thus we can multiply our returns by the side, so whenever we’re betting short the negative returns become positive and vice-versa. Effectively, we flip the y=1 and y=-1 labels if side=-1.

Let’s take a shot at the implementation (based on MLDP’s code).

First, we define the procedure for getting the timestamps of the horizon barriers:

def get_horizons(prices, delta=pd.Timedelta(minutes=15)):

t1 = prices.index.searchsorted(prices.index + delta)

t1 = t1[t1 < prices.shape[0]]

t1 = prices.index[t1]

t1 = pd.Series(t1, index=prices.index[:t1.shape[0]])

return t1

Now that we have our horizon barriers, we define a function to set upper and lower barriers based on the volatility estimates computed earlier:

def get_touches(prices, events, factors=[1, 1]):

'''

events: pd dataframe with columns

t1: timestamp of the next horizon

threshold: unit height of top and bottom barriers

side: the side of each bet

factors: multipliers of the threshold to set the height of

top/bottom barriers

''' out = events[['t1']].copy(deep=True)

if factors[0] > 0: thresh_uppr = factors[0] * events['threshold']

else: thresh_uppr = pd.Series(index=events.index) # no uppr thresh

if factors[1] > 0: thresh_lwr = -factors[1] * events['threshold']

else: thresh_lwr = pd.Series(index=events.index) # no lwr thresh for loc, t1 in events['t1'].iteritems():

df0=prices[loc:t1] # path prices

df0=(df0 / prices[loc] - 1) * events.side[loc] # path returns

out.loc[loc, 'stop_loss'] = \

df0[df0 < thresh_lwr[loc]].index.min() # earliest stop loss

out.loc[loc, 'take_profit'] = \

df0[df0 > thresh_uppr[loc]].index.min() # earliest take profit

return out

Finally, we define a function to compute the labels:

def get_labels(touches):

out = touches.copy(deep=True)

# pandas df.min() ignores NaN values

first_touch = touches[['stop_loss', 'take_profit']].min(axis=1)

for loc, t in first_touch.iteritems():

if pd.isnull(t):

out.loc[loc, 'label'] = 0

elif t == touches.loc[loc, 'stop_loss']:

out.loc[loc, 'label'] = -1

else:

out.loc[loc, 'label'] = 1

return out

Putting it all together:

data_ohlc = pd.read_parquet('data_dollar_ohlc.pq')

data_ohlc = \

data_ohlc.assign(threshold=get_vol(data_ohlc.close)).dropna()

data_ohlc = data_ohlc.assign(t1=get_horizons(data_ohlc)).dropna()

events = data_ohlc[['t1', 'threshold']]

events = events.assign(side=pd.Series(1., events.index)) # long only

touches = get_touches(data_ohlc.close, events, [1,1])

touches = get_labels(touches)

data_ohlc = data_ohlc.assign(label=touches.label)

Meta-Labeling

On a conceptual level, our goal is to place bets where we expect to win and not to place bets where we don’t expect to win, which reduces to a binary classification problem (where the losing case includes both betting on the wrong direction and not betting at all when we should have). Here’s another way to look at the labels we just generated:

A binary classification problem presents a trade-off between type-I errors (false positives) and type-II errors (false negatives). Increasing the true positive rate usually comes at the cost of increasing the false positive rate.

To characterize this more formally, let us first define:

ŷ ∈ {0, 1, -1} : prediction of the primary model for the observation

r : price return for the observation

Then at prediction time the confusion matrix of the primary model looks like the one below.

We are not too worried about the false negatives — we might miss a few bets but at least we’re not losing money. We are most concerned about the false positives — this is where we lose money.

To reflect this, our meta-labels y* can be defined according to the diagram:

y*=1:true positive

y*=0:everything but true positive

In effect, the primary model should have high recall — it should identify more of the true positives correctly at the expense of many false positives. The secondary model will then filter out the false positives of the first model.

Meta-Labeling Implementation

First, we create a primary model. Before we do so, an important preprocessing step is to make sure that our training data has balanced labels.

The labels in the original dataset are heavily dominated by 0 values, so if we train on those, we get a degenerate model that predicts 0 every time. We deal with this by applying the synthetic minority over-sampling technique to create a new training dataset where the label counts are roughly equal.

from imblearn.over_sampling import SMOTE

X = data_ohlc[['open', 'close', 'high', 'low', 'vwap']].values

y = np.squeeze(data_ohlc[['label']].values)

X_train, y_train = X[:4500], y[:4500]

X_test, y_test = X[4500:], y[4500:]

sm = SMOTE()

X_train_res, y_train_res = sm.fit_sample(X_train, y_train)

Next we fit a logistic regression model to our resampled training data. Note that at this point we should not expect our model to do well because we haven’t generated any features, but we should still see an improved F1 score when using meta-labeling over the baseline model.

from sklearn.linear_model import LogisticRegression

clf = LogisticRegression().fit(X_train_res, y_train_res)

y_pred = clf.predict(X_test)

We can see that our model predicts more 1’s and -1’s than there are in our test data. The blue portions of the leftmost and rightmost columns represent the false positives, which we intend to eliminate by meta-labeling and training the secondary model.

Let’s map our triple-barrier predictions into binary positive/negative meta-labels introduced earlier and check the confusion matrix:

def true_binary_label(y_pred, y_test):

bin_label = np.zeros_like(y_pred)

for i in range(y_pred.shape[0]):

if y_pred[i] != 0 and y_pred[i]*y_test[i] > 0:

bin_label[i] = 1 # true positive

return bin_label from sklearn.metrics import confusion_matrix

cm= confusion_matrix(true_binary_label(y_pred, y_test), y_pred != 0)

primary model

As expected, we see no false negatives and a lot of false positives. We will try to reduce the false positives without adding too many false negatives.

.

.

# generate predictions for training set

y_train_pred = clf.predict(X_train)

# add the predictions to features

X_train_meta = np.hstack([y_train_pred[:, None], X_train])

X_test_meta = np.hstack([y_pred[:, None], X_test])

# generate true meta-labels

y_train_meta = true_binary_label(y_train_pred, y_train)

# rebalance classes again

sm = SMOTE()

X_train_meta_res, y_train_meta_res = sm.fit_sample(X_train_meta, y_train_meta)

model_secondary = LogisticRegression().fit(X_train_meta_res, y_train_meta_res)

y_pred_meta = model_secondary.predict(X_test_meta)

# use meta-predictions to filter primary predictions

cm= confusion_matrix(true_binary_label(y_pred, y_test), (y_pred * y_pred_meta) != 0)

secondary model

The results in the secondary model show that we introduce a few false negatives, but we eliminate over 30% of false positives from the primary model. While it isn’t always a worthy trade-off, remember the context of trading — we miss out on some trading opportunities (false negatives), but that is a cheap price to pay for cutting down many trades that blow up in our faces (false positives). The classification report confirms our intuition that the efficiency of the classifier improves as measured by the F1 score.

# WITHOUT META-LABELING label precision recall f1-score support



0 1.00 0.66 0.79 1499

1 0.14 1.00 0.24 81



micro avg 0.67 0.67 0.67 1580

macro avg 0.57 0.83 0.52 1580

weighted avg 0.96 0.67 0.76 1580 # WITH META-LABELING label precision recall f1-score support



0 0.97 0.76 0.85 1499

1 0.12 0.59 0.20 81



micro avg 0.75 0.75 0.75 1580

macro avg 0.55 0.68 0.53 1580

weighted avg 0.93 0.75 0.82 1580

While neither model is great, remember that we are merely demonstrating a technique for improving classifier efficiency, which can conceivably work well on a larger dataset, better model, and more powerful features.

Meta-Labeling: Quantamental Approach

In general, the interpretation of meta-labeling + secondary model is to predict the confidence level of the primary model. In our example, both the primary and the secondary models were data-driven, but this doesn’t always have to be the case.

In addition to improving F1 scores, meta-labeling has another extremely powerful application — it can add a machine learning layer on top of non-ML models, including econometric forecasts, fundamental analysis, technical signals, and even discretionary strategies. This offers a powerful combination of human intuition/expertise and quantitative edge that is favored by many asset managers for its explainability and robustness.

Summary

Labeling observations is a crucial component for supervised learning. In this demo, we’ve developed an approach for labeling observations of a financial asset, as well as a meta-labeling technique to help achieve better F1 scores in classification problems. I encourage you to combine these labeling techniques with other datasets and parameters, and share your results. Thanks for reading and feel free to reach out with comments/suggestions!