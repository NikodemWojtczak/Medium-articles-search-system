Figure 1. Electricity demand versus time in LA. Note the sinusoidal pattern, with the summer season experiencing more demand because of air conditioning.

After the initial cleansing and preparation of the day I performed some exploratory data analysis to look at initial relationships between the independent (weather) and the dependent (electricity) variables. Figure 2 shows the Pearson correlation coefficients and Figure 3 the r² values between every feature and electricity demand. We see that daily cooling degree days have a strong positive correlation with electricity demand — the vast majority of electricity usage is during the summer months of LA for cooling.

# Code to produce figure 2

import pandas # df is our main dataset with weather + electricity data

print('DEMAND CORRELATIONS (PEARSON) FOR LA')

print(df.corr()['demand'].sort_values(ascending=False)[1:])

Figure 2. Pearson correlation coefficients for all weather features with electricity demand. Cooling degrees is a strong predictor of electricity demand.

# Code to produce figure 3

import scipy

import pandas as pd # get r^2 values per column and print

demand_r = {}

for col in df.columns:

if col != 'demand':

slope, intercept, r_value, p_value, std_err = scipy.stats.stats.linregress(df['demand'], df[col])

demand_r[col] = r_value**2 print('DEMAND CORRELATIONS (r^2) FOR LA')

demand_r_df = pd.DataFrame({'col': demand_r.keys(), 'r^2': demand_r.values()})

print(demand_r_df.sort_values(by='r^2', ascending=False))

Figure 3. r² values for all weather features with electricity demand.

Some of our weather features have collinearities; there are three features for pressure (station, sea level, altimeter) and additionally three for temperature (wet bulb, dry bulb, dew point). Collinearities make predictive modeling more difficult, so we drop the two temperature and pressure features with the lowest r² values.

Since people behave differently during the day versus the night regardless of how hot or cold it is that day, I added an ‘hourlytimeofday’ column, representing day or night (zero between 6AM and 6PM, one otherwise). The temperature data is important for identifying heating and cooling peaks, but the regression can start leaning on temperature as a proxy for daytime. This feature increased our multivariate regression r² considerably, signifying its importance.

Before using other machine learning techniques, we perform a multiple ordinary least squares (OLS) regression on all the features versus electricity demand to isolate insignificant features. Figure 4 shows the results. High p-value features confirm our intuitions — that heating is not important for LA. Sky condition, i.e., the cloud coverage, and wind speed are intuitively not important features for predicting electricity demand. These less-useful features are indicated by high p-values and any feature with a p-value greater than 0.1 was dropped from the data set.

Figure 4. Multivariate OLS regression for all the features + constant. Shown are a list of coefficients with errors, t-statistics, confidence intervals.

Machine Learning Modeling

Now we prepare our dataset for machine learning modeling. Our first step is to transform our time series data into a form suitable for supervised learning. Drawing heavily from this blog post, I transformed our dataset such that I used all of the features (i.e., electricity demand plus all the weather features) from the previous time step to predict electricity demand at the current time step. Since electricity demand from the hour before seems like a relevant proxy for the current electricity demand, this procedure seems intuitive. After reframing our data in this format, we split our data into a training and testing set, then begin evaluating our machine learning models. To make the analysis consistent with our later use of recurrent neural networks and long short-term memory, we designate the first year of data as the training set and the rest of the data as the testing set (~32% training and ~68% testing). This split avoids overfitting and a more traditional 80/20 split on training and testing yields virtually the same model performance. The general process for modeling is: 1.) use a min-max scaler on all the features to make them more comparable, 2.) fit to the training set, and 3.) compute the relevant metrics on the testing set and compare all of the models at the end. In our analysis we perform no hyperparameter tuning for the models (default settings of sklearn) and still achieve quite remarkable results.

I trained and tested a plethora of standard machine-learning models as described above using the sklearn module. The specific procedure and results of the modeling can be found in this IPython notebook. With straight, out-of-the-box models, I achieved impressive accuracy, with r² values over 0.95 for over half the models I tried. In addition, the training and testing r² values are within a few percent of each other, indicating that I’m not overfitting.

Besides out-of-the-box models, we also used recurrent neural networks (RNN) and long short-term memory (LSTM). A detailed walkthrough on the basics of RNNs and LSTM along with the modeling procedure can be found in this IPython notebook. Using this model, I achieved comparable performance to the best machine-learning models I tested previously. Figure 5 shows the loss (in mean-absolute error) versus epoch for training and testing sets using LSTM modeling. The testing loss remains a bit higher than the training loss, indicating that the model is not overfitting.

# Code to produce figure 5

from keras.models import Sequential

from keras.layers import Dense, LSTM

import matplotlib.pyplot as plt # design network

model = Sequential()

model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))

model.add(Dense(1))

model.compile(loss='mae', optimizer='adam')

# fit network

history = model.fit(train_X, train_y, epochs=50, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)

# plot history

plt.plot(history.history['loss'], label='train')

plt.plot(history.history['val_loss'], label='test')

plt.xlabel('Epoch')

plt.ylabel('Loss (MAE)')

plt.legend()

plt.show()

Figure 5. Loss measured in mean-absolute error (MAE) versus epoch for both training and testing of our LSTM network. Testing error remains slightly above training, indicating that we are not overfitting. The LSTM network was optimized using the Adam implementation of stochastic gradient descent with MAE loss.

EIA also provides a day-ahead electricity demand forecast that we would like to compare to our models. EIA uses various methods to forecast demand. The forecast depends heavily on weather forecasts and the day of the week. Using the same testing set we find that EIA’s day-ahead demand forecast achieves worse accuracy than all but one of our models, indicating the power of simple, out-of-the-box machine learning models on regression problems.

Results are summarized in Table 1 sorted by r² . In general, ensemble machine learning models perform better than others, although simple linear regression performs very well on its own. The training and testing errors for all models remain within the percent level of each other, indicating we are not substantially overfitting the data. Since cross-validation methods become trickier when dealing with correlated, time stream data, we leave out cross-validation in our analysis to keep all of our models comparable and consistent. We find that the gradient boosting model has the best performance, although the top five models are all similar in performance (differences of ~1% in performance between them).

Table 1. Table of results of machine learning models. The best performing model (gradient boosting) is highlighted.

With our best-performing model chosen (gradient boosting), we now want to see which features are most important in predicting electricity demand. This is shown in Figure 6. “Feature importance is calculated as the decrease in node impurity weighted by the probability of reaching that node. The node probability can be calculated by the number of samples that reach the node, divided by the total number of samples.” Demand during the previous hour is the most important feature for predicting demand at the current hour, but other proxies like cooling degree days are also a strong predictor as we expected.

Figure 6. Feature importance for each feature in the gradient boosting model. Demand at the previous time step is four times more important than cooling degree days. Reframing our supervised learning problem in this way greatly improved the predictive power of the models.

Conclusion

We show that outperforming EIA’s day-ahead electricity demand forecast is actually quite easy with machine learning. Remarkably, all but one of our models (k-NN) performance better than the EIA’s model. Relatively simple data scraping, cleaning, and machine-learning modeling from public data sources results in much more accurate modeling than traditional methods. We report an overall ~5% increase in performance over EIA’s model; energy companies could use these models to better brace for high-demand spikes in electricity and ultimately increase profits. Additional hyperparameter tuning and cross-validation techniques could be performed to increase performance now that the best model has been identified, although the increase would most likely be marginal at best and since our models already outperform the EIA’s, we conclude the report without tuning. Please check out my GitHub for this project for a more detailed report, code, plots, and documentation.