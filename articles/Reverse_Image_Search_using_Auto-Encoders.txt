Have you ever wondered how Google Reverse Image search work? How do they scan all the images and return appropriate results very fast? In this blog, we will make our own lightweight reverse search engine. We will be using AutoEncoders for this purpose.

What is an AutoEncoder?

AutoEncoders are a special type of feed-forward neural networks where the input is the same as output. They are trained in an unsupervised manner to learn the low-level features of an input. These low-level features are generally called as latent features. These latent features are then used to reconstruct the original input.

AutoEncoder consists of 3 major components

Encoder: Used to compress the input image

Latent Representation: The low-level features of an input which preserves much of the information

Decoder: Used to reconstruct the original input from the latent features.

In this case, the input is an image. The below diagram explains clearly how an autoencoder works.

What we will cover in this blog?

We will use the Fashion MNIST Dataset and preprocess it

Train an AutoEncoder to learn the latent representations of all the images

Index the latent features using Annoy

Find similar images for a given image

Let’s get started.

Fashion MNIST Dataset

Before we could load and process our data, it’s a good practice to have an understanding of the dataset, the number of classes, the number of samples in the dataset.

Fashion MNIST database consists of 60,000 training samples and 10,000 test samples of 28*28 grayscale images of fashion products. Each category has 6000 training samples and 1000 test samples. You can read more about the dataset here.

Import Libraries

import numpy as np

import matplotlib.pyplot as plt

import tensorflow as tf

from tensorflow import keras

from annoy import AnnoyIndex

import os

Download the Data

fashion_mnist = keras.datasets.fashion_mnist

(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

Preprocessing and Exploratory Data Analysis

The pixel values are between 0 and 255. We will scale the pixel values between 0 and 1.

# resize the pixel values between 0 and 255

train_images = train_images/255

test_images = test_images/255

Let’s map the class labels to the appropriate product category.

# different product categories in the dataset

labeldict = {

0: 'T-shirt/top',

1: 'Trouser',

2: 'Pullover',

3: 'Dress',

4: 'Coat',

5: 'Sandal',

6: 'Shirt',

7: 'Sneaker',

8: 'Bag',

9: 'Ankle boot'

} # no of times each product category is present in the dataset

category_counts = dict(zip(*np.unique(train_labels, return_counts=True)))

Let’s show some sample images.

plt.figure(figsize=(12,8))

for index in range(16):

rand_idx = np.random.randint(0,train_labels.shape[0])

plt.subplot(4,4,index+1)

plt.xticks([])

plt.yticks([])

plt.grid('off')

plt.imshow(train_images[rand_idx], cmap='Greys_r')

plt.xlabel(labeldict[train_labels[rand_idx]])

Helper Functions

Let’s define a utility function to plot the original and the reconstructed images.

def plot_images(original, reconstructed):



fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(12,4))



for images, axes in zip([original, reconstructed], axes):

for image, ax in zip(images, axes):

ax.imshow(image.reshape(28,28), cmap="Greys_r")

ax.get_xaxis().set_visible(False)

ax.get_yaxis().set_visible(False)



fig.tight_layout(pad=0.1)

Placeholders

Let’s define the placeholders for the inputs and targets. In this case, the inputs and targets are equal. We will also define a placeholder for batch-size.

def placeholders(image_size, n_channels ):

inputs = tf.placeholder(dtype=tf.float32, shape=[None, image_size, image_size,n_channels], name='inputs') targets = tf.placeholder(dtype=tf.float32, shape=[None, image_size, image_size, n_channels], name='targets') batch_size = tf.placeholder(dtype=tf.int32, name='batch_size') return inputs, targets, batch_size

Encoder and Decoder Network

We will use a Convolutional Neural Network to train our model. The encoder network converts the 28*28 image into a 4*4*8 low-level representation. The decoder uses this low-level representation to reconstruct the 28*28 image. The parameters for the network is taken from here.

def encoder_decoder_network(X):



#ENCODER NETOWRK



# X's shape - 28*28*1

W1 = tf.get_variable("W1", shape=[3,3,1,16], initializer=tf.contrib.layers.xavier_initializer(seed=0))

# 28*28*16

conv1 = tf.nn.conv2d(X, W1, strides=[1,1,1,1], padding='SAME')

relu1 = tf.nn.relu(conv1)

#14*14*16

pool1 = tf.nn.max_pool(relu1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')



#14*14*8

W2 = tf.get_variable("W2", shape=[3,3,16,8], initializer=tf.contrib.layers.xavier_initializer(seed=0))

conv2 = tf.nn.conv2d(pool1, W2, strides=[1,1,1,1], padding='SAME')

relu2 = tf.nn.relu(conv2)

#7*7*8

pool2 = tf.nn.max_pool(relu2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')

#7*7*8

W3 = tf.get_variable("W3", shape=[3,3,8,8], initializer=tf.contrib.layers.xavier_initializer(seed=0))

conv3 = tf.nn.conv2d(pool2, W3, strides=[1,1,1,1], padding='SAME')

relu3 = tf.nn.relu(conv3)

#4*4*8

pool3 = tf.nn.max_pool(relu3, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME', name='encodings')



encoded = pool3



# the image is now 4*4*8



# DECODER NETWORK





upsample1 = tf.image.resize_nearest_neighbor(encoded, (7,7))

#7*7*8

W4 = tf.get_variable("W4", shape=[3,3,8,8], initializer=tf.contrib.layers.xavier_initializer(seed=0))

conv4 = tf.nn.conv2d(upsample1, W4, strides=[1,1,1,1], padding='SAME')

relu4 = tf.nn.relu(conv4)



upsample2 = tf.image.resize_nearest_neighbor(relu4, (14,14))

# 14*14*8

W5 = tf.get_variable("W5", shape=[3,3,8,8], initializer=tf.contrib.layers.xavier_initializer(seed=0))

conv5 = tf.nn.conv2d(upsample2, W5, strides=[1,1,1,1], padding='SAME')

relu5 = tf.nn.relu(conv5)



# 28*28*8

upsample3 = tf.image.resize_nearest_neighbor(relu5, (28,28))



W6 = tf.get_variable("W6", shape=[3,3,8,16], initializer=tf.contrib.layers.xavier_initializer(seed=0))

conv6 = tf.nn.conv2d(upsample3, W6, strides=[1,1,1,1], padding='SAME')

relu6 = tf.nn.relu(conv6)



W7 = tf.get_variable("W7", shape=[3,3,16,1], initializer=tf.contrib.layers.xavier_initializer(seed=0))

conv7 = tf.nn.conv2d(relu6, W7, strides=[1,1,1,1], padding='SAME')



logits = conv7



decoded = tf.nn.sigmoid(logits, name='decoded')



return encoded, decoded, logits

Define the training operations

We then compute the error between the original and the reconstructed image. As the objective is to minimize the error, we will use Adam Optimizer algorithm to learn the parameters of the network for which the error is minimum.

def train_operations(logits, targets, learning_rate): # define the loss

loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=targets)) # use adam optimizer for faster convergence

training_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)

return loss, training_op

Training the model

Next, we will define a function which will be used to train our model. Instead of using feed_dict, we will use the recommended way of using Dataset and Iterators to feed data to the model. If you are not aware of Dataset and Iterators you can refer to this blog.

Once the training is over, we will also save the model so that we can restore it afterwards.

def train_model(epochs, image_size, n_channels, batch_size, learning_rate, model_save_path):



# reset the graphs

tf.reset_default_graph()



# get the placeholders

inputs, targets, batch_op = placeholders(image_size, n_channels)



# create a Dataset from the input data

dataset = tf.data.Dataset.from_tensor_slices((inputs,targets))



# create batches of data

dataset = dataset.batch(batch_size)



# define an iterator to consume the data

iterator = tf.data.Iterator.from_structure(dataset.output_types,dataset.output_shapes)



train_initializer = iterator.make_initializer(dataset, name='init_iterator')



# get the batch of data using get_next

input_batch, target_batch = iterator.get_next()



encoded, decoded, logits = encoder_decoder_network(input_batch)

loss, training_op = train_operations(logits, target_batch, learning_rate)



saver = tf.train.Saver()

with tf.Session() as sess:

sess.run(tf.global_variables_initializer())

for epoch in range(epochs):

epoch_loss = 0



# run the initializer

sess.run(train_initializer, feed_dict = {

inputs: train_images.reshape(-1, image_size, image_size, n_channels),

targets : train_images.reshape(-1, image_size, image_size, n_channels),

batch_op: batch_size

})



try:

while True:

batch_loss, _ = sess.run([loss, training_op])

epoch_loss += batch_loss

except tf.errors.OutOfRangeError:

pass



print("Epoch {}/{}: Loss is {:.3f}".format(epoch+1, epochs, epoch_loss))





print("Training over

")

# save the model

saver.save(sess,model_save_path)



print("Model saved at {}".format(model_save_path))

Model parameters

We will train the model in batches of 400 and for a total of 20 epochs.

epochs = 20

batch_size = 400

image_size = 28

n_channels = 1

learning_rate = 0.001

Running the model

Before running the model, we will create a checkpoints directory to save our model.

if not os.path.exists('checkpoints'):

os.mkdir('checkpoints')



# checpoint directory

chkpt_dir = os.path.join(os.getcwd(), 'checkpoints')

# path to save the model

model_save_path = os.path.join(chkpt_dir, 'fashion-mnist.chkpt')



train_model(epochs=epochs,

batch_size=batch_size,

image_size=image_size,

n_channels= n_channels,

learning_rate = learning_rate,

model_save_path = model_save_path)

Loss

Epoch 1/20: Loss is 64.375

Epoch 2/20: Loss is 48.220

Epoch 3/20: Loss is 46.779

Epoch 4/20: Loss is 46.140

Epoch 5/20: Loss is 45.726

Epoch 6/20: Loss is 45.435

Epoch 7/20: Loss is 45.215

Epoch 8/20: Loss is 45.031

Epoch 9/20: Loss is 44.868

Epoch 10/20: Loss is 44.724

Epoch 11/20: Loss is 44.593

Epoch 12/20: Loss is 44.470

Epoch 13/20: Loss is 44.357

Epoch 14/20: Loss is 44.251

Epoch 15/20: Loss is 44.152

Epoch 16/20: Loss is 44.060

Epoch 17/20: Loss is 43.975

Epoch 18/20: Loss is 43.894

Epoch 19/20: Loss is 43.820

Epoch 20/20: Loss is 43.750

Visualize the reconstructed images

Let’s define a function which takes the path of the file where the model is saved and a list of images which needs to be reconstructed.

def test_network(model_path, images):

with tf.Session() as sess:

saver = tf.train.Saver()

saver.restore(sess, model_path)

default_graph = tf.get_default_graph()

inputs = default_graph.get_tensor_by_name('inputs:0')

targets = default_graph.get_tensor_by_name('targets:0')



test_iterator_init = default_graph.get_operation_by_name('init_iterator')

decoded = default_graph.get_tensor_by_name('decoded:0')

reconstructed =[]

sess.run(test_iterator_init, feed_dict={

inputs:images,

targets:images

})

try:

while True:

reconstructed.append(sess.run(decoded))

except tf.errors.OutOfRangeError:

pass

return reconstructed

We restored the various tensors and operations defined during training. The inputs and targets tensor were used to feed the images to the network. The decoded tensor is run to get back the reconstructed images.

Let’s pass some images from the test dataset to the model and visualize their reconstructed images.

test_sample_images = test_images[:10]

test_sample_images = test_sample_images.reshape(-1, image_size, image_size, n_channels)

reconstructed_images = test_network(model_save_path, test_sample_images)

reconstructed_images = np.array(reconstructed_images).reshape(10,28,28,1)

plot_images(test_sample_images, reconstructed_images)

The model has done a decent job of reconstructing the image.

Latent Features

Now that we are satisfied with the trained model, let’s compute the latent representations of all the training samples. For that, we will define a function which takes the path of the file where the model is saved, a list of images for which we want to get the latent representations and the size of the encoded images.

def get_encodings(model_path, images, encoding_vector_length):

with tf.Session() as sess:

saver = tf.train.Saver()

saver.restore(sess, model_path)

default_graph = tf.get_default_graph()

inputs = default_graph.get_tensor_by_name('inputs:0')

targets = default_graph.get_tensor_by_name('targets:0')



iterator_init = default_graph.get_operation_by_name('init_iterator')

encoded = default_graph.get_tensor_by_name('encodings:0')

encoding_vectors =[]

sess.run(iterator_init, feed_dict={

inputs:images,

targets:images

})

try:

while True:

encoding_vectors.append(sess.run(encoded))

except tf.errors.OutOfRangeError:

pass

return np.array(encoding_vectors).reshape(-1, encoding_vector_length)

Annoy

Approximate Nearest Neighbors(Annoy) is a library used to search for points in space that are closer to a given query point. We will use Annoy to save all the latent representation of all the training samples.

Next, given a query image, we will compute the latent representation for the given image and compare it with all the encoded representations to find similar images. If you want to learn more about how Annoy works, please refer to this wonderful blog post.

Let’s define the parameters we need to construct the annoy index.

encoding_vector_length: the size of the encoded representation of the image

annoy_file_name: name of the file in which the annoy index will be saved

num_trees: builds a forest of n_trees. Higher the number of trees, more the precision.

# our encoded image is of the shape 4*4*8, hence it's represted by a # vector of length 128

encoding_vector_length = 128

annoy_file_name = 'fashion-mnist.annoy.index'

num_trees = 10

Create the annoy index

def build_annoy_index(encoding_dim, num_trees, annoy_index_file, encodings):

ann = AnnoyIndex(encoding_dim)

for index, encoding in enumerate(encodings):

ann.add_item(index, encoding) # builds a forest of num_trees, higher the number of trees, higher the precision

ann.build(num_trees)



#save the index to a file

ann.save(annoy_index_file)

print("Created Annoy Index Successfully")

# resize the training images

train_images = train_images.reshape(train_images.shape[0], image_size, image_size, n_channels) # get the encoded representations of all the training samples

encodings = get_encodings(model_save_path, train_images, encoding_vector_length) # create the annoy index

build_annoy_index(encoding_vector_length, num_trees, annoy_file_name, encodings)

Let’s define a function to get similar images for a given query image. The number of similar images to be searched is denoted by the parameter n_similar

def get_similar_images(image, n_similar=10): # get the encoded representation of the image

encoding = get_encodings(model_save_path,image.reshape(-1, image_size, image_size,n_channels), encoding_vector_length) # Load the annoy index

saved_ann = AnnoyIndex(encoding_vector_length)

saved_ann.load(annoy_file_name) # get the nearest images

#get_nns_by_vector returns the indices of the most similar images

nn_indices = saved_ann.get_nns_by_vector(encoding[0], n_similar) print("Similar images are")

for i, index in enumerate(nn_indices,1):

image = train_images[index].reshape(28,28)

plt.subplot(2,5,i)

plt.xticks([])

plt.yticks([])

plt.imshow(image, cmap='Greys_r')

Query Image

sample_image = test_images[0]

print("Sample Image")

plt.imshow(sample_image, cmap='Greys_r')

Similar Images

get_similar_images(sample_image)

All the images returned are of the same class as that of the query image.