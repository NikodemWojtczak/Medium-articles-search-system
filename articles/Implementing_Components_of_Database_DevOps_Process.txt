Implementing Components of Database DevOps Process

Let’s look at how some components of the database DevOps process can be implemented, on the example of a database of potential employees. Evgeniy Gribkov · Follow Published in Towards Data Science · 7 min read · Dec 20, 2019 -- Listen Share

In recent years, database DevOps has become one of the most demanded processes. Even small companies can’t really work effectively without it, and most IT companies have already implemented DevOps database automation and made it a crucial part of their work process.

So, what exactly is database DevOps?

The main role of the database DevOps process is to continuously deliver changes from development and testing environments to the production environment, monitoring the state of the production environment, and providing the ability to rollback changes when needed. After this, the changes are transferred to the production environment management service. Basically, the DevOps process connects the development and testing with various types of administration (system, database, etc.) and other production environments and system management processes. DevOps database automation specifically includes automation at all environments and makes development and admin tasks for a database simpler and safer.

Usually, the database DevOps process is divided into two parts:

1. Before production — everything that happens before actually committing changes to the production environments. This includes the following stages:

1.1 Compiling and documenting the code

1.2 Filling the database with test data and running auto tests

1.3 Applying the changes from development environments to testing environments

1.4 Filling the database with test data

1.5 Deciding whether the changes are ready to be deployed to production environments

The first part of the process is mostly facilitated by the work of developers and testers, with stage 1.3 being implemented by DevOps engineers. The stages 1.2 through 1.4 can be performed in any order based on task specification and how the DevOps process itself is established. The idea is to successfully pass all the steps from compiling the code to deciding if it’s ready to be deployed to production.

After stage 1.3, all sorts of testing are performed on the solution (stress, functional, integration testing, and others). If any major deviation from working criteria is detected (for example, if errors or performance issues are found), usually it is decided at stage 1.5 that the code is not ready to be deployed to production and so should be sent back to development. After this, the process starts over again from stage 1.1. Things are similar with stage 1.2.

Stage 1.5 can also be failed due to reasons not related to stages 1.2 or 1.3. Only after stage 1.5 is passed, the DevOps process moves to the second (and the most important) part:

2. Implementation — moving the solution to production. This includes the following stages:

2.1 Deployment

2.2 Primary monitoring

2.3 Rolling back the deployment or moving the changes to production environment management services.

Stage 2.1 is usually started manually by combined efforts of DevOps specialists and production environment management service specialists to avoid issues on production. It would be nice to mention that stage 2.1 includes stage 1.3 — the only difference being that the changes are deployed to production. If deployment fails (stage 2.1), the issues are located and then either stage 2.1 is repeated or the DevOps process is returned to the beginning of part 1.

On the other hand, if deployment is successful but primary monitoring fails, it is usually decided that the deployment should be rolled back (stage 2.3), with the DevOps process returned to its first part.

Only if stages 2.1 and 2.2 are completely successful when assessed against all the necessary criteria and no issues are detected, the changes are handed to the production environment management service (stage 2.3) for continuous maintenance by the administration. This is where an iteration of the DevOps process ends.

It’s important to note that this process happens continuously, iteration by iteration, thus minimizing the time it takes to deliver the code from development environments to testing environments and vice versa or between development or testing environments. The DevOps process also shortens the time of delivery to production environments with the ability to monitor and rollback the changes.

PowerShell scripts creation to organize the database DevOps process

Let’s look at how some components of the database DevOps process can be implemented. In this example, we’ll use a database of potential employees.

We’ll look into the following aspects and how to generate PowerShell scripts for them to organize the database DevOps process. dbForge DevOps Automation for SQL Server and various dbForge tools will help us with this:

1. Before production:

1.1 Code compilation and documentation (SQL Complete, Source Control, Documentation)

1.2 Filling the database with test data and running auto tests (Data Generator, Unit Test)

1.3 Deploying the solution from development environments to testing environments (Schema Compare, Data Compare)

1.4 Filling the database with test data

1.5 Deciding whether the code is ready to be deployed to production (this is a managerial responsibility, so we won’t be talking about it here)

2. Implementation:

2.1 Deployment (database schema comparison, data comparison, dbForge Data Pump)

2.2 Primary monitoring (dbForge Event Profiler, dbForge Monitor, dbForge Data Pump)

2.3 Rolling back the deployment or handing the changes to the production environment management services (this can be done in multiple ways, so we will also omit its discussion, but transaction rollback can be performed using dbForge Transaction Log)

We won’t be delving into stage 1.1 and the second part too deeply in this article. However, one important feature of SQL Complete should be mentioned. Specifically, to format a folder with scripts, we can use the following PowerShell script:

$result = Invoke-DevartFormatScript -Source $scriptFolder

Here, $scriptFolder is the full path to the necessary folder with scripts that needs to be formatted. Also, the Invoke-DevartFormatScript cmdlet is used to invoke the formatting functionality of the SQL Complete tool. As we can see from the script, the cmdlet only has one parameter — the path to the folder that contains database schema creation or update scripts.

Now we’ll look in detail at the stages 1.2–1.4. As these stages can be performed in any order during the database DevOps process, we’ll arrange them as follows:

Filling the database with test data (dbForge Data Generator) Running auto test (dbForge Unit Test) Deploying the solution from development environments to testing environments ((database schema comparison, data comparison)

We’ll need to either save a data generation project or create a new one with necessary parameters and save it as JobEmplDB.dgen.

Now we can use the Invoke-DevartDatabaseTests cmdlet to invoke the data generation process according to the JobEmplDB.dgen project:

Invoke-DevartDatabaseTests -InputObject $ConnectionString -DataGeneratorProject $JobEmplDBFolder -InstalltSQLtFramework -RewriteReport -UnInstalltSQLtFramework -IncludeTestData

Here, replace $ConnectionString with the connection string and $JobEmplDBFolder with the full name of the JobEmplDB.dgen file, i.e. the full path to this file including the actual file name and extension.

Now, let’s take a look at how we can use a PowerShell script to run auto tests.

We’ll use the Invoke-DevartExecuteScript and Invoke-DevartDatabaseTests cmdlets for this:

Invoke-DevartExecuteScript -Input $UnitTestsFolder -Connection $ConnectionString Invoke-DevartDatabaseTests -InputObject $ConnectionString -InstalltSQLtFramework -OutReportFileName $TestReportOutputFileName -ReportFormat JUnit

The first cmdlet creates tests for the database from a folder, where:

$UnitTestsFolder — the path to the folder containing unit tests; $ConnectionString — the connection object for the database for which the tests will be created (in our case, it’s the JobEmplDB database)

The second cmdlet runs unit tests against the database:

InputObject $ConnectionString is the connection string for the database we need to run the tests against. The InstalltSQLtFramework parameter specifies that the tSQLt framework should be installed prior to running the tests.

The OutReportFileName $TestReportOutputFileName parameter is the path to the output test report file, and ReportFormat JUnit specifies the format of this file

In the future, the Invoke-DevartDatabaseTests cmdlet will be divided into two separate parts — one for running auto tests, and the other for generating test data.

To transfer the schema changes, we can use the following PowerShell script using the Invoke-DevartSyncDatabaseSchema cmdlet:

$syncResult = Invoke-DevartSyncDatabaseSchema -Source $sourceConnection -Target $targetConnection

Here, $sourceConnection is the connection string for the source database, and $targetConnection is the connection string for the target database. The $syncResult variable will contain the result of database schema synchronization.

Currently, there is no cmdlet for synchronizing data between databases, but there will be one in the future.

For now, we can use the Invoke-DevartDataImport cmdlet that launches dbForge Data Pump to transfer the data:

Invoke-DevartDataImport -Connection $connection -TemplateFile

Here, $connection is the connection string for the JobEmplDB database, and $importDataFileName is the full name of the file that is used to create import data. There is also a line of other useful cmdlets from Devart that will help implement various DevOps process components using PowerShell scripts. You can learn more about them here.

As we can see, database changes (both in the schema and in the data) can be automatically transferred between environments with the help of dbForge Schema Compare and dbForge Data Compare (dbForge Data Compare can only work in manual mode for now). We can also use the dbForge Data Pump component cmdlets for the purpose of transferring data.

With the help of PowerShell scripts, we can automate the entire DevOps process — starting from code compilation to deploying the code to production and primary monitoring.