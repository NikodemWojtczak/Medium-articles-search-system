Supervised vs. Unsupervised

A general definition of machine learning is that we are using data to train a model that is trying to learn the logic of a complex decision. Generally the decision is so complex that the programmer either doesn’t know the correct logic themselves, doesn’t know how to translate that logic into a computer-friendly format, or both. If the logic were simple and known by the programmer they could just program that in directly. Learning the logic of a complex decision is typically accomplished through the use of a dataset. Depending on the type of data we receive there are two different ways we can learn our decision logic.

In supervised learning we have a training dataset of observations which are labeled. Labeled data means that the ‘right’ answer is provided to us. In more technical terms, we know the expected output for that particular input. Having labels is important for two reasons: First, say we generate a logic, give it inputs, and generate outputs (predictions). Since we have labels, we can compare the predictions of our logic to the expected outputs (labels) and see if our predictions seem accurate or not.

Accuracy on the training data is useful because we assume the training data closely resembles the unlabeled, real-world data we eventually want to use this logic on. Good accuracy on the training data usually translates into good real-world accuracy as well. If the training data does not resemble the real world then it’s essentially impossible to generate good predictions. This is often described by the phrase “garbage in, garbage out.”

If we find that our logic is not outputting good predictions then we can use the labels for their second purpose: guiding the improvement of our logic. In fact, this is precisely where the ‘supervised’ term comes from: Algorithms like this typically have a learner element, which is the model/logic of the problem. They also have a teacher element which supervises the learner and uses the labels to provide feedback on the predictions.

Let’s walk through a brief example of this. Suppose you wanted to be able to predict the gas mileage of cars accurately, how could you approach that? One thing you could easily do is just to point at cars on the road and say “I think that gets X mpg.” Without any kind of feedback, however, it’s pretty unlikely that would ever improve your guesses. Not only would you not know if your guesses were close or not, you also wouldn’t know how they were wrong; were they too high or too low? Were they in the ballpark or way off?

Without feedback the process stalls, but we can un-stall that by gathering more data. Suppose, then, that you went to a car show where there were hundreds of cars with (literal) labels in the window listing, among other things, the gas mileage. Now you could look at a car, make a prediction, and then compare that prediction with the actual gas mileage listed in the window. Suppose that at first you guessed that sedans got around 100mpg, and SUVs were even better at around 120mpg. Exposure to even two or three different labeled examples would be enough to make you realize that both guesses were way too high, and that in fact SUVs get worse gas mileage than sedans (in general).

The more feedback you get the more you would recognize trends like larger cars having worse gas mileage, larger engines having worse gas mileage, and newer cars getting better gas mileage than older cars. In the long run all these patterns translate to making better overall predictions. Not only that, they give you a pretty good idea of the kind of errors to expect when you predict unlabeled data. If your average error on predictions is 5mpg then when you make a new prediction you should be confident that it’s close to the true value. If your average error is 20mpg then maybe it’s not possible to make accurate predictions with the data available. In either case supervision gives us crucial information about our predictive model.

Unsupervised learning is… well not supervised. Basically “unsupervised” becomes a catch-all term for anything that doesn’t follow the teacher/learner pattern, and has no element of supervision. In fact, by some definitions unsupervised learning isn’t even machine learning at all because without a “teacher” element there’s similarly no “learner” element. By that argument unsupervised learning is just statistical analysis.

By most definitions, however, unsupervised learning still counts as machine learning, and we are covering it here. It’s hard to explain unsupervised learning like we did with supervised learning above, because each unsupervised approach tends to solve a different problem and makes different assumptions. The best way I can think to explain it is to say that, in supervised learning, we are looking for patterns within the data so we can connect those patterns to the labels in a way that hopefully allows us to predict the labels accurately.

In unsupervised learning we are still looking for patterns, but we won’t have labels to associate those patterns with. So unsupervised approaches tend to look for specific types of patterns based on the assumption that those patterns are usually important. Finding clusters of similar points, for example, typically means that those points have something in common, even if we don’t know what that is. Finding points that are extreme outliers to the rest of the data typically means there’s a feature of those points that is very unique compared to the rest of the data. Without further investigation we generally won’t know what that unique feature is, but identifying outlier points is the necessary first step in that process.