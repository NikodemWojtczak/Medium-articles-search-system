Regularization for Machine Learning Models

A common problem in machine learning is overfitting, where a model falsely generalizes noise in the training data:

A popular approach to remedy this problem and make the model more robust is regularization: A penalty term is added to the algorithm’s loss function. This changes the model’s weights which result from minimizing the loss function.

The most popular regularization techniques are Lasso, Ridge (aka Tikhonov) and Elastic Net. For the exemplary case of simple linear regression with only one weight parameter w (the slope of the linear fit), their penalty terms look like this (including a scaling parameter λ):

Lasso (L1) : λ·|w|

: λ·|w| Ridge (L2) : λ·w²

: λ·w² Elastic Net (L1+L2): λ₁·|w| + λ₂·w²

The different terms have different effects: Compared to L1, the quadratic L2 regularization becomes negligible at small weights (close to zero), but stronger at large weights. This leads to the following behaviours, casually phrased: