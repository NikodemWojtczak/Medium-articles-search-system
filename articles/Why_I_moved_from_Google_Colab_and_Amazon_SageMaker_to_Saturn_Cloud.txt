When it comes to scientific experimentation and collaboration, people tend to need the same thing: an easy-to-use interface to hack and optimize their algorithms, a data input-output system, and the support for their preferred programming language. A natural solution to these problems emerged in 2011 with the release of Jupyter, an extremely versatile web application that allows you to create a notebook file that serves as an interactive code interface, a data visualization tool, and a markdown editor.

Jupyter Cloud Solutions

There are many ways to share a static Jupyter notebook with others, such as posting it on GitHub or sharing an nbviewer link. However, the recipient can only interact with the notebook file if they already have the Jupyter Notebook environment installed. But what if you want to share a fully interactive Jupyter notebook that doesn’t require any installation? Or, you want to create your own Jupyter notebooks without installing anything on your local machine?

The realization that Jupyter notebooks have become the standard gateway to machine learning modeling and analytics amongst data scientists has fueled a surge in software products marketed as “Jupyter notebooks on the cloud (plus new stuff!)”. Just from memory, here’s a few company offerings and startup products that fit this description in whole or in part: Kaggle Kernels, Google Colab, AWS SageMaker, Google Cloud Datalab, Domino Data Lab, DataBrick Notebooks, Azure Notebooks…the list goes on and on. Based on my conversations with fellow data science novices, the 2 most popular Jypyter cloud platforms seem to be Google Colab and Amazon SageMaker.

Google Colab

Google Colab is ideal for everything from improving your Python coding skills to working with deep learning libraries, like PyTorch, Keras, TensorFlow, and OpenCV. You can create notebooks in Colab, upload notebooks, store notebooks, share notebooks, mount your Google Drive and use whatever you’ve got stored in there, import most of your favorite directories, upload your personal Jupyter Notebooks, upload notebooks directly from GitHub, upload Kaggle files, download your notebooks, and do just about everything else that you might want to be able to do.

Visually, the Colab interface looks quite similar to the Jupyter interface. However, working in Colab actually feels very dissimilar to working in the Jupyter Notebook:

Most of the menu items are different.

Colab has changed some of the standard terminologies (“runtime” instead of “kernel”, “text cell” instead of “markdown cell”, etc.)

Colab has invented new concepts that you have to understand, such as “playground mode.”

Command mode and Edit mode in Colab work differently than they do in Jupyter.

A lot has been written about Google Colab troubleshooting, so without going down that rabbit hole, here are a few things that are less than ideal. Because the Colab menu bar is missing some items and the toolbar is kept very simple, some actions can only be done using keyboard shortcuts. You can’t download your notebook into other useful formats such as an HTML webpage or a Markdown file (though you can download it as a Python script). You can upload a dataset to use within a Colab notebook, but it will automatically be deleted once you end your session.

In terms of the ability to share publicly, if you choose to make your notebook public and you share the link, anyone can access it without creating a Google account, and anyone with a Google account can copy it to their own account. Additionally, you can authorize Colab to save a copy of your notebook to GitHub or Gist and then share it from there.

In terms of the ability to collaborate, you can keep your notebook private but invite specific people to view or edit it (using Google’s familiar sharing interface). You and your collaborator(s) can edit the notebook and see each other’s changes, as well as add comments for each other (similar to Google Docs). However, your edits are not visible to your collaborators in real-time (there’s a delay of up to 30 seconds), and there’s a potential for your edits to get lost if multiple people are editing the notebook at the same time. Also, you are not actually sharing your environment with your collaborators (meaning there is no syncing of what code has been run), which significantly limits the usefulness of the collaboration functionality.

Colab does give you access to a GPU or a TPU. Otherwise, Google does not provide any specifications for their environments. If you connect Colab to Google Drive, that will give you up to 15 GB of disk space for storing your datasets. Sessions will shut down after 60 minutes of inactivity, though they can run for up to 12 hours.

The greatest strength of Colab is that it’s easy to get started, since most people already have a Google account, and it’s easy to share notebooks since the sharing functionality works the same as Google Docs. However, the cumbersome keyboard shortcuts and the difficulty of working with datasets are significant drawbacks. The ability to collaborate on the same notebook is useful; but less useful than it could be since you’re not sharing an environment and you can’t collaborate in real-time.

Amazon SageMaker

Amazon SageMaker is a fully managed machine learning service that helps data scientists and developers to quickly and easily build & train models, then directly deploy them into a production-ready hosted environment. It provides an integrated Jupyter authoring notebook instance for easy access to your datasets for exploration/analysis, so you don’t have to manage servers. It also provides common ML algorithms that are optimized to run efficiently against extremely larger data in a distributed environment. With native support for bring-your-own algorithms and frameworks, Amazon SageMaker offers flexible distributed training options that adjust to your specific workflows.

First, you spin up a so-called “notebook instance” which will host the Jupyter Notebook application itself, all the notebooks, auxiliary scripts, and other files. No need to connect to that instance (actually you cannot, even if wanted to) or set it up in any way. Everything is already prepared for you to create a new notebook and use it to collect and prepare some data, to define a model, and to start the learning process. All the configuration, provisioning of compute instances, moving of the data, etc. will be triggered literally with a single function call. This nifty process dictates a certain approach to defining the models and organizing the data.

SageMaker is built on top of other AWS services. Notebook, training, and deployment machines are just ordinary EC2 instances running specific Amazon Machine Images (AMI). And data (also results, checkpoints, logs, etc.) is stored in S3 object storage. This might be troubling if you are working with images, videos or any huge datasets. The fact is that you have to upload all your data to S3. When you configure the training you tell SageMaker where to find your data. SageMaker then automatically downloads the data from S3 to every training instance before starting the training. Every time. For a reference, it takes around 20 minutes to download 100Gb worth of images. Which means that you have to wait at least 25 minutes before the training begins. Good luck debugging your model! On the other hand, when all preliminary trials are done elsewhere and your model is already polished, the training experience is very smooth. Simply upload your data to S3 and get interim results from there as well.

Another aspect for consideration is pricing. Notebook instances can be very cheap, especially when there is no need to pre-process the data. Training instances, on the other hand, may easily burn a hole in your pocket. Check here all the prices, as well as the list of regions where SageMaker has already been launched.

Introducing Saturn Cloud

Saturn Cloud is a new kid on the block that provides data scientists who are not interested in setting up infrastructure but care more about doing data science easily. More specifically, the platform helps manage Python environments in the cloud.

You can get started with the free tier after signing up for an account. In the dashboard, you can create a Jupyter Notebook for a project by choosing the disk space and the size of your machine. The configurations cover requirements for a lot of practical data science projects. Furthermore, you can define auto-shutoff duration for your project, which would keep your project from shutting down because of inactivity.

It’s extremely easy to share notebooks via Saturn Cloud. I did a previous project that explores the Instacart Market Basket Analysis challenge, and you can view the public notebook here: https://www.saturncloud.io/yourpub/khanhnamle1994/instacart-notebooks/notebooks/Association-Rule-Mining.ipynb. I especially enjoyed how the code blocks and the visualization get rendered without any clutter as we see with the Google Colab notebooks. It looks just like a report as it is intended to be. I also like the “Run in Saturn” option provided where users can just go and click over to run this code by themselves without any need for an explicit login.

Overall, using Saturn Cloud makes it easy to share your notebooks with other teammates without having to deal with the hassle of making sure they have all the correct libraries installed. This sharing capability is superior compared to Google Colab.

Furthermore, for those of you who have had the issue of running notebooks locally and running out of memory, it also allows you to spin up Virtual Machines with the memory and RAM required and only pay for what you use. This cost-association is a tremendous advantage compared to Amazon SageMaker.

There are also a few other bells and whistles that really mitigate some of the complexities of work typically done from a DevOps perspective. That’s what makes tools like this great. In many ways, until you work in a corporate environment where even sharing a basic Excel document can become cumbersome, it is almost hard to explain to younger data scientists why this is so great.

Conclusion

Saturn Cloud is still far from being production-ready, but it’s a very promising solution. I personally think that it is the only solution so far that comes close to the ease-of-use of a local Jupyter server with the added benefits of cloud hosting (CVS, scaling, sharing, etc.).

The proof of concept is indeed there, and it merely lacks some polish around the angles (more language support, better version control, and easier point-and-click interface). I am excited to see the future version(s) of the Saturn platform!

References