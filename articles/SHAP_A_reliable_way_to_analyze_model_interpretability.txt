by Steven Wright on Unsplash

I had started this series of blogs on Explainable AI with 1st understanding what’s the balance between accuracy vs interpretability, then moving on to explaining what are some of the rudimentary model agnostic techniques to understand reasons behind model predictions, and finally had explained the know hows of LIME. Below are the links to these blogs for your quick reference:

1. The balance: Accuracy vs. Interpretability

2. How to interpret machine learning models?

3. LIME: Explaining predictions of machine learning models

In this blog, I will be talking about one of the most popular model agnostic technique that is used to explain predictions. SHAP stands for SHapley Additive exPlanations.

Shapely values are obtained by incorporating concepts from Cooperative Game Theory and local explanations. Given a set of palyers, Cooperative Game Theory defines how well and fairly to distribute the payoff amongst all the payers that are working in coordination. The analogy here is: players are equivalent to independent features and payoff is the difference between the average prediction of the instance minus the average prediction of all instances.

SHAP values for each feature represent the change in the expected model prediction when conditioning on that feature. For each feature, SHAP value explains the contribution to explain the difference between the average model prediction and the actual prediction of the instance. Picking up the same example from previous post, we want to predict the probability of the instance to default credit card payment given age, gender, and limit balance as independent features.

Steps to compute SHAP values for the independent feature: age.

For every instance, gender and limit balance are kept intact Value of age is replaced from the randomly selected value from the distribution The model predicts the probability to default for this particular simulated instance The difference between the average prediction across all instances and this particular instance is attributed to randomly sampled age independent feature Steps 1 to 4 are repeated for all combinations of gender and limit balance The SHAP value for age is the weighted average of all the marginal contributions to all possible combinations

Please refer to my previous post on LIME for the details on the use case I have picked up to explain. Let’s explore some basic views that the SHAP package in Python offers. Global view of SHAP values for each feature:

1. Global view of SHAP values for each feature:

This is very helpful view that shows at a global level in which direction does each feature contribute as compared to the average model prediction. The y-axis in the right side indicates the respective feature value being low vs high. Each dot represents 1 instance in the data, and that is why you can see a cluster of dots indicating there are many instances in the data with that particular SHAP value.

PAY_0 indicates the bill repayment status in the previous month. This tells us that higher the value of the PAY_0 feature (indicated by red color) more likely it is for the instance to have higher than average tendency to default the credit card payment.

LIMIT_BAL indicates the amount of credit left. This tells us that lower the value of the LIMIT_BAL feature (indicated by blue color)

2. Individual instance view

Again, I personally find analyzing the instances that are correctly predicted and instances that are wrongly predicted. This just gives a sense of which features are leading to the wrong prediction. Here the base (average) value is mentioned and each feature’s contribution to the prediction is shown. Features indicating red contribute to the prediction being higher than base i.e. towards default, whereas features indicating blue contribute to the prediction being lower than base i.e. towards non-default

Picking up the same example from my previous post. Incorrect predicted as non-default (Actual: Default)

The main contributor for this instance to be classifies as non-default seems to be PAY_0 = 0. This indicates that the previous month’s payment being done on timely basis definitely increased the odds for this instance to be predicted as non-default. You can further analyze the behavior of other continuous features that are leading the prediction to be non-default. You can 1st check their overall distribution statistics and distribution statistics for each default and non-default label.

Why is SHAP more popular and reliable technique?

1. Local Accuracy:

The explanation model should match the original model

2. Missingness:

If the simplified inputs represent feature presence, then missingness requires features missing in the original input to have no impact

3. Consistency:

Consistency states that if a model changes so that some simplified input’s contribution increases or stays the same regardless of the other inputs, that input’s attribution should not decrease

4. Efficiency:

As the foundation of SHAP values is based on computational game theory, this is the only method that can failry distribute the gain of the feature.

5. Global comparison:

SHAP values provide a way to compare the feature importance at a global level. You can also change the dataset from global to a subset dataset of interest.

Drawbacks:

1. Computation time:

Number of possible combinations of the features exponentially increases as more number of features are added. This in turn increases the turn aound time of calculating SHAP values, and approximation is the only solution.

2. Order of feature selection in all possible combinations:

Typically while solving real world problems, the target is non-linearly related with the independent features, and there is some correlation amongst the independent features too. In such cases the order in which the features are selected in the combination matters and can impact the SHAP values.

3. Simulation of scenarios:

SHAP doesn’t return a model, like LIME. So if you want to simulate scenarios of increase in a particular feature will impact the output by how much, then it’s not possible with SHAP.

4. Correlation amongst independent features:

Again in most real world problems the independent features will be correlated. In such situations, when we sample from feature’s marginal distribution there can be instances generated which might not be possible in real world.

Copy of the Jupyter notebook

If you wish to have a copy of the Jupyter notebook with the use case above, please let us know your email ID in the comments section below or through the ‘Contact Us’ section.

What’s next?

In the next blog, I am planning to explain some of the other visualizations that the SHAP package in Python has to offer.

This field of Explainable AI is evolving rapidly, and there are lot of new developments in terms of tools and frameworks. In the comments section please write your feedback on the blog, and the latest tools you have used in this field. Also, comment if you would like me to write about any particular topic.

This content was originally published on my personal blogging website: http://datascienceninja.com/. Click here to view it and subscribe to receive updates on latest blogs instantly.